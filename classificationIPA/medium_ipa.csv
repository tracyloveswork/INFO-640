Title,Link,Date,Author,Body
How voice assistants seemingly came from nowhere - SPACE10 - Medium,https://medium.com/space10/how-voice-assistants-seemingly-came-from-nowhere-33747876b91f?source=search_post---------3,"Dec 7, 2016",Owen Williams,"Top highlightChatty devices, by Sandy van HeldenHow voice assistants seemingly came from nowhereOwen WilliamsFollowDec 7, 2016 · 9 min readIf you weren’t paying attention, you’re probably wondering why, all of a sudden, we’re conversing with devices like we can with humans. Instead of clicking on buttons on your screen, in 2016 you’re able to simply talk out loud in a room, and have a speaker with a voice assistant inside understand exactly your intent and what you said.This article is a part of the “Do you speak human?” lab — enabled by SPACE10 to explore conversational interfaces and AI. Make sure you dive into the entire publication.Over the last few years, inventions such as Apple’s Siri, Google Assistant and Microsoft Cortana have flipped the way we think of interacting using voice with our devices on its head.Siri, which launched in 2011 along with the iPhone 4S, was incredible at the time: an assistant that could understand you, and even crack jokes — how could a computer be so charming?“Siri understands context allowing you to speak naturally when you ask it questions, for example, if you ask “Will I need an umbrella this weekend?” it understands you are looking for a weather forecast.” — iPhone 4S launch, 2011Fast-forward to now and devices like Amazon’s Echo and Google Home, which are small speakers that sit on your countertop with powerful microphones, are able to hear you from anywhere in the room and understand what you want, then execute that task in a matter of milliseconds.Saying “OK Google, dim the lights” and it actually happening was something of a pipe dream just a few years ago — but now it’s technology all of us can own for just over a hundred dollars.Your new best friend.Amazon Echo and Google Home are the personification of voice assistants, offering them a spot in the home where you can interact with them on an ongoing basis throughout your day, rather than just as a tool on your pocket computer and that’s the key to starting a long journey toward our dystopian Her-like future.Slow, but steady progressHow did we get here, exactly? Well, it’s been a long road. For years, it’s been slow progress. As far back as 1970 IBM already had a computer that could take a simple sentence in and understand at least the words themselves, with one catch: it took over an hour to crunch the data.The biggest problem, other than sound quality, is that a sentence could quite literally start with any word, and a five-word sentence made up of vocabulary of 20,000 words could have 3.2 x 10²¹ possibilities. In other words, it’s a huge task to actually figure out what you actually said before even tackling what you mean.Nuance is one of the biggest speech recognition and text-to-speech companies in the world, and just happens to be the company that provided the technology for Siri. The voice assistant actually started out as a military-funded project, but eventually spun out into its own company.But what’s interesting about Nuance isn’t that it’s a speech recognition company, but it’s also heavily invested in artificial intelligence, which is required to interpret the words you’re saying.Neural networks set out to replicate the brain’s learning system, by Sandy van HeldenGoogle, Amazon and Microsoft are also making enormous investments in artificial intelligence — or neural networks to be precise — to help understand what you really mean from the millions of possibilities in every sentence.The problem with computers is they’re not very good at understanding reason or context like humans are. The order of words in a sentence can drastically alter its meaning, but a computer can’t intuitively know why that’s the case. Neural networks address that by allowing computers to make sense of the world by training themselves in what they see.I didn’t teach Google what a beach is, but it knows somehowA good example of this in action is Google Photos: it uses a neural network to learn the contents of your photos, then lets you search based on the things shown in them without you ever typing a word.If you search “beach” you’ll get every photo of a beach in your collection — that’s because Photos has seen so many photos of a beach that it’s learnt what it is. Creepy? Maybe. Useful? Absolutely.Neural networks learn from data coming in and use algorithms to train themselves at better understanding the world. So the more you use your voice assistant, the smarter it gets. Neural networks, however, take immense resources to crunch data — far more than is available in your phone.Where’s the real breakthrough, then? It lies in two places that converged, seemingly all at once: bandwidth and cloud hosting.Voice processing was always difficult in the past because your machine’s hardware alone wasn’t fast enough to actually crunch the data needed, let alone learn from its experiences — and until recently, your connection was probably too slow to send that somewhere else to be processed.In 2016, with high-speed LTE and domestic internet connections the norm, it’s flipped the equation on its head. Now you’ve got a big enough, always on internet-pipe that’s able to send your voice data in the blink of an eye. The bandwidth problem has, for the most part, been solved.In the last decade there’s been a fundamental shift in how computing is done in business. Before, if you wanted to build an online service you’d need to buy server hardware, a physical space, buy access to an internet provider and a whole lot more.Now, thanks to Amazon Web Services you can have access to the most powerful computer hardware in the world without moving slightly on your couch — for a few cents an hour. That happens to be perfect for running a voice assistant, even on a massive scale.Siri, if you strip away the fancy interface, is actually a product of Moore’s Law — the rule in computing that the number of transistors in a chip double about every two years. The only reason it was released when it was, and not sooner, is because previously the computing power simply wasn’t available to deliver a meaningful, conversational experience.In other words, computers needed to get powerful enough to both understand you and synthesise a sentence in just a few seconds. Comparing just how much processing power it takes to crunch a Siri search up against a normal web search puts that in perspective:“The computational resources required for a single [Siri] query is in excess of 100 times more than that of traditional web search.”Data, data, dataOne other problem stood in the way of voice assistants getting off the ground: they didn’t actually know anything at all. Each assistant starts out at ground zero, and needs to be trained.Data processing is done in the cloud, by Sandy van HeldenIf you used early voice recognition software like Dragon Dictate, which was released in 1990, you probably remember that to actually have it understand you, it was necessary to read phrases to the computer for hours on end. For modern assistants that’s actually still somewhat true, but how it’s done might surprise you.As mentioned earlier, the more voice data a company has to crunch, the better the assistant. Google used an interesting method to grow its own database of voice samples — likely without you even knowing it:Marissa Mayer, then a Google VP, explained at the time, “The speech recognition experts that we have say, ‘If you want us to build a really robust speech model, we need a lot of phonemes, which is a syllable as spoken by a particular voice with a particular intonation…1–800-GOOG-411 is about that: Getting a bunch of different speech samples so that when…we’re trying to get the voice out of video [or other tasks requiring voice recognition], we can do it with high accuracy.”Another problem was the microphones themselves: it’s pretty hard for a computer to understand what you’re saying with all that background noise.Far-field microphones, used in both Google Home and Amazon Echo, are special, powerful arrays of microphones that let devices zero in on your voice regardless of where you are in a room, or what background noise there might be.The concept of microphone arrays and far-field technology isn’t new, but the algorithms used to detect, as well as follow, your voice in a room are new. High-quality audio is fundamental to getting the computer to understand the query in the first place, as well as reducing confusion or incorrect queries.Amazon was the first company to use one of these powerful microphones to solve the voice quality part of the equation, and everyone else is rapidly following suit: you can already find it in other products like Google Home and Sense.Not quite there yetNow you know that Siri and Google Assistant are akin to black magic, it’s time to pull back the curtain: they’re still not very good.It’s easy to flummox Siri by asking it a question, like “What’s the best Thai food around here” then following up by saying “how far away is that?” Siri still has no idea what “that” relates to, because it’s already forgotten your previous search.Google Assistant, however, is one step ahead of Siri in this regard. You could ask it “What’s that movie with Jennifer Lawrence?” Then, when it answers, you could immediately follow up and ask “What year was that released?” You’ll get the correct answer.Assistant’s short term memory is better than Siri’s, but both tools still don’t know enough cues about you to be really useful. Google’s own Assistant team gave a great example of how easy it is to take the magic away:Right now, the Google Assistant will perform to expectations if you ask it to book a table at a Mexican restaurant near you. But if you ask it for a table at “one of my usual places,” you’re taking a Thelma and Louise drive into the Flummoxed Valley.“Sorry,” it will say, “I can’t help with that.”What does usual mean? Assistant knows where you’ve been lately, and how far you might be willing to drive, but there are so many permutations of what “usual” could imply based on time of day or location that it’s a confusing query.The key is understanding the true meaning of places, things and context. Google knows where you are, where you’ve been, how far you generally travel, how long you spend at home, when you last went overseas and a whole lot more — but it doesn’t really know what any of those signals mean.What’s the biggest challenge to getting there, then? According to Scott Huffman, VP of Cloud Developer Experience at Google that’s making it useful enough to keep using it:“Honestly, the challenge for us is going to be to have enough of the conversational capability — which we think we do — to convince people to keep doing it.”The holy grail of voice assistants is exactly this: understanding the true meaning of disparate data. We’ve gotten to the point where they can figure out what we’re saying, and talk back to us, but they still don’t know how to connect all of the dots together.If you look at how far we’ve come since Siri first debuted in 2011 it’s actually pretty incredible: when Siri launched, you’d say your query, wait for a while, then get an answer.Now you’re now able to talk to a voice assistant in real time, then get a response within a second or two: it only took five years to get to this point, and it’s already normal.Advancements in processing power, bandwidth and neural networks themselves have facilitated this transition — and another five years from now we’ll find it hard to understand how we ever lived without voice assistants."
The Voice-Powered Web: combining browsers with AI voice assistants,https://medium.com/samsung-internet-dev/the-voice-powered-web-combining-browsers-with-ai-voice-assistants-77344195a1f9?source=search_post---------9,"Nov 7, 2017",Peter O'Shaughnessy,"The Voice-Powered Web: combining browsers with AI voice assistantsWhat if you could interact with the Web via voice assistants like Alexa, Bixby, Cortana, Google Assistant and Siri?Peter O'ShaughnessyFollowNov 7, 2017 · 4 min read“Go to mozillafestival.org. What are the navigation items? … Go to ‘Schedule’.”Imagine voice was a first-class input device on the Web.Imagine your favourite voice assistant could be your navigator as you interact on the Web, reading pages to you and filling out forms for you.And imagine a web page could advertise its own voice service to your browser, allowing you to interact with it in a natural way.This was the topic of our recent “AI Voice Assistants & The Web” session at Mozilla Festival in London, in collaboration with patrick h. lauke from The Paciello Group and Michael Henretty from Mozilla’s Common Voice Project.Our “AI Voice Assistants & The Web” session at Mozilla Festival 2017. Inset: the world’s worst “The Voice” fan art?Voice assistants are on the rise…Big tech companies like Samsung (my employer), Apple, Amazon, Google and Microsoft are all investing heavily into their voice assistants. They are set to become even more ubiquitous in our homes and other environments over the next few years — see Project Ambience for example.…and they are already talking to web browsersVoice assistants are already starting to gain the ability to control our web browsers. Here’s an example I recorded to demonstrate Samsung’s Bixby:What if we could go a step further and be able to interact with the web pages themselves via voice too?The Web has had tools for voice output for a long time: screen readers. Patrick began the session by sharing how screen readers are usually used via a keyboard and mouse. They can be good at outputting what’s on the screen (plus programmatic properties), but they’re essentially bound by outputting what’s visually there. Correctly coded web content can be read, understood and operated with screen readers. On complex pages though, it can be tedious.What if we could use our voice assistants for voice input on the Web too? Patrick shared an example of searching for flights. Imagine if — instead of navigating a form like this (especially via a screen reader) — we could just say: “find flights from Manchester to Frankfurt for Wednesday next week, coming back the following day”.Expedia.co.uk — an example of a complex input form — could it be easier to control via voice?Could our new wave of ‘AI’ voice assistants perform the functions of screen readers, but in a smarter way? Could they be useful for web users without the ability to use a touchscreen/keyboard/mouse, or those who struggle to use them for longer periods due to conditions such as RSI?Open voice servicesMichael then shared how Mozilla are working on foundations for open, public voice services. Training a voice assistant takes a lot of data though: 10,000 hours of recordings. To put that in context, the total of all of the TED talks out there comes to about 100 hours: still 2 orders of magnitude away! That’s why Mozilla have opened up Project Common Voice to allow the public to lend their own voices. They created an experiment called Voice Fill to allow you to search via voice on Google, Yahoo and DuckDuckGo. And they have been starting to explore the idea of open voice service registration under a preliminary name of Voice HTML.Use casesWe then asked our participants for help in a brief design thinking session. What use cases are there for voice interaction on the Web? After brainstorming, we asked them to select a particular scenario and sketch it out in groups.We had some great, thought-provoking ideas, including:Voice ID: could smart voice assistants help us to do away with passwords? They could potentially recognise our voice and authenticate us directly. We are already seeing telephone banking start to incorporate this, so perhaps we can incorporate it into the Voice-Powered Web too? Could it work well with Web Authn?Content filtering: instead of simply reading some web-based content to you verbatim, a smart assistant could adapt it for you, if you wish. For example, it could omit things that you’ve asked it to filter out, or replace offensive words with milder ones.Going hands-free: a number of groups discussed useful hands-free situations such as cooking, practising first aid and translating your speech when you’re abroad. What if we could power these kind of voice interactions using the Web?Empathy: smart voice assistants could detect your mood and use that to automatically adapt their tone and their suggestions.Sorry if I missed anything out: if you attended the session and you would like me to add, edit or add a credit, please just leave a comment :-)Next stepsWe think that there is a massive potential for the Voice-Powered Web — and we think it’s time to start discussing together how the Web can facilitate these kind of interactions in an open, secure and privacy-respecting way.You can check out our slides from the session here. If you’re interested in this topic and you would like to participate in further conversations and potential future web standards work, please join the WICG discussion here, send us an email, or leave a comment below.Update (August 2018): There is now a Call for Participation for a new Voice Assistant Standardisation Community Group!"
Voice Assistants Will Bring the Next Billion Users Online,https://onezero.medium.com/voice-forward-phones-google-assistant-and-the-next-billion-users-119a9868837b?source=search_post---------5,Feb 28,Peter Gasston,"Listen to this story--:--5:00Voice Assistants Will Bring the Next Billion Users OnlineVoice-forward phones could make the internet more accessible in the developing worldPeter GasstonFollowFeb 28 · 4 min readScott Huffman, vice president of engineering for Assistant in Search presenting the KaiOS-powered WizPhone WP006 at Google for Indonesia on December 4. Source: KaiOSA wide range of new phones were shown off at Mobile World Congress last month. At one end of the scale, Samsung introduced three variations of its premium Galaxy S10, though many of the headlines went to another new model, the Galaxy Fold, with its innovative folding screen — and almost $2,000 price tag.But for a large portion of the developing world, the most important announcements came from companies like LG, Nokia, and Orange. LG announced the K40, K50, and Q60 and Nokia revealed the 3.2 and 4.2 model phones, all mid-to-entry level Android smartphones, which will sell for about $200 and under. Each has a dedicated hardware button to launch the Google Assistant.The Sanza phone by Orange.The Orange Sanza and WizPhone WP006, both 4G-feature phones, running KaiOS (an operating system based on the abandoned Firefox OS project) were also announced and will retail for under $20. The Sanza is intended for the mostly French-speaking countries of West Africa, while the WP006 will be sold from vending machines in Indonesia for about $7. These phones have even more prominent microphone buttons — they’re voice-forward phones, also powered by Google Assistant.This is a deliberate part of Google’s strategy to reach the next billion users. On average, more than a million people a day came online for the first time in 2018, with a similar number likely for this year. And those people are, more often than not, found in fast-growing international markets like Brazil, China, India, Indonesia, and Nigeria.We need to look not at Silicon Valley or London, but to places like Sao Paulo, Bangalore, Shanghai, Jakarta, and Lagos to truly understand where the internet is going. — Caesar Sengupta, GoogleAmazon has focused Alexa on key, wealthier markets, and currently supports only six languages and regional dialects. Google Assistant, meanwhile, has the advantage of being integrated with Android devices and has grown to be available in 30 languages. A lot of that growth comes in languages spoken in the “next billion countries” — in India, for example, Google Assistant understands Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, Telugu, and Urdu, as well as English.One of the claimed benefits of voice input is that it helps people in areas of high illiteracy gain access to mobile devices and the internet. Another benefit is said to be that some languages, such as Mandarin, have complicated character sets which are difficult to type.But this isn’t always the case; in Indonesia, for example, where the WizPhone will launch, about 95 percent of the country is literate. And while it’s true that speaking is generally faster than typing, modern phones already have easier software keyboard input methods such as Pinyin.Putting Google Assistant into phones for new internet users has other advantages — it doesn’t require a powerful phone to run, so you can have a decent experience on a $7 phone, and it doesn’t use a lot of data, which is a real boon in countries like Nicaragua and Kenya where 1GB of data can cost almost 10 percent of your income.A less tangible benefit of voice-forward phones is that there’s a reduced requirement to understand or learn legacy user-interface conventions. People in many countries, especially in the West, have been using computers since they were children, and smartphones for around a decade. They have grown up with buttons and tabs and carousels. But someone from rural India coming online for the first time today doesn’t need to learn those conventions if they have a voice-forward phone — they can speak naturally to their devices to achieve what they want to do.In these fast-growing countries like India, Indonesia, Brazil, and Mexico, voice is often the primary way users interact with their devices because it’s natural, universal, and the most accessible input method for people who are starting to engage with technology for the first time in their lives.— Brad Abrams, GoogleA generation of new mobile users, who came online in Africa in the late 2000s, led to the creation of new SMS-based micropayment networks like M-PESA and Zaad, free from legacy banking systems and innovative in ways that Western money apps are only recently catching up to. A whole new generation of voice-forward devices, free from legacy interfaces, might also drive innovation in the ways people interact with businesses and services for the next billion users."
Inside Snips: Meet the team behind the private-by-design voice assistant,https://medium.com/snips-ai/inside-snips-meet-the-team-behind-the-private-by-design-voice-assistant-3df302364b44?source=search_post---------7,Mar 11,Yann Lechelle,"Inside Snips: Meet the team behind the private-by-design voice assistantYann LechelleFollowMar 11 · 1 min readby Yann Lechelle COO, Joseph Dureau CTO, Sébastien Maury CPOIntroducing the people building Snips, one of the largest AI voice companies in Europe.The Snips team in our Paris Headquarters, March 2019Let us reintroduce ourselves. Over the past year, Snips has more than doubled our team size and opened an additional office in Tokyo. Here’s what that momentum looks like in numbers:— Over 60% of Snips employees are engineers; — 15% hold PhDs; — 8 patents filed for different bricks of our technology; — 25% of our workforce is female.Read more about each team and some of the rising stars making technology disappear on the Snips blog.Interested in joining us in Paris?Check out our open roles and explore our technologies.If you liked this article and want to support Snips, please share it!Follow us on Twitter: @Snips"
The Real Reason Voice Assistants Are Female (and Why it Matters),https://medium.com/pcmag-access/the-real-reason-voice-assistants-are-female-and-why-it-matters-e99c67b93bde?source=search_post---------4,"Jan 29, 2018",PCMag,"Top highlightThe Real Reason Voice Assistants Are Female (and Why it Matters)PCMagFollowJan 29, 2018 · 3 min readOur interactions with AI teach and train it, but we are also shaped by these experiences.By Chandra SteeleAsk your phone, Echo, or computer something. Or call your bank and talk to the automated menu. I’ll wait.Whatever you asked, a synthesized version of a woman likely answered you, polite and deferential, pleasant no matter the tone or topic.That’s because Siri, Alexa, Cortana, and their foremothers have been doing this work for years, ready to answer serious inquiries and deflect ridiculous ones. Though they lack bodies, they embody what we think of when we picture a personal assistant: a competent, efficient, and reliable woman. She gets you to meetings on time with reminders and directions, serves up reading material for the commute, and delivers relevant information on the way, like weather and traffic. Nevertheless, she is not in charge.When performed by humans, these tasks have sociological and psychological consequences. So one might think that using an emotionless AI as a personal assistant would erase concerns about outdated gender stereotypes. But companies have repeatedly launched these products with female voices and, in some cases, names. But when we can only see a woman, even an artificial one, in that position, we enforce a harmful culture.Still, consumers expect a friendly, helpful female in this scenario and that is what companies give them.“We tested many voices with our internal beta program and customers before launching and this voice tested best,” an Amazon spokesperson told PCMag.A Microsoft spokesperson said Cortana can technically be genderless, but the company did immerse itself in gender research when choosing a voice and weighed the benefits of a male and female voice. “However, for our objectives — building a helpful, supportive, trustworthy assistant — a female voice was the stronger choice,” according to Redmond.Apple’s Siri and the Google Assistant currently offer the option to switch to a male voice; Siri since 2013 and Google since October. But Alexa and Cortana don’t have male counterparts.Consider that IBM’s Watson, an AI of a higher order, speaks with a male voice as it works alongside physicians on cancer treatment and handily wins Jeopardy. When choosing Watson’s voice for Jeopardy, IBM went with one that was self-assured and had it use short definitive phrases. Both are typical of male speech — and people prefer to hear a masculine-sounding voice from a leader, according to research — so Watson got a male voice.Women, meanwhile, use more pronouns and tentative words than men, according to Psychologist James W. Pennebaker. Pronoun use, particularly of the word “I,” is indicative of lower social status. AI assistants are very prone to using “I,” particularly in taking responsibilities for mistakes. Ask Siri a question she can’t process and she says, “I’m not sure I understand.”It’s critical that we challenge stereotypical gender roles in our personal assistants. Our interactions with AI teach and train it, but we are also shaped by these experiences. It’s why parents are concerned about unintentionally raising rude children when Alexa does not require a “please” or “thank you” to carry out a task.As our relationship with technology enters a new stage of intimacy, it’s worrying to think of what will happen when some people’s primary sexual experiences will be with a sexually acquiescent robot. Sexually harrassing Siri for a YouTube video might be amusing to some, but it’s unsettling to hear how similar that language is to what women hear from street harassers. There is the same societal expectation that both just accept it.Humans aim for linguistic style matching in their social interactions, meaning they try to match the language patterns of the human — and now AI — with which they are speaking. But as AI enters our physical realm, there are serious personal and social consequences for treating it in a degrading manner. The companies behind AI are cashing in on bias and that is not the way to a utopia, tech or otherwise.Read more: “AI Has Been Creating Music and the Results Are…Weird”Originally published at www.pcmag.com."
