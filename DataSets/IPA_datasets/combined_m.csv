doc_id,title,term,url,text
1,"What is Zero UI?
Are the interfaces running out? Understand the term “Zero UI” and what it means to us designers.",zero ui,https://brasil.uxdesign.cc/o-que-%C3%A9-zero-ui-d416d8aac8e4,"A quick Google search shows that for some time there has been talk about Zero UI (zero user interfaces) and what will become of the designer profession when screens disappear.
But Zero UI does not mean the complete extinction of interfaces; it just means the end of interfaces as we know them today.
Consider the following for a moment: Today interfaces only exist because computers are “dumb”.
So we humans, very intelligent beings, need to go there and give instructions to the computer. In the past the only way to do this was by memorizing commands and typing our intentions into an interface like this one:

Over time, we designed more user-friendly visual interfaces that helped us accomplish the same tasks much more simply.

And more recently, even simpler:

In this process of technology evolution, the visual interfaces we know today take on a secondary, background role, giving us space to engage with the things that are really important to us - other humans, the physical spaces we inhabit, nature, the environment. society.
Zero UI is precisely the name of the process of disappearing visual interfaces, to make room for other types of interfaces . Voice, movement, eyetracking, brainwaves, among others. That is, the term “zero UI” itself is incorrect because the interface does not cease to exist. But for now, as a way of educating our industry and provoking designers about this impending technological change, it works.

And what does this mean for us designers?
That the experiments we are designing (and the interfaces that control them) are becoming much more complex - as many other aspects of human behavior must be taken into account.

Design principles remain the same: design experiences that are easy, intuitive, enjoyable, and subtle. Only what changes is the material we use to make these experiences happen.
What to expect as a designer:
Much less software that draws pixels (Photoshop, Sketch, Illustrator), and much more software that draws interaction streams.
Much, but much more testing. Internal testing while something is being designed (after all, not just “looking at the layout” to see if it works), quality control testing , and user testing .
The emergence of other types of specialization in our industry, such as verbal designer, conversational designer, gesture designer.
Our role as UX Designers remains exactly the same: identifying our audience's needs and wants, thinking about products, services and features that deliver those demands, strategically thinking and prototyping these interfaces, and then testing with users to enhance the experience with each new cycle. .
Ready to head for it?"
2,"Bots, Chat, Voice, Zero UI, and the future of Mobile Apps",zero ui,https://chatbotsmagazine.com/the-future-of-ui-bots-conversation-voice-zero-ui-and-the-end-of-the-mobile-apps-defa5dcc09f5,"“Alexa”- “Play Blake Shelton-Hillbilly bone.”
“Siri”– “Text Alex that I am running late for the meeting.”
As I do this without opening an app on my iPhone with its crowded screen and some 50+ apps, I am hit hard with the thought, the idea of a mobile app as an “independent interaction destination” is becoming irrelevant. The action has shifted to Voice, Natural Language, Notifications, API, and the Conversation; and this has a huge impact on existing mobile app-dependent businesses and how we build and design products in the future.
Before you shake your head in disbelief, let’s put things in perspective:
1. The mobile app discovery and its feature discovery layer is broken. There are more than three million+ apps combined in all app stores, and the last thing we need is more apps or features.
2. Mobile users have limited attention spans, often jumping from one app to another. An average smartphone user has 42 apps on his device but spends 90% of his time on only 9 or 10 of them.
3. Companies spend billions of dollars every year to drive installs and user engagement, but 25% of apps are only used once and 75% of users leave within the first three months.
4. The growth of mobile and its app-centric world has been the opposite of the web, and there is no analog of PageRank for mobile apps.
5. The interaction at the UI layer of mobile is very crowded. A mobile screen with crowded app icons as a dominant design pattern feels old and inefficient now.
6. Messaging is the most widely used application on the mobile platform, with chat-based apps like WhatsApp, WeChat, and Facebook Messenger owning the communication layer of the world.
7. Application and device interaction is beginning to shift due to developments in Voice Control and Intelligent Assistants (IA). Already, more than 15% search queries are made on Baidu using Voice Input.
These points have a major bearing on mobile products and app-dependent businesses. Recent advancements in voice technology and AI suggest that we are undergoing a paradigm shift in how we interact with technology. Ever tried Apple’s Siri, Google’s Now, Microsoft’s Cortana, Amazon Echo, Google Nest, or Facebook’s M?
All this progress leads to a whole new set of open questions — is Conversation the next frontier? What role does design play in creating one of these experiences? And most importantly, what happens when our devices finally begin to understand us better than we understand ourselves?
Advances in AI and technology are causing a change in our traditional understanding of UI and human-computer interaction.
So let’s dive deep.
Historical Evolution — The Story of New Platforms
Technology Wave leverages platforms to build scale. New platforms enable new applications, which in turn make the new platforms more valuable, creating a positive feedback loop. Technology progress in the last 20 years has thrown up three key platforms — mainframe/PC, the Web, and mobile. Each platform requires recreation of the application layer, built and customized to grow and scale the new platform. Refer to the picture below.

Because of these technology product cycles, there are mutually reinforcing interactions between platforms and applications. In the last seven to eight years, we have shifted from an “era of the web with browser-based applications” to an “era of smartphones with native apps.” With the growth of mobile -as- a- platform, the application layer also underwent a substantial change.
With the change in platform, this shift is set to happen again. What we are witnessing now is the “hyper-growth phase” of the mobile platform and, by 2020, almost 80% of people in the world will have a smartphone.
If 2020 is “peak mobile,” then we are already in the gestation phase of a new platform, which will require recreating new application layers. This means that applications built for mobile as the primary platform are slowly beginning to lose relevance.
Our experience of the mobile screen as a bank of app icons is dying and we are moving toward a UI-less computer interaction environment. This change in paradigm has a massive impact on how we need to design and build applications.
Remember, thus far, all human progress has been about how humans interact with machines.
The Paradigm Shift in the Transfer of Knowledge
How would you define the UI of Amazon Echo, Nest thermostat, or Siri?
Is it voice, a shrug, a grunt, or a caress?
Before you answer that, let’s refer to an important thread in technology history — The Transfer of Knowledge.
The transfer of knowledge, thus far, has evolved in three paradigms — human to human (past), human to machine (present), and machine to machine (future). For the first time in our history, the new transfer of knowledge will not involve humans.
Advancements in the Internet of Things (IoT), Artificial Intelligence (AI), and robotics are ensuring that the new transfer of knowledge and skills won’t be to humans at all. It will be directly machine to machine (M2M).
Objects such as smartphones, connected cars, iBeacons, wearables, and cameras will begin to communicate in ways that no entrepreneur has yet to imagine. Google Mind, Boston Dynamics-Atlas, virtual reality, and drones are just a glimpse of things to come.
Our experience of technology is becoming so immersive that the idea of an app as a destination is becoming far less important, and very soon the primary interface for interacting with apps might not be the app itself.
We’re entering the age of apps as service layers, where apps are intelligent, purpose-built, and informed by contextual signals like location, hardware sensors, previous usage, and predictive computation. They respond to us when we need them to, and represent a new way of interacting with our devices.
Hence, the Application-User Interface is undergoing a massive paradigm shift, and these are the key trends driving it:
1) Apps are Becoming APIs and Bots are Taking Over
Looking back to the Web 2.0 era of the mid-2000s, we’ve witnessed a huge change to the Internet. It went from static pages and moved towards applications. Sites like Google Maps or Flickr opened up their APIs, allowing anyone to build websites pulling in their data. With this ability to create new things on top of this data, websites were transformed from static pages into dynamic services.
Now, we are witnessing the same ‘shift’ in Mobile. With an increasing number of mobile apps and decreasing size of the mobile screens, we’re reaching a peak in the mobile “OS & Apps” paradigm. Downloading, setting up, managing, and switching between so many apps on our mobile device is merely becoming inefficient.
Thus, transaction-based apps such as those to book cabs, search, and shop or order food are predominantly turning into APIs within the messaging apps. It does not matter anymore which app owns the user. Eventually, the operating system or the platform everyone has built their application on ends up owning them all. However, there are some exceptions to this, particularly in China, where the OS platform itself is still fragmented, and the applications reside in the root of the mobile OS on that platform.
We are also witnessing something similar to a trend in the ’90s, when browsers replaced the desktop OS as the new platform, and websites replaced client applications. A similar transformation is happening today. Messaging bots are beginning to replace mobile apps.
Bots are to modern messaging apps what APIs were to Web 2.0: a way to build on top of other services and a new way of interacting with underlying (existing) services, significantly changing human-and-computer interaction.
Therefore, we are witnessing the start of an era, where messaging is the new OS, Bots are the new apps, and the “bot store” is the new app store.
We are just at the beginning of the bots era and there are many more developments to come. These developments will open up new and giant opportunities for consumers, developers, and businesses.
Some leading players in this space are Messenger, WeChat, Line, Kik, Slack, and Telegram.
2) Notification is the Interface and Emergence of Super Apps-as-Platforms
The days of sending lots of notifications to bring people back to an app are going away. The focus is on designing notifications that people can engage with then and there without opening the app. This new paradigm matches much more closely with how real life works. We don’t live our lives in silos, like the app silos that exist today. People are beginning to forget about apps and thinking in terms of businesses, products, solutions, and services.
There is a definite trend towards notifications as the new interface. Very soon, we may not need to open an app to interact with it. It will still live on the device, but its interface will reside on the top layer of our phone, the notification layer. With the container for content soon becoming invisible, learning to engage users via notifications is the key.
Paul Adams, former Googler wrote this:
“In a world where notifications are full experiences in and of themselves, the screen of app icons makes less and less sense. Apps as destinations make less and less sense. Why open the Facebook app when you can get the content as a notification and take action — like something, comment on something — right there at the notification or OS level. I believe screens of apps won’t exist in a few years, other than buried deep in the device UI as a secondary navigation.”
Action now resides in the notification. Google Streaming and Apple Extensions are a step in that direction.
This trend is very much visible in what’s already happening in China with Baidu and WeChat, where smaller apps are getting bundled within bigger apps, only surfacing when some interaction in the UI invokes the smaller app. For example, in Baidu Maps, you can find a hotel, check room availability, make a booking, and make payments, all inside the app.

Baidu Maps allowing users to search, book and confirm a movie
Looking at the Chinese internet market is always a great way to challenge one’s belief in what’s inevitable. In this case, Baidu Maps, Alipay, and WeChat, amongst others, are thriving on exactly this approach, that is, bundling multiple services into one Super-App.

Walled Gardens, Portals, Platforms- Chinese App UI’s
In the western world, Facebook is also looking to bundle its Messenger back with a layer of accessories, highlighting how short the cycle of unbundling can be in a mobile-first world. I think Facebook is very likely to make some major announcements at the forthcoming F8 in April 2016 around Bots and Messenger as a platform.
3) The Shift to Context, Conversation, and Human UI
Since messaging apps are used more than any other application on the mobile platform, what if instead of installing an app, we allow a service to chat with us via a message?
One trend that is making good use of advancement in natural language processing is the rise of Chatbots and Conversational UI’s, that is, using automated software inside WhatsApp/Messenger/WeChat or other messaging apps.
The cause of this shift is simple: human nature. The conversational interface allows users to ask questions, receive answers, and even accomplish complex tasks in both the digital and physical world through natural dialogue.
Humans are innately tuned to converse with others. It’s how we share knowledge, emotions, and organize ourselves. Language has been part of our makeup for hundreds of thousands of years. So, of course, we message all day long in bursts and binges, with family, friends, and colleagues. Messaging has become a platform through which our daily life is conducted.
My experience, in China and other South East Asian countries, of using Weixin (WeChat)and Alipay for booking cabs, movie tickets, checking flights, making payments, video calls, and more has only reinforced my conviction — that a chat and voice-based messaging, payment, and commerce platform is the future of the “attention economy.”
These apps don’t use a traditional UI as the means of interaction. Instead, the entire app design revolves around a single messaging screen; hence, they are called “Invisible” or “Conversational” apps. While these apps support a slew of different functions, from checking our bank accounts, scheduling a meeting, making a reservation at a restaurant, to being our travel assistant, they all have one thing in common: messaging is at the heart of the interaction.
Led by Facebook Messenger in the West, and WeChat in the East, messaging apps are quickly transforming themselves into all-encompassing platforms, thereby allowing users to buy products and services, send payments, communicate with businesses, and engage in commerce, leading to a convergence of context and transactions within messaging platforms.
Two good examples of this approach are Baidu Maps and WeChat in China.
Using Baidu Maps, you can not only find a restaurant but also book a table or order home-delivery. You can find where a film is showing and choose a seat. You can order a cab, with cabs from all the services in your city shown together on the map. You can book a hotel room or an on-demand cleaner. Most of these services are provided through embedded third-parties service providers’ applications — Baidu is already doing deals with other Chinese Internet companies to provide this.
WeChat (which now has 840M users) is doing much the same, but, of course, starting from a chat experience rather than a maps experience. WeChat has amassed a massive user-base, become a platform (where the underlying OS doesn’t have power because of multiple operating systems), and emerged as a ‘Super-App’ — the mother of all the apps.

Booking a doctor appointment via WeChat Wallet- Image credit a16z.com

WeChat Virtual Assistants- KPCB
‘An App within an App’ is very different from ‘Chat based Interface.’ You’d need to use WeChat in China to understand the location ‘context’, and the entire WeChat ecosystem — Official Accounts, Payments, Location,QR codes, O2O and API’/Application Layer, tech architecture, etc. It is akin to a Portal, Platform, and Mobile OS combined, and very different from anything in the western world.
Another important aspect to note is the increasing role of pre-existing context. Extending the Baidu Maps example, bookings are an organic extension of the context the user already is in. We’ll already be searching for restaurants on the map depending on our location, but now “make a booking” and “pay” will appear.
In the Western world, a significant step in that direction is the recent partnership between Facebook and Uber; now we can order an Uber-Cab within the Messenger app. Facebook is also releasing M, a personal “Intelligent- Assistant,” integrated within Messenger to help us do just about anything.
We are seeing this trend in conversational interfaces where companies like Operator and Magic are leading the way, designing rich experiences for their clients to interact with directly.

Operator.com screenshots
In conversational interfaces, the user experience is primarily a sequence of messages that flow back and forth and involve little layout and design, thus rendering the old paradigms obsolete.

New start-ups developing invisible and conversational apps understand that the UI is not the product itself, but only a scaffolding allowing us to access the product.
And that brings me to our next key trend.
4) Voice Interface, Zero UI and the End of the Screen-Based Interface
Extending the Echo and Siri example — are we in an era of Zero UI?
“Not entirely, but we’re getting close.”
Zero UI is the concept of seamless interaction with technology by removing the barrier between user and device. Typically we interact with a device directly through a touchscreen or indirectly with remote control. However, Zero UI is the push to become even more integrated with technology, such as touch-less tech in the case of Amazon Echo.
Natural language processing, the field of computing and AI concerned with making computers understand and speak the natural languages of humans, has come a long way in recent years. Once the stuff of science fiction, advances in the field of machine learning and voice control have made voice interfaces far more practical, and this is making it easier to communicate with the devices around us.
While a world entirely devoid of physical interfaces may never be a reality, a world tied less to our devices is already happening. At the helm of this transition are gesture-based user interfaces such as Project Soli.

The gaming world was the first to adopt gesture controls as a way of providing a more natural user experience. Think the Wii, PlayStation Move, and Microsoft Kinect. Our gaming consoles are less tied to button commands and instead integrate more properties of physical space and motion into our experience.
All of this is causing a shift in “design-think.” Instead of just designing for two-dimensions — that is, what a user is trying to do right now in a linear, predictable workflow — now the designers need to think about what a user is trying to do in any possible workflow.
Andy Goodman, group director of Fjord, coined the term “Zero UI,” and defined it as “The new paradigm of design when our interfaces are no longer constrained by screens, and instead turn to haptic, automated, and ambient interfaces.“
Zero UI is not a new idea. If you’ve ever used an Amazon Echo, changed a channel by waving at a Microsoft Kinect, or set-up a Nest thermostat, you’ve already used a device that could be considered part of Zero UI thinking.
It’s all about interfacing with the devices around us in more natural ways: voice control and artificial intelligence, haptics, and computer vision.
Maybe next time you’ll say,
“Alexa — book me a flight to Singapore and charge to Visa card!
In Conclusion: Re-Architecture and Re-Platforming for the Next Decade
Web and mobile platforms are going through a massive re-architecture and re-platforming.
2016 is definitely the year of everything conversational, and messaging is eating the world. Every community, marketplace, on-demand service, a dating app, social game, or e-commerce product has or will soon have chat as part of the experience to drive retention, engagement, and transaction volume.
The current “pull-based” (we visit websites or download mobile applications) world is giving way to “push-based” (everything will come to us) world. With this “Big Reverse,” applications are disappearing into the background much like our electricity or water supply.
Powerful speech technology from leading Internet companies is making it much easier to ignore the touchscreens on devices for something much more efficient and intuitive: our voice. The advancements in voice interface will also lead to the start of voice payments and voice commerce.
Although, we are witnessing is the rise of the conversational and voice interface, the next wave is likely to revolve around Artificial Intelligence (AI). Intelligent Assistant applications like Echo, Google Now, Cortana, Siri, and Facebook M are an early example of how text/voice-based, conversational interfaces, will shape the world around us.
The entire history of computing is nothing but the history of progress in our ability to communicate with machines. With AI-powered interfaces, what happens once we achieve Zero UI? What happens when our devices finally understand us better than we understand ourselves? What happens when the machines reach or surpass the “human level” of intelligence and computers get embedded in us?
Do we become the next UI? — — Singularity, Anyone!!!"
3,Zero UI Andy Goodman explores a possible future in which we don’t need complicated interfaces to interact with our products,zero ui,https://medium.com/net-magazine/zero-ui-dabeb2bc6aeb,"Zero UI has been sometimes misinterpreted as meaning getting rid of the interface entirely. What it actually refers to is a process where many of the visual interfaces we currently spend so much time with recede into the background, leaving us open to engage with the stuff that is important and useful to us. It is analogous to inbox zero, where we strive to achieve a blissful state in which everything is dealt with, calm and invisible. If that is possible, of course.
I started my career as an interaction designer in 1994, although it wasn’t called interaction design then. My job title was probably ‘graphic designer’ , and since then I have been labelled a UX designer, experience designer and service designer. What we can deduce from these titles is that the object being designed has become less tangible over time, and less to do with interactions happening on the screen. I’m not sure what people that practice Zero UI will be called, but it will be something different again.
Electronic mesh
This shift away from the very controllable — although quite primitive — environment of screen and pointer means the things we are trying to do are becoming more complex. They now have to take into account a lot more ideas around human behaviours, motivation, emotion, and all kinds of weird things like that.
We’ve always had to bring aspects of psychology and perception into our work, admittedly in a pretty amateurish way most the time. Understanding what would make someone click a button, how users would retain information, and the barriers to committing to a decision is important — but for all the elegance of the interfaces we have designed, they are all two-dimensional, with simplistic cues and triggers.
As we move into a connected world where objects, people and environments are all joined together by a mesh of invisible electronic tethers, the decision making, the services we want, and the results we expect from our interactions become exponentially more complicated. Not only will a system have to predict what someone wants to do next, but it will also need to know where they are, where they are heading and what their intent is. It will be about how we as humans interact with entire systems, and how the constellations of things around us become part of an endless dialogue between us and the world.
I don’t think there is a huge groundswell of opinion bemoaning the terribleness of interactive systems, products and devices. Quite the opposite in fact; we seem to be entranced by them all. And why not? The devices are beautiful, the systems are intelligent and the services make life so much easier. A few dissident voices, from the likes of Sherry Turkle, have put together pretty strong arguments for the social and emotional dissonance that our addiction to electronic media causes. But in the end the benefit the digital world has brought us far exceeds the problems it has caused.
Nevertheless, we can all agree that removing the complexity these devices bring into our lives would be a genuine improvement on the state of things. Not just for the older generations, who try as they might are often confounded by the intricacy and closed-shop paradigms of software, but for all of us that have ever struggled with an update or service switch.
Visual creatures
The phrase ‘Zero UI’ is designed to provoke people, because as designers we spend a lot of time thinking about the way things look, and not much time thinking about anything else. It is inevitable because of the way platforms and computer interfaces have always been, and also the way that we interact with the world generally. We are primarily visual animals, and so we sometimes forget how important all those other senses are in conveying experience, and how important a part of our memory and identity they are.
If we think of the ways in which we can make use of those other senses, we can start creating interactions that become more intuitive, more pleasurable and more subtle; and that create less work for us. The objective is to be able to spend less time fiddling around with computers, but still achieve the same outcomes, enjoy the content they provide for us and the communications they enable.
The irony is that rather than delivering on the promise of freedom, computers have in some sense enslaved us. Not in a Terminator or Matrix sense, but in a much more mundane way. It’s the fact that the battery life on our phone isn’t good enough, or that we can’t work out what’s wrong with our router that’s causing most problems. Who would have thought such banal things would become such a major factor in our lives?
Because the machines haven’t been designed well enough, it feels sometimes like we’re serving them rather than the other way around. We’re constantly having to feed them and keep them warm and keep them powered. Kevin Kelly’s seminal book What Technology Wants alludes to this idea that machines are a kind of domesticated animal that have evolved over time to get us to look after them.
Because the machines haven’t been designed well enough, it feels sometimes like we’re serving them rather than the other way around. We’re constantly having to feed them and keep them warm and keep them powered
Established patterns
Let’s wind back and consider the problem interaction design was there to solve initially. It was designed to help us understand how a computer or a machine works and provide an interface for us to operate it. When I was younger, I always knew I was going to do something to do with computers and design because I was the only one in the family that could programme the VCR. I would think: this is really bad, why can’t it be easier? That is the motivation of any designer.
A lot of those purely functional parts of the UI have been solved now, with the help of patterns that are pretty good for simple kinds of interface problems. You could go and design a whole different set of patterns, but they probably wouldn’t be as good and would require people to learn new ways of interacting.
However, there is a whole set of more complex things we are trying to do now, which are really quite hard problems to solve. One example is the Uber app on your Apple watch: in principle a genius simplification of the experience — just open the app and call a car. But we all know how flaky GPS is, what if the car gets sent two blocks away? With such a simple interface the user has no way of adjusting the information to give the precision required. So they end up checking on their phone and the magic is killed. We will need multiple layers of failsafe and redundancy in systems to allow these types of interactions to become commonplace.
Coordinated systems
Imagine a Zero UI scenario where the user wants to travel to the other side of the country. Leaving aside the booking of the plane ticket for now (the complexity of which requires a detailed visual interface), all the systems that enable you to get to your destination could coordinate, from the alarm that gets you up in the morning, to your coffee machine grinding a double shot, to the alert that tells you when to leave and that you need to take the subway rather than trying to get a cab across town at that time of morning, to the system that allows you to walk straight through the pay barrier at the train station, and so on.
Recently Matías Duarte, Google’s VP of design, talked about how atomised apps are the future of the mobile experience, and how even computer power will be distributed into smaller units, away from the device. This is very close to the vision I have for Zero UI, but perhaps a bit more conservative (necessarily). I would love to see a world where we can go about our daily business without having to waste valuable brain cycles on trivial things like making sure the cab finds our exact GPS coordinate."
4,Zero UI: Designing for Screenless Interactions,zero ui,https://medium.com/thinking-design/zero-ui-designing-for-screenless-interactions-8b2215bc2d9f,"We are starting to live our daily lives in the post screen world. The advent of smart and contextually aware devices have changed how we interact with content. Data is a snapshot of our contextual connectivity to our physical and digital environments. Designers will need to build screen-less experiences that leverage data and algorithms to create value for users.
By definition, Zero UI removes the traditional graphic user interface (GUI) from the equation and uses nature interactions of haptic feedback, context aware, ambient, gestures and voice recognition.
The best interface is no interface.
– Golden Krishna
Haptic Feedback
Haptics (or kinesthetic communication) provides the user with motion or vibration-based feedback. Sega’s Moto-Cross video incorporated haptic feedback through the controller to add to the experience of colliding with other players on screen. Nintendo brought haptics to home consoles with their Rumble Pak accessory. Today, we commonly experience haptics as we interact with small touch screens. Haptics are currently used by fitness trackers and smartwatches to provide notifications to the wearer. Haptic feedback is being extended to clothing, while ultrasound or ultrahaptics will bring the sense of touch to virtual reality (VR) and Kinect style gaming. Keeping haptics to a minimum and avoiding overuse enhances the value of the feedback for users.
Context Aware
Devices and apps that are contextually aware and remove the need for additional interactions simplify digital and physical experiences by anticipating user wants. Personalization allows Zero UI devices and applications to be preemptive, predictive, and proactive. Domino’s Zero Click App works on the premise that if you launch the app, you want pizza delivered. The only way not to have pizza delivered is to close the app within ten seconds.

Domino’s Zero Click App via Google Play
Context of use is being integrated into devices, lowering the overall need to interact with the app or device settings to achieve user wants. At first glance, the new Apple AirPods are just wireless earphones. However, they offer relevant and customized experiences to the user. Remove one AirPod, and music playback switches to mono. Remove both, and playback stops and resumes when the user puts them back in.

Apple AirPods via Apple
Other devices, like the Nest Thermostat, rely on collecting data on usage and interactions to adapt to the user’s anticipated needs. By leveraging sensors within a device or location data, we can design contextual experiences that become implicit rather than explicit interactions. What is the situation or context the user is in? What is the context of use of the device? The need is to build and design interactions that are in the background and temporal.
The real problem with the interface is that it is an interface. Interfaces get in the way.
– Don Norman
Glanceability and Ambience
Ambient devices work on the principle of glanceability, with no need to open applications or read notifications. One glance should provide the user with the needed information or context, much like a wall clock or a single day calendar. The Nabaztag Rabbit was a companion style device that provided glanceable experiences by combining colored lights and changing positions of the rabbit’s ears. While the Chumby provided a snapshot of information through widgets. The underlying premise of ambient devices is to create a seamless bridge between physical and digital spaces. These interactions are connected, browserless experiences. Both of these devices were short-lived as they were not able to adapt to changing technologies and the information needs of users.

Nabaztag Rabbit — Image via CNET
Gesture Based Interactions
Gestures need to be easily learned and repeatable. One of the problems with gestures is getting systems to recognize the physicality of the actions (wave, hover and swipe) in the three dimensional space. System responsiveness to gesture inputs can create problems as users start going through multiple gesture sets trying to get the system to respond. There needs to be a balance between teaching the gestures and providing feedback to the user on successfully completing actions and tasks. Designing for gestures requires being adaptive and working within the limitations of the system (processing speed) and the focal length of the camera and establishing a minimum and maximum distance from the screen/camera for the interactions to occur. Google’s Project Soli makes “your hands the only interface you need” by using radar to detect fine movements. UI layers and interactions should not overextend the user’s motion away from the body as this can be fatiguing. Make gestures small and natural.

Xbox Kinect Gestures via Microsoft: Programming with the Kinect
Voice Recognition
Voice recognition has been a part of science fiction since the early 1970’s and entered the mainstream with the advent of voice recognition and search in both iOS and Android phones. Users will accentuate syllables, words and phrases, thinking that this will allow the system to recognize their queries and commands. Designing the conversational UI requires additional user research to find out how users will phrase and construct their queries or statements. One of the difficult parts of voice recognition is the need to adapt the system to regional dialects and slang. Something as simple as ordering a pizza may become a complex design problem when all the different variants for ordering are considered. Understanding user wants and intent, or motivation, helps determine the phrasing or expression.

Image via Amazon
The Amazon Echo limits interactions to ambient or background conversations by requiring an initiation phrase with each command and query.
Conclusion
The future of Zero UI is in leveraging data and understanding user intent to design and build personalized user experiences that are relevant and anticipate user needs. Contextual devices create experiences that extend beyond the screen and connect our digital and physical worlds. Minimizing dashboards and providing information that is glanceable and creates value for users adds to the challenge of designing for screenless interactions. User research will extend beyond how we interact with an interface to how we live and use physical objects in our daily lives."
5,Zero UI — Designing Invisible Interfaces,zero ui,https://medium.com/subscribed-magazine/zero-ui-designing-invisible-interfaces-d859fc964913,"Zero UI refers to a paradigm where our movements, voice, glances, and even thoughts can all cause systems to respond to us through our environment. At its extreme, it implies a screen-less, invisible user interface where natural gestures trigger interactions, as if the user was communicating to another person.
It is brought about by the emergence and eventual mainstream adoption of sensors, wearables, distributed computers, data analytics, connected everything, where anticipatory, adaptive and contextually aware systems provide what we want when we want it — “by magic.” Zero UI will not be limited to personal devices but will extend to homes, entire cities, even environments and ecosystems, and as a result have a massive impact on society as a whole."
6,The Zero UI Debate,zero ui,https://medium.com/swlh/the-zero-ui-debate-e4b8bee4b742,"The new paradigm of design calls for “design of artifacts, environments and systems” that respond to our thoughts, voices, movements and glances. The field of Interaction Design was realized to help understand how humans will interact and operate on computers. This has now led to everything being crunched into a rectangle screen and a central unit empowered to compute, creating self-knit personalities. Zero UI might remove this disconnect between humans, maintaining the human-computer interaction.
“The real problem with the interface is that it is an interface. Interfaces get in the way”
These words of Don Norman have been adapted in many designs, the most commercial products of such kind being personal assistants like Alexa and Echo. These devices are trained to anticipate user needs.
In his talk at CHI 2014, Scott Jenson introduced the concept of a technological tiller. According to him, a technological tiller is when we stick an old design onto a new technology wrongly thinking it will work. The term is derived from a boat tiller, which was, for a long time, the main navigation tool known to man. Hence, when the first cars were invented, rather than having steering wheels as a mean of navigation, they had boat tillers. The resulting cars were horribly hard to control and prone to crash. It was only after the steering wheel was invented and added to the design that cars could become widely used. As a designer, this is a valuable lesson: A change in context or technology most often requires a different design approach. In this example, the new technology of the motor engine needed the new design of the steering wheel to make the resulting product, the car, reach its full potential.
When a technological tiller is ignored, it usually leads to product failures. When it is acknowledged and solved, it usually leads to a revolution and tremendous success. And if one company best understood this principle, it is Apple, with the invention of the iPhone and the iPad.
A technological tiller was Nokia sticking a physical keyboard on top of a phone. The better design was to create a touch screen and digital keyboard.
A technological tiller was Microsoft sticking Windows XP on top of a tablet. The better design was to develop a new, finger-friendly OS.
And I believe a technological tiller is sticking an iPad screen over every new Internet of Things thing. What if good design is about avoiding the screen altogether?
Learning about technological tillers teaches us that sticking too much to old perspectives and ideas is a surefire way to fail. The new startups developing invisible and conversational apps understand this. They understand that the UI is not the product itself, but only a scaffolding allowing us to access the product. And if avoiding that scaffolding can lead to a better experience, then it definitively should be avoided.
Designers who rely on visualgasm would require to think beyond screens. Intuitive interfaces are an example pointing towards this paradigm. Imagine you want to travel overseas. The ticket booking would be done by saying simple keywords. An alarm will be set and it will remind you accordingly, your cab will be ready at the same exact time you are done packing and the payment would be directly through your bank, all satisfying the Paulo Coelho’s words,
“And, when you want something, all the universe conspires in helping you to achieve it.”

Would you call this a UI or Zero UI?
It will require efforts from both academia and investors to move ahead in this domain. Mind-reading computers to brain controlled robots, the age is near where products will be out of their Research phases, ready to kick in the markets. The role of designers shall change and UI designers will have to adapt to these.
If we look at the history of interfaces, Xerox PARC and Apple Lisa, it required display of content. The current age demands control. The quest for making screens thinner will be gone for now, it will all be invisible. A question definitely will arise now, How will we get all the information we used to get earlier. To answer this, have a glance at your smartphone and think how much content you see is relevant to you now. You may even look at the webpage and see how much data is relevant to you for a particular time. Other things are just there because you might need them. Zero UI would enable computers to anticipate and produce content as you need.
So does this means that all UI designers will be unemployed?
Not really. As far as I know, UIs will still be needed for computer output. While bringing about the transformation, people will still use the screens to read, watch videos, visualize data and so on. What I do believe, however, is that design processes will be driven by these new technologies. This is necessary to understand for those planning to have a career in Interaction Design. In a future where computers can see, talk, listen and reply to you, what good are your prototyping skills going to be? What new software would Adobe launch for these designers now.
Let this be fair warning against complacency. As UI designers, we have a tendency to presume a UI is the solution to every new design problem. If anything, the AI revolution will force us to reset our presumption on what it means to design for interaction. It will push us to leave our comfort zone and look at the bigger picture, bringing our focus on the design of the experience rather than the actual screen. Zero UI would still mean there is an interface for users to operates, its just way too different than screens. And that is an exciting future for designers.
First step of all, definitely, is to have a proper name for this. Zero UI or No UI is too much offense for the existing UI Designers, nor does it explain what it is.
"
7,Zero UI: The End of Screen-based Interfaces and What It Means for Businesses,zero ui,https://medium.com/@inkbotdesign/zero-ui-the-end-of-screen-based-interfaces-and-what-it-means-for-businesses-11da8150d331,"Zero UI: The End of Screen-based Interfaces and What It Means for Businesses
The television introduced us to the world of screens for the first time.
Today, not a minute goes by without interacting with a screen, whether it is the computer or the mobile.
Soon, however, we will be entering the era of screen-less interaction or Zero UI.
A lot of devices and applications such as Google Home, Apple Siri, and Amazon Echo are already engaging with their end-users without the need for a touchscreen.
What Is Zero UI?
In simple terms, Zero UI means interfacing with a device or application without using a touchscreen.
With the increasing proliferation of the Internet of Things (IoT), touchscreens will eventually become out-of-date.
Zero UI technology will finally allow humans to communicate with devices using natural means of communication such as voice, movements, glances, or even thoughts.
Several different devices such as smart speakers and IoT devices are already using Zero UI.
As of 2018, 16% of Americans (around 39 million) own a smart speaker with 11% having an Amazon Alexa device, while 4% possess a Google Home product.
Zero UI is fast becoming a part of everything, from making a phone call to buying groceries.

Components of Zero UI
Tech companies around the globe are using a variety of technologies to build Zero UI-based devices.
Almost all these technologies are related to IoT devices such as smart cars, smart home appliances, and smart devices at work.
Haptic Feedback
Haptic feedback provides you with a motion or vibration-based feedback.
The light vibration felt when typing a message on your smartphone is nothing but haptic feedback.
Most smartwatches and fitness devices use this technology to notify the end user.
Personalisation
Some applications also eliminate or minimise the need for a touchscreen with the help of personalisation.
Domino’s “Zero-Click Ordering App” relies on the consumer’s personalised profile to place an order without requiring a single click.
If you already have Dominos Pizza Profile and an Easy Order, the app will place your order automatically in ten seconds after opening it.
You can also open this app using your smartphone voice assistant such as Apple’s Siri.

Voice Recognition
Speaking of Siri, the voice search and command itself is a component of Zero UI.
Cortana, Google Voice, Amazon Echo, and Siri are a few examples of voice recognition-based Zero UI applications.
This technology allows a device to identify, distinguish and authenticate the voice of an individual.
That’s why it has found applications in biometric security systems.
Glanceability
Face recognition is also turning into one of the most popular Zero UI technologies.
Most laptops and computers already use this technology to unlock screens.
However, Apple’s new Face ID feature takes it to a whole new level.
With this feature, you can unlock your iPhone with just a glance.
There is no need to hold your phone to your face.
It uses infrared and visible light scans to identify your face, and it can work in a variety of conditions.
Gestures
Gesture-based interfacing is available on a variety of smart devices.
For example, Moto Actions, a gesture-based interface on Motorola smartphones, allows you to carry out tasks such as turning on the camera or flashlight without unlocking the phone.
A bit more advanced example refers to Microsoft Kinect.
You can add it to any existing Xbox 360 unit.
It uses an RGB-color VGA video camera, a depth sensor, and a multi-array microphone to detect your motion.
Messaging Interfaces
Messaging interfaces such as Google Assistant are also a part of Zero UI.
If you already have a credit card or e-wallet added to your Google account, you can ask Google Assistant to order food with few text messages instead of wading through different food ordering apps.
Similarly, you can perform several other tasks such as calling, texting, and sending out emails using the Assistant.

Brands That Are Doing It
Several brands are trying to create a Zero UI experience for their customers.
While some companies are building intuitive apps, others are making their services available on Zero UI devices such as Amazon Echo and Google Home.
Uber
The Uber Alexa skill has been available on Amazon Alexa for some time now.
Once you have installed Uber Alexa skill on your Amazon Echo and set the location of your Amazon Echo in the Uber App settings, just say “Alexa, ask Uber to book a ride” to call a ride at your doorstep.
Starbucks
Just like Uber, you can also order your favourite Starbucks drink using Starbucks Reorder Skill on Alexa.
You can use the Google Assistant on your phone to order beverages and snacks from the Starbucks menu.
You will need to link your Starbucks account and select a payment method before making the first purchase though.

Apple’s HomeKit
Apple’s HomeKit allows you to control smart home appliances using your iPhone.
With a simple voice command, you can regulate the thermostat, or turn the lights on/off.
CapitalOne
The banking sector is also taking advantage of Zero UI.
CapitalOne, one of the leading banks in the US, also provides banking service through Amazon Alexa.
Registered users can check their bank balance or transfer money with voice commands.
CBS
CBS provides the latest news from all over the globe through Amazon Alexa.
You can also access CBS TV shows, movies, and other programs using this feature.
How Will Zero Ul Affect Web Design?
Although Zero UI may seem like the end of visual interfaces, that’s unlikely to happen.
Humans are visual beings.
We can retain visual information better and longer.
So, Zero UI is not going to eliminate screens.
However, it is going to change the present concept of web design forever.
Contactless management and predictive thinking are going to be the pillars of Zero UI design.
The present web design, being two dimensional, is mostly built on linear sequences.
For example, voice search is often carried out using simple voice commands such as “Call my father” or “Call an Uber” or “Tell me the score of yesterday’s Knicks game.”
However, putting them all together “Call my father then an Uber and then tell me the Knicks game score last night” will probably render your voice search query useless.
Web designing will have to evolve to handle such complex nature of human conversation.

What This Means for Designers — Understanding Data and AI
Zero UI brings the search and the purchase history (or behavioural data), two critical components of digital marketing, closer to each other.
In other words, designers will need to design systems that are intelligent enough to contemplate and create the content you need.
This, in turn, means, as a designer, you will need a thorough understanding of data analytics and Artificial Intelligence (AI).
You will need to design the UI for interaction with the device, not just the screen as it will not be limited to the smartphones or computers.
For example, let’s say you have to build a thermostat that can sense gestures.
However, there are dozens of ways a user will tell it to increase the temperature.
That’s where data analytics and AI comes in.
The more you know about your consumers’ psychology and behavioural patterns, the easier it becomes to design a Zero UI device.
What Zero Ul Will Mean for Businesses
In a world of Zero UI, your business will rely on providing the right recommendations at the right time.
You can’t afford to wait for the customer to initiate searches themselves.
Your business needs proactive inspiration.
Data
You are probably already collecting tons of data for your business.
You will need to continue gathering this information for Zero UI development as well.
However, the new extended data structure will require various technologies including machine learning, AI, IoT, data analytics, and the Blockchain to work together.
As a result, it will mostly be collected and controlled by tech giants such as Amazon and Google.
So, small and medium businesses will have to form strategic alliances or register themselves with these brands to get this data.
Context
You will need to understand the meaning of what users are attempting to find.
Most people start their search with a simple term, but they want more information about it.
For example, if a user frequently orders Chinese takeout, his query for “restaurants near me” should list Chinese restaurants at the top.
This is automatically inferred context.
So, whenever someone uses a phrase related to your product or service, your system should automatically initiate the next steps to guide them to your brand.
Design
As mentioned earlier, your business will have to move away from the linear web designing process.
Your prospects can take any of the channels in the extended data structure to reach your brand.
Your system must be ready to gauge most of this incoming traffic.
For example, a query for “nearest pizza shop” can come from a smart vehicle or a smartphone or even a smart home.
Can your web design handle it?
Content
The content creation process will also become more dynamic than ever.
You will need to produce content in conjunction with the changes in the consumer data.
Local SEO will play a crucial role in Zero UI as most people will be searching for places and services in a given area.
For example, you can think of promoting location-based deals to attract more foot traffic to your store.
With voice search taking over text, natural language search will also take precedence in your SEO.

How to Optimise Your Site for Natural Language Search?
When it comes to natural language search, voice search is the most prevalent way of communication.
Nearly 70% of requests on Google Assistant are natural language searches, and not the typical keywords people type in a web search.
Optimising your website for this type of search is the first step towards creating a Zero UI experience for your prospects.
Structure Your Content
Usually, natural league queries are in the form of questions instead of phrases.
If you are looking for a burger shop, you will ask “Where is the best burger shop in Chicago?”
According to SEO Clarity, “how,” “what,” and “best” are the top three keywords used in these search terms.
So, you need to optimise your website content for these keywords.
Try to include them in your blogs, articles, and Q&A pages naturally.

Use Long-Tail Keywords
Conversational phrases also comprise long-tail keywords as people often use more words when they are talking.
So, you will also need to optimise your website content for longer and more conversational phrases and keywords.
While you are at it, try to maintain natural language throughout your content marketing efforts.
Add your Website/Store to Google My Business
Adding your website to Google My Business (GMB) helps attract more foot traffic through mobile searches, especially the “near me” searches.
Add your phone number, physical address, business hours, user reviews, and store or office location to GMB.
Zero UI and the Problem of Privacy
In its attempt to provide users with a seamless experience, Zero UI may also lead to serious privacy and security issues.
The ability to order an Uber or a pizza from your Amazon Alexa may seem like a magical experience, but that requires sharing your personal information such as credit card details with not only Amazon but also third-party sites.
Meanwhile, IoT devices are providing cybercriminals with new ways to steal personal data.
On October 12, 2016, Mirai, a self-propagating botnet virus brought down the internet service on the US east coast.
The virus infected poorly protected internet devices. Similar attacks are taking place as we speak.
However, using smart home appliances can also lead to the invasion of your privacy.
Recently, Amazon Echo recorded a family’s private conversation and sent it to a random individual in their contact list.
The victim, a Portland, Oregon, resident felt so insecure that she unplugged all the devices in her house.
Though rare, incidents such as this raise some serious privacy and security concerns.

Future of Zero UI
At the moment, the future of Zero UI looks bright despite the security and privacy concerns.
In the coming years, it will take the smart home and smart city concept to the next level.
For example, planning a trip to a shopping mall in 2030 will require you say the magic words “Plan a trip to the XYZ mall tomorrow at 7 PM” to Google Home or Amazon Alexa.
The system will take care of everything from planning a travel route to paying for the car parking in advance.
Coupled with AI and advanced data analytics, Zero UI devices will be able to form an empathetic and a personalised relationship with your consumers.
As human-machine interactions become more natural and human-like, it will open new opportunities for digital marketers.
In Conclusion
The Zero UI concept aims to make every community, marketplace, on-demand service, e-commerce site, and mobile application more interactive.
However, this is going to open new doors for marketers and designers alike.
Hopefully, this comprehensive coverage of the said topic will prove helpful in understanding its magnitude, consequences, and the opportunities it will create.
If you still have doubts, let us know in the comment section. We will get back to you as soon as possible."
8,Conversational AI is Eating the Web,zero ui,https://medium.com/@billrice/could-the-web-as-we-know-it-disappear-f8b5fed5cb7c,"Could the web, as we know it, disappear?
Is it possible that beautifully frustrating browsers and websites disappear? Could these essential tools of daily life and work become technology footnotes alongside Usenet, Gopher, IRC, DMoz as antiquated interfaces to all the Internet’s amazing treasures?
This future may seem hard to imagine considering most of us grew up experiencing the Internet by browsing websites inside of web browsers. But, believe it or not, the browser (1993 — Mosiac) and hyperlinked websites (1989 — HTTP) are relatively new ways to get information from the Internet. So, it’s not inconceivable that we’re about to transition into a new user interface.

“closeup photo of eyeglasses” by Kevin Ku on Unsplash
If you’re curious about what is to come, you’ll have to monitor a variety of discussions. Real innovation in user interface and experience (UI/UX) design is embedded in several related technology trends — Zero UI, Conversational AI, AI Assistants, Conversational Agents (Chatbots).
I like the term Conversational AI because it best describes the underlying technologies that will ultimately enable this new UI/UX. To understand my prediction for Zero UI in the future, let’s better define this Conversational AI domain.
What is Conversational AI?
Conversational AI is the use of messaging, voice, and assistants to communicate with computers in a way that creates personal user experiences. In simpler terms, it’s talking to computers in a natural, human-like, conversational style.
Of course, the challenge is computers aren’t naturally conversational. We have to teach them, which is where artificial intelligence (AI) comes into play. To make a computer conversational, we have to solve three technical challenges:
Natural Language Understanding (NLU) — Computers need to understand the user’s request (intent).
Natural Language Processing (NLP) — Computers need to process that intent into a task to retrieve the appropriate response.
Natural Language Generation (NLG) — Computers need to translate the response into something that makes sense to the user.
Each of these steps requires enormous amounts of analysis and learning (machine learning) fueled by a massive number of diverse conversations (training data set). This collection, analysis, and learning have only recently become realistic at the scale necessary to even get close to making computers conversational.
The ubiquity of mobile phones and the swift proliferation of devices like Amazon Echo and Google Home have made crowdsourcing these conversations, at scale, relatively inexpensive and straightforward. Combine this explosion in training data with nearly free data storage, brought on by cloud computing, and rapid advances in machine and deep learning and you have a perfect storm. Conversational AI, as a superior interface, is becoming a reality.
What is the web?
It turns out that the web is an inferior version of Conversational AI. Think about our current version of the web. To give you reasonably intuitive access to information on the Internet, we force designers, developers, and users (people) to do all of the natural language processing.
We build websites, informed by what we think are the most common use cases — workflows that users will use to get to what they want from our sites.
(Secret Insider Sidenote: Ironically, that’s not really how we design websites. Truthfully, we design them to “funnel” you to where we want you to go. All along the way coaxing you to do what we want you to do.)
Here in lies the everyday frustrations of using a web browser and navigating websites. This current UI/UX paradigm requires users to find what they want by interpreting what the designer thought when they designed the site. Not ideal.
Why is Conversational AI is a Better UI/UX?
One persistent theme in technology innovation is that users, especially power users, tend to fill the gaps and inform creators on how to make their inventions more accessible. Conversational AI is in this phase and swiftly advancing because of rapid user preference for this kind of user interface.
Users, talking to their mobile phones and marginal conversational agents, are crowdsourcing a massive repository of incredibly diverse training data.
Meanwhile, users and enterprises, over the last couple of decades have sourced and published nearly the total of all human knowledge on billions of websites. The vision of the Library of Alexandria has been realized in the Googleplex. Google (mainly) is indexing, organizing, optimizing, and learning without pause from publishers and searchers. This index has created another massive set of training data.
But, these two activities are not yet ideally interfaced. Web users are still using old interfaces that force them to make compromises to the less conversational computers.
For example, if you go to find something on the Internet today you probably still break your search down into keyword fragments. You’re subconsciously trying to think like the computer to get the most relevant response. Then, you go to a variety of websites and hunt and peck around using a combination of navigation cues and guesses to try and find what you need.
The first process, search via Google, is getting much closer to conversational AI. I can type in a question and typically get a pretty reasonable list of potential answers and often time a direct summarized response. However, the second process, giving users answers to their questions on a website is a marginal experience even on the best-designed sites.
The problem is understandable, a personal blog, small business website, or even an enterprise website doesn’t have enough conversations (traffic and interaction data) to sufficiently train its human designers to understand, process, and generate natural responses to their customers’ visits.
So, the question becomes: Is it necessary to put this burden on websites and their owners? After all, that responsibility exists only because the web browser is the default user interface.
Removing the limitations of current web user interfaces is where Conversational AI starts to get interesting as a superior web UI/UX.
Conversational AI platforms and tools are giving enterprises, and even small businesses access to powerful NLP and machine learning at a technically accessible level for the average programmer or even business analyst. As this challenge to deploy bar increasingly lowers, adding this conversational layer to your website or application is going to become expected.
The added benefit to the business is also compelling. If I can converse with your website — and it works — then you can focus more resources on producing valuable content and less on organizing it for customer discovery.
How Conversational AI will make the web disappear
It’s already happening.
Navigation is disappearing from websites, reduced in many cases to a hamburger menu with a few obligatory links (i.e., contact and about pages). Websites, and the people that design them seem to be assuming that Google search is the user’s preferred navigation, especially for deep pages on our site.
Think about your behavior.
Here are a couple of the typical behaviors I witness in analyzing people asked to navigate the web.
First, and probably the most common is a user asks Siri or Google Now a question in hopes of getting a direct answer or at least a relevant list of web pages that might help.
Second, if I happen to be sitting in front of a desktop or laptop, I’ll type that same question into the search bar (formally known as the URL bar — how fast we evolve) in my favorite browser, which, interesting enough, will produce the same result. Google or Bing attempts to give me a direct answer along with a list of other possible direct answers, along with a relevant list of web pages.
Behind each of these scenarios is Conversational AI.
The approaching reality is that web users don’t need our weak attempts to anticipate their needs with human-crafted user interfaces and experiences. Very soon, it will be less critical to design navigation and intuitively organize our websites. Instead, we will just load up great content and let AI agents arrange, sort, interpret and talk to the humans.
Before I leave you and to tease my next article, could web designers evolve into the intelligent designers of Conversational AI experiences? Think the eerie hidden corridors of Westworld."
9,"Zero UI, Alexa and Google Home; the next era of interaction? or a massive hype?",zero ui,https://medium.com/@clowster/zero-ui-alexa-and-google-home-the-next-era-of-interaction-or-a-massive-hype-f10c40eab87,"The year is 2017. From CES, the launch of Samsung Bixby, to that viral video of a toddler asking Alexa to play digger digger, voice user interfaces (VUIs) have been hailed as the future of interfaces.
A Place For Voice
Voice works for some use cases: in-car experiences and home automation are prime examples. It’s also great for questions with absolute answers and short, information based transactions.

Image via Rapid API
Take the example of the image above, getting Alexa to tell you the answer would probably take 2–3 seconds maximum. The alternative would be to find your phone, unlock it, open your browser, tap the search bar, and type in “What is hello world in German” wait for the page to load and see the results load; this would take at least double the time for you to just ask.
For this test, a voice-only UI wins, but are these standalone home assistants the solution to our impatient, busy lives? Not really.
When voice only UIs become a problem
Compared to GUIs, voice-only UIs create a massive cognitive load on the user; instead of having all the options in front of them, they are have to remember the options at every given stage of the user flow. Yes, the assistant or system may be able to give the user options at each stage of the process, but will the user be able to retain the information given?
What’s easier, remembering a shopping list told to you over the phone? Or having the list in front of you?
Once a non-binary choice comes into the equation, voice-only UIs fail tragically.

Do you remember of that?
A bit of theory
One of the most influential theories of cognitive psychology is the work of George Miller, stating that 7 is the magic number of working memory capacity. In other words, we can only retain 7 (plus or minus 2) pieces of information at any given time. So does that mean we can remember 5–9 choices? Well, technically yes, but in reality, no. Those seven “memory slots” are not for exclusive use of Alexa. As these new home assistants do not have a screen, the user would probably be doing something else at the same time. Looking at the cobweb on the ceiling? One slot. Thinking about how hungry you are? another slot. Loud neighbours? One more. We also have to include things that run at the back of our minds. Realistically, we don’t have the capacity to retain that many choices, let alone remember the right prompts to continue with your query or action.
Screens + Voice = Win
The irony is that if voice UI wants to become mainstream, it cannot be zero UI (except for simple, linear journeys with absolute answers). It has to be used in conjunction with GUIs to create the optimum experience. We don’t have to look that far to find good examples of this, both Apple’s Siri and the Google Assistant on phones are a mix of voice and graphical interfaces. However, these only use the voice aspect as the input and don’t normally allow for a full voice story.
Adding to the existing G(V)UIs
Apart from just using the GUI to display results, the visuals can also be used to prompt further user input in longer, non-linear stories. A mockup that I’ve recently been working on, was inspired by the little prompt bubbles seen on Facebook’s Chatbots and Google Assistant.
For this particular project, we looked at a journey with multiple branches. We found that displaying the options to the user, reduces the demands on them and allowed them to see where they are in the journey. In addition, it provided them with an option to step back to the previous set of options.

The visual part of the GVUI interface through a journey.
It’s common that first time users approach emerging technology with some doubts. We see many people in our lab completely blank when asked to speak to our assistants. Having the reassurance that they can go back and see their options made them feel more at ease. Furthermore, when given the blank slate of “ask me anything”, users often freeze and don’t know where to start — giving them a starting point and some direction is key.
when given the blank slate of “ask me anything”, users often freeze and don’t know where to start.
Artificial Intelligence cannot really understand us (at the moment)
As much as Sci-Fi and glossy product launches would like us to think that artificial intelligence powered interfaces can answer all of our questions, the reality is we are still quite far off from machines completely understanding what humans mean in context. For example, a sustained conversation with Siri is still not possible beyond a few interactions. In other words, most automated conversational UIs require huge UX effort to guide the user down the path of possibility.

Artificial Intelligence is not at the point where it can truly understand us.
Bottom Line
Voice is faster and more intuitive when the interaction between the user and the system is linear, short and binary, such as turning on the lights or asking for currency exchanges. However, once the journey branches out or if the interaction has many steps, a voice-only UI will not suffice, even when machine learning is used to predict user intent.
A Graphical + Voice User Interface (GVUI), with some careful UX crafting will allow users to go through more complex journeys, by prompting them and allowing them to see where they are in the journey. We need to take a step back from the Zero UI hype, and go back to our roots of GVUIs, the stomping ground of Siri and the Google Assistant on phones."
10,Zero UI — Introduction to new way to interact,zero ui,https://medium.com/@ShahnazShahJaha/zero-ui-introduction-to-new-way-to-interact-6942d1b3bb32,"Let me clear the dust from outset, neither this is a browser nor a library to build the interface. This is simply a new approach to delight your customers as this helps them to complete their actions while doing something else as well (simply multitasking).
Recently, I lead the team to build the android app prototype that practically doesn’t have any interface and totally voice based.
Initially, we were quite skeptical about our approach to complete the entire purchase path without any interface —

Purchase Path
Our target audience are —
Person driving a car wants to place an order
Parent feeding their baby and wants to complete an action
A millennial teen stuck with headphone and doesn’t want to look at the screen
This list will have more and more entries in future. I have no hesitation to deduce that soon (in next 4–5 years) browser will be out of market and screenless apps will have more footprint than UI based. Designers will not talk about responsive design but will emphasis on #responsivevoice. Mouse (not the living one) will be part of technology museum and every device will be able to listen and respond.
Current timeline shows so many prototypes and experimental apps, but soon there will be standardized and useful options available for users. Google and Amazon are the frontrunners providing the framework and platform; but they are more focused to promote and boost their offerings rather than offer it for general purpose.
There is a huge vacuum for neutral players to develop a platform/framework or programming language to attract more developers. Hope someone like James Gosling, somewhere is sipping his favorite coffee and designing new ‘Java’, to foster more independent innovations in this #zeroUI bubble."
11,Voice User Interface (VUI) — A Definition,vui,https://medium.com/@Nikita_Tank/voice-user-interface-vui-a-definition-6e6b973c0ad6,"The Switch to Voice User Interface
Each decade we have seen a new form of interaction between human and computer. If I go a little bit back in time, we were using mouse and keyboard to communicate or to talk with a computer. Then we started using touchscreen laptops and smartphones where there is no need for keyboard or mouse to interact with a device. Nowadays, laptops and computers have inbuilt VUI to communicate with a computer.
Why VUI?
Voice User Interface allows a user to interact with computer or mobile or other electronic devices through speech or voice commands. Voice or speech is an input/output for any VUI device. VUI is an interface of any speech recognition applications. Many Voice User Interface devices and voice examples are available on the market.
Amazon Alexa, Echo Dot, Google Home, Google Mini, Siri, Cortana, and the Google Assistant are the examples of VUI devices which take voice /speech as an input and returns in the same manner. People use VUI device in their daily life one or another way.
Voice Examples using the Alexa Tool
1. The Capital One skill allows you to check your credit card balance or make a payment when one is due. This is secure: The skill performs security checks and requires you sign in using your username and password. Then, when you open the skill, you must provide a four-digit code to confirm your identity. Just be wary of who is around when using the skill — anyone who overhears you say your key can access your banking or credit card info just by asking Alexa.
2. You can use IFTTT to push additions to your Amazon To-do List to Google Calendar, or you can use the Quick Events skill. Say something like “Alexa, tell Quick Events to add go to the grocery store tomorrow at 6” to add an event to your calendar.
3. If you wear a Fitbit tracker on your wrist, you can enable the Fitbit skill. With this skill, you can ask Alexa about your progress or how you slept the night before. Before you can use the skill, however, you will need to link your Fitbit account by going to the skill page for Alexa and linking your account.
I have mentioned a few skills of Amazon Alexa here, but you can ask about the weather, education, population and many more things. You can even play games! Amazon Alexa device offers Akinator. It can read your mind and tell you what character you are thinking about, just by asking a few questions. Think of a real or fictional character and Akinator will try to guess who it is. This is one of best games offered for Amazon Alexa. If you’re interested in creating your own Alexa skill, check out our paragraph on voice user interface design below.
VUI Device
Google Assistance(Android), Siri(Apple), Cortana(Microsoft) are available in smart device or laptop. Their functionality is almost same as Amazon Alexa. While Google Home, Google Mini, Amazon Alexa, Amazon Echo are the devices available in the market.
Voice User Interface Design
Does VUI sound very exciting to you? If you enjoyed the voice examples, I’ll bet you’re looking to get started right away. Some of you may be experienced developers while others have recently interacted with a Google Home or Alexa device and are looking to build your own bot. Look no further! Enter Botsociety.
Botsociety is a conversational design tool that offers support for everything from Facebook Messenger and slack bots, to Google Assistant and Alexa. With Botsociety, you can effortlessly create a voice user interface design that will be impactful and hold real value. You can preview the paths and different ways a conversation between users and the bot you wish to build will take place."
12,Everything You Should Know About Voice User Interface Design in 2018,vui,https://medium.com/hackernoon/everything-you-should-know-about-voice-user-interface-design-in-2018-2384451f2f5b,"Voice User Interface (VUI) has more and more effect in our daily lives. VUI designers should create a voice user interface that can provide a better user experience.
Language is a communication method unique to human beings. We are now in the era of rapid development of artificial intelligence(AI) and will inevitably liberate our hands through the use of Voice User Interface (VUI). Voice User Interface is a major trend in 2018 and has become a part of our daily lives. VUI is used in smartphones, smarthomes, smart TVs, and a range of other products. All over the world, people are getting used to talking to Siri, Google Assistant, Cortana, or Bixby.

With the rapid development in VUI, designers need to create a product that provides superior user experience. That is, if they want to get ahead of the competition.
1. What is Voice Interface Design (VUI)?
“A voice-user interface (VUI) makes human interaction with computers possible through a voice/speech platform in order to initiate an automated service or process.” VUI design focuses on the process of interaction design for the user and the voice application system. VUI is a user-facing interface, so it is vital that it meets the needs of the user.

2. VUI is your personal voice assistant
VUI will become your personal assistant. The development of new technologies will make it easier for designers and developers to provide tailored digital experiences. VUI will not only be your personal assistant, understand your current needs, but it will also predict your future needs. It involves all aspects of your life, even in areas you can’t imagine.

3. What is the Voice User Interface Designer’s job?
There is not much difference between VUI design and “traditional” web design. However, there are 3 main aspects VUI designers should focus on:
Conduct user research to understand who and where (environment) the user is. Designers should also understand the entire communication process between the system and the terminal device from beginning to end;
Designers should be responsible for product prototype design (Mockplus is a great example of a prototype design tool) and product description, describing the interaction between the system and the user.
Designers should consider the requests that need to be processed when they describe the interactive behavior between the system and the user. Analyze the data to understand where the system has gone wrong. At the end of the day, designers need to continuously check and improve the system.

4. 6 Basic principles for designing the Voice User Interface (VUI)
Reduce cognitive overload for a better user experience
Humans store audio as short-term memory. It is impossible for people to remember a lot of new information at once, so don’t overtax short-term memory.
The voice device must understand the user’s main needs accurately and provide answers quickly.
For example, the speech system asks the user: “What are your main symptoms”. The user answers: “A fever and a cold.”
The system must understand that the user is talking about about two symptoms. It must then provide solutions for both symptoms.

Information and user interface components must be presented to the user in a perceptible way.
Create content that can be presented in different ways (such as a simple layout) without losing information or structure.
Provides different ways to help users navigate, find information, and determine its location.

5. Graphical User Interface (GUI) and Voice User Interface (VUI) make a better product
The content of the GUI is mainly graphics and text, while the content of the VUI is mainly text. People interact with the GUI through clicks and gestures, while people interact with the VUI through dialogues. The VUI must understand what people are saying, then give the correct response.
The VUI combined with the GUI helps simplify the entire navigation process and the selection and confirmation operations.

Conclusion
Voice User Interface design is a promising brand new field which provides solutions through voice control. With VUI and GUI combined, human-machine interaction can be enhanced and streamlined using input via facial expressions, gestures, and audio."
13,Voice User Interface Insights,vui,https://blog.prototypr.io/voice-user-interface-insights-686fe441e425,"Imagine you have a new project in UX/UI and it is related to Voice User Interface (VUI) and you don´t know much about it. Here some info to help you with it!
VUI Stats
Why is voice a trend?
Gartner says it, so it is a trend, no discussion: You can see it at the top left on the curve: “Virtual Assistants”.

How do people know about Voice Assistants?
You can see at Statista See here

What do people use Voice Assistants for?
See original here.


Virtual Assistant Smartphone Penetration
original here.

Digital Assistant Market Expectation
Source

Trend studies about Voice User Interaction.
Some good slides about VUI:
Speak Easy Global Edition

Voice assistants & smart speakers

The Rise of Voice

#AllAboutVoice A new era with new Business Opportunities

Informe Altavoces Inteligentes

IA for VUI: The Blueprints of Conversation

Technical area for VUI
Where to learn VUI specifications from different platforms?
Now, let´s move to the technical part, so we can have a look at what the big ones recommend on VUI. These are Amazon Alexa, Google Assistant, Windows Cortana, Apple Siri
Amazon, Alexa Voice Service (AVS)

There´s a place for programmers at Platform for Alexa Voice Service (AVS)
And for Designers, The Alexa Skills Kit. There´s even an online seminar: Situational Design: How Designing for Voice Differs from Designing for the Screen.
The Voice Design Guide is very intuitive, points on Design process, what users say, how Alexa works, Design checklist, and a glossary. I recommend doing the whole tutorial. You can watch these videos as introduction:



From the UX point of view, you work as in any other project, so you have your personas, your user journeys, your user flow and your intents. Understanding intents is critical if you want to develop Alexa Skills. If you don´t nothing about developing you can always use StoryLine which is the same with a graphical interface.
Skills work same as Apps, you must have a developer account and once you have your skill ready, upload it to Amazon Alexa Skills platform. Then you can be approved or denied. The process is strict.
Google Voice Assistant
For Google the main concept of Conversation Design is about Actions

There´s even a UI toolkit for Sketch. The site for Coversation Design has a step by step tutorial in order to develop your user personas, flows and dialogs. In fact, Google uses Dialogflow, Dialogflow is Google’s natural language understanding tool for building conversational experiences, such as voice apps and chatbots, powered by AI.

Here´s the guide if you want to get started.
The key point in Google Voice Assistant for me are the conversational components: all the things that make up a prompt, like acknowledgements or questions. They also include chips, which are used to continue or pivot the conversation. Prompts and chips are the core of the conversational interaction and should be designed for every turn in the dialog.
And then you have visual components, these include cards, carousels, lists, and other visual assets. Visual components are useful if you’re presenting detailed information, but they aren’t required for every dialog turn. Cards are a must.
Google/DialogFlow have a platform to make trials and tests and it works really good.
Cortana Skills Documentation
Although I have work with Alexa skills and Google Voice Assistant, I haven´t worked yet with this one, but do not understimate it.

Clearly the orientation is for developers and for bot building.
Siri, Human Interface Guidelines
You can take a look at the website, there´s a Siri Kit explaining about intents and topics.

The UX Point of view
I would recommend two soucres:
Nielsen Norman Group. They have several articles
Smashing Magazine: Good examples of people who has already worked in the area of VUI
More Resources
If you still want more:
There´s the Mobile Voice Usage Trends 2018 report from Stone Temple
The 2017 Voice Report by Alpine (fka VoiceLabs)
Conversational eCommerce by Capgemini
Here’s a useful list of lists: as many guiding principles as we could find, all in one place. List compiled and edited by Ben Sauer
Here´s a good compilation of nice tools to work with VUI"
14,4 Lessons Mr. Rogers Taught Me About VUI Design,vui,https://blog.soundhound.com/4-lessons-mr-rogers-taught-me-about-vui-design-d6f30105f64,"Like so many other American children, I grew up watching the television show Mr. Rogers’ Neighborhood. The show was created and filmed at my local PBS member station WQED Pittsburgh. It’s where Fred Rogers got his start in public television. Since his rise to fame, Fred has become a legend across the country, but especially in the Pittsburgh area where his likeness is memorialized by many murals and statues throughout the city. Every time I’m home, I’m reminded of the impact Mr. Rogers’ Neighborhood and Fred himself continues to have, on my life and that of so many others.

Since its wide release last year, the critically-acclaimed documentary film Won’t You Be My Neighbor? about the life and work of the late Fred Rogers brought this man and his teachings back into the limelight. Like me, many people reflected on how he became a role model for generations of children by addressing difficult subjects and aspects of human life with straightforward, honest messages.
In the 2018 biography The Good Neighbor: The Life and Work of Fred Rogers, Maxwell King chronicles the story behind “Freddish” — a concept created by two writers of the show. With great success, they honed Fred Rogers’s voice by following these rules:
“State the idea you wish to express as clearly as possible, and in terms preschoolers can understand.” Example: “It is dangerous to play in the street.”
“Rephrase in a positive manner,” as in “It is good to play where it is safe.”
“Rephrase the idea, bearing in mind that preschoolers cannot yet make subtle distinctions and need to be redirected to authorities they trust.” As in, “Ask your parents where it is safe to play.”
Rephrase your idea to eliminate all elements that could be considered prescriptive, directive, or instructive.” (i.e., ask): “Your parents will tell you where it is safe to play.”
“Rephrase any element that suggests certainty.” (i.e., will): “Your parents CAN tell you where it is safe to play.”
“Rephrase your idea to eliminate any element that may not apply to ALL children (as in, having PARENTS): “Your favorite GROWN-UPS can tell you where it is safe to play.”
“Add a simple motivational idea that gives preschoolers a reason to follow your advice”: “Your favorite GROWN-UPS can tell you where it is SAFE to play. It is good to listen to them.”
“Rephrase your new statement, repeating Step One (i.e., GOOD as a personal value judgment): “Your favorite GROWNUPS can tell you where it is SAFE to play. It is important to try to listen to them.”
“Rephrase your idea a final time, relating it to some phase of development a preschooler can understand (i.e., growing): “Your favorite GROWN-UPS can tell you where it is SAFE to play. It is important to try to listen to them. And listening is an important part of growing.”
After reading about Freddish last year, I was astonished at the parallels between the Freddish rules and the voice user interface (VUI) brand guidelines my company (Voxable) helps our clients define.
The three core components of VUI brands are:
Voice - The quality of the words a VUI uses
Tone - The way those words modulate for specific situations
Persona - The character that embodies the voice and tone
Similar to how the Freddish rules enabled different Mr. Rogers’ Neighborhood writers to come together to create a singular voice, tone, and persona for the show, well-defined VUI brand guidelines help a design team align to create a cohesive conversational experience for users.
The Freddish rules inspired me to devise an exercise for the conversational design workshops that Voxable delivers to our clients. Voxable’s Freddish exercise centers around a VUI design team reflecting on how the Mr. Rogers’ Neighborhood writers established the Freddish rules, then working to understand or define their company’s brand and collectively creating a set of rules to describe how that brand speaks. The rules defined in this exercise are different for every team as they reflect each company’s unique brand direction.
Regardless of client type or size, we’ve found every team wanting to create a cohesive VUI voice, tone, and persona benefits from these four Freddish-inspired tips:
1. Know Your Audience
Fred Rogers excelled at connecting with his audience because he understood how language affected them. Since his primary audience was children, he used simplistic, positive phrases to build trust and communicate his message. Fred Rogers knew that words mattered and respected the perceptiveness of children. The Good Neighbor describes Rogers’s attention to language:
His secretary Elaine Lynch remembered how careful he was with each word. When one script referred to putting a pet “to sleep,” Rogers excised it for fear that children would be worried about going to sleep themselves.
When building a VUI brand, it’s important for a design team to perform foundational research to understand the VUI’s audience just as Fred Rogers understood his audience. Talking directly with users to understand their needs, motivations, and emotions enables the team to create a conversational brand that resonates with those users.
2. Start With the Core Meaning
The first rule of Freddish is “State the idea you wish to express as clearly as possible…” which is a perfect place for VUI designers to start when creating a voice experience. VUIs often rely too heavily on conversational flourish which can get in the way of user understanding and usability. It’s essential for a design team to establish the VUI’s reason for existence, every user context the VUI will support, and the core meaning of prompts and responses the VUI delivers to users.
A design team should only concentrate on establishing the VUI brand after a clear VUI use case has been vetted. The initial VUI designs, complete with sample scripts and flow diagrams, inform what’s generated during VUI brand development. Because natural language is so integral to the structure of a VUI, a design team should leverage real users’ voice interactions to generate Freddish exercise examples.
3. Test the Language
Every Freddish rule is accompanied by a clear example of how that rule is applied to a phrase and affects the language. Examples are an integral component to any brand guidelines because they provide concrete direction as to how to appropriately implement the guidelines. Collaborating to create examples is an important activity for a VUI design team to ensure alignment to the defined guidelines.
A VUI design team should document a handful of examples across varying use cases for each VUI brand guideline they establish during the Freddish exercise. The process of negotiating language decisions amongst a team tests the defined VUI brand guidelines and helps team members grasp the nuances in language choices that affect the overall brand.
4. Be Authentic
Part of what makes Fred Rogers legendary is the authenticity in his message and desire to affect positive change through children. Mr. Rogers’ Neighborhood was crafted with a distinct purpose to teach children compassion for themselves and others and help them navigate the world better. Maxwell King describes Rogers’s essence in The Good Neighbor:
This is Rogers’s signature message: feelings are all right, whatever is mentionable is manageable, however confusing and scary life may become. Even with death and loss and pain, it’s okay to feel all of it, and then go on.
All aspects of Mr. Rogers’ Neighborhood aligned to that altruistic mission which is why the show was so effective and had such an impact.
When creating the VUI brand, a design team must consider foundational brand elements like mission, vision, and values. It’s essential to explore the ways those values manifest in conversational interactions, and how the foundational elements affect the VUI’s language.
Establishing words and concepts that are integral to the brand’s vision is helpful (For example, Mr. Rogers repeats the phrases, “Won’t you be my neighbor?” and “I like you just the way you are.”). Ensuring the VUI speaks authentically builds user trust and helps them understand the VUI’s capabilities most effectively.
When Mister Rogers sang, “Won’t you be my neighbor?” at the start of every show, he was inviting the kids watching to share their thoughts and feelings about topics that mattered to them. Nothing was more important to him than making children in the extended “neighborhood” feel not only secure, but also “heard,” especially on topics parents might have a hard time grappling with, like the death of the family dog or sibling rivalry.
— Maxwell King, The Good Neighbor: The Life and Work of Fred Rogers
Regardless of a VUI design team’s size, aligning various team members to craft on-brand messages using consistent language is a significant challenge. Similar to how Fred Rogers and the Mr. Rogers’ Neighborhood writers created the Freddish rules, defining a set of documented VUI brand guidelines along with concrete examples of their practical use helps establish the VUI’s voice, tone, and persona for more uniform, user-focused messaging. When I think about the care and intention behind Fred Rogers’s communication with children, I’m inspired to design and galvanize other designers to create similarly effective VUI experiences with users.
If you’d like to learn more about designing a successful VUI for your brand, take a look at SoundHound Inc.’s in-depth best practice guide.
"
15,Designing for the Ear,vui,https://medium.com/@AutomatedInsights/designing-for-the-ear-61629b205b06,"We are just now scratching the surface of incorporating voice user interface (VUI) with natural language generation (NLG) and the future landscape is an exciting one. By utilizing NLG, platforms such as Amazon’s Alexa can now have more human-like conversations.
The current state of VUIs and voice user experience (VUX) leaves a lot to be desired. Most of the focus when designing a VUI that delivers a enjoyable VUX revolves around paying a lot of attention to a user’s potential requests. Responses however, typically sound awkward and very unnatural. By leveraging NLG, responses can become more human like, become conversational, and lead user’s on a unique journey through a skill. This leads to an optimal VUX.
In January, Automated Insights and Amazon Alexa hosted the first conversational language hackathon, highlighting this new incorporation of NLG. Teams from around the country gathered at Automated Insights’ headquarters to teach Alexa new NLG based skills. Demonstrating cutting-edge integrations using Automated Insights’ Wordsmith platform and Alexa, projects analyzed company stocks, evaluated soccer plays, reported on a loved-one’s well-being, shared a child’s academic status, suggested television shows, music and concerts, and prepared a financial advisor for a client meeting.

As the technology continues to evolve, product managers, designers, and writers of voice interfaces interested in making new NLG based Alexa skills need to change the way in which they design those new skills. Quite simply, it is time to start “designing for the ear.” When setting out to develop a new skill which utilizes NLG, it is important to keep three things in mind.
Have a good user interface
Make it sound human
Keep it simple
Building a Good User Interface
Having a good user interface may seem like an easy tenet of designing a new skill for the Alexa platform. Often times, especially with a new kind of user interface where best practices are yet to be defined, this can be easily overlooked. It is essential to make functions discoverable.
Building Faith
As a user interacts with the device, the skill should offer suggestions that facilitate user discovery. Perhaps, including responses such as “Would you like to check your balance,” “Would you like to purchase more credits,” and the like offer a pathway to take a user deeper into the interface.
Building trust with the user provides a great user experience. This can be achieved by a subtle or implicit confirmation of the user’s request. Provide responses that incorporate the user’s original question. “Buy tickets for the new Star Wars,” a user may state. Responding with, “Star Wars: Rogue One is playing at 7:30 and 9, which would you like?” lets the user psychologically build faith in the skill.
Accounting for Question Variability
Likewise, it is important to allow users to ask questions in different ways. For instance, if a user is attempting to check the balance within an account and ask, “How many credits do I have,” a bad response would be “I didn’t understand that, you can say ‘check your balance.’”
By incorporating NLG into a new skill, the user’s seemingly vague question can be understood and appropriate responses can be utilized to guide the user through the depth of the new skill.
Making a Skill Sound Human
Sounding human seems like it should be easy, but it is incredibly easy to fall into the trap of providing only static responses. It is essential to adapt logically, providing insights rather than giving a user static information. For instance a bad response may look like, “You have 1000 credits. The average user has 600 credits.” Useful? Slightly. Human sounding? Not at all.
Providing responses such as, “You are doing great! You have far more credits than the average user,” instills a conversational, human-like tone when responding to a user’s request.
Responding in the same way every time sounds robotic. It’s important to naturally change word choice and sentence structure so that your responses don’t get stale. Using NLG, this is now possible and allows for personality to be incorporated right into the responses and provides the user with an incredible experience.
The typical, robotic sounding error messages can benefit greatly from the use of NLG. Instead of responding to a user’s command with a similarly structured, “I’m sorry, I did not understand your request,” the platform can now suggest possible answers to an unrecognizable command. This allows the responses to adequately teach and guide the user.
Keeping it Simple
Being an early adopter on the forefront of a new and exciting integration of technology is exciting, but it is important to keep it relatively simple. Just like in good writing, it is important to keep it short. Be concise, be consistent. Use fewer clauses, as having too many independent clauses within sentences makes it hard for a user to parse the device’s response.
This also introduces more instances for the text-to-speech engine to pause, making the conversation more awkward in tempo. Use simple, clear language. Don’t over complicate things with big words. Use language familiar to users, especially when your potential audience may include children. Simplicity is key."
16,Job Prospects for Voice User Interface Design (VUI Design),vui,https://medium.com/@raffaelarein/job-prospects-for-voice-user-interface-design-vui-design-dd214574a5ae,"Since announcing our Voice User Interface Design course built in collaboration with Amazon Alexa, several people have asked about the job prospects for Voice UI Designers, so I decided to do some research.
Being ready for the voice-first era is amongst the highest priorities for leading companies around the globe.
Because of this there is a huge need for specialized talent who are ready for this next big frontier in technology. Googling the term “careers voice user interface design” yields 319 jobs on Indeed, 569 jobs on LinkedIn, and another 325 on Glassdoor.
Looking a little further into it, I found that there are a gazillion other jobs around VUI that are not titled VUI, such as Siri Designer, Conversational Interface Designer, or Voice User Interface Engineer (which requires knowledge of UI/VUI as well as some coding such as C++). Often the job ads are simply titled ‘User Interface Designer’ and knowledge of VUI is then found in the job description.
Companies hiring Voice UI Designers come from all areas, but there is a strong trend in automotive, electronics, enterprise systems and telecommunications industries — no big surprises there! However, over the next few years, voice design will become the norm in many other industries and help us to efficiently carry out many online transactions. As VentureBeat put it: “Texting, making a phone call, troubleshooting a wireless problem, showing me pictures from my vacation, researching a complex topic — they are all voice-enabled today to some extent, but not exactly voice-friendly yet.”
“Amazon’s Alexa is everywhere at CES 2017” The Verge
The Verge reported that this year’s Consumer Electronic Show, one of the world’s largest trade show for electronics, was dominated by voice services such as Amazon Alexa. From LG’s voice powered refrigerator, to Ford adding Alexa into its F-150, to Huawei putting Alexa apps into its phones by default, voice is everywhere. And where manufacturers have not yet launched a voice powered product, you can certainly bet that they are working on doing so at lightning speed.
Still wondering if there are enough job prospects for Voice UI Designers?
To summarize:
Becoming voice-first is a top priority for companies today (instead of mobile-first).There is huge amount being invested into developing voice technologies, and a big need for talented designers who are ready to turn their hand to voice interactions.
Voice interfaces are currently being deployed into products at ALL major companies in the automotive, electronics, enterprise systems and telecommunications sector.
Specializing in VUI now will make you the go-to person for all things voice in your company and set you apart for at least the next decade

If you liked this article, give it some love :)
If you are interested in more information, I am currently working on putting together even more data and info on the job market for voice. Coming soon….
Lastly, here you can check out our new course, the first comprehensive and mentored course on Voice User Interface Design built in collaboration with Amazon Alexa."
17,Designing a VUI — Voice User Interface,vui,https://uxplanet.org/designing-a-vui-voice-user-interface-c0b3b9b57ace,"More and more voice-controlled devices, such as the Apple HomePod, Google Home, and Amazon Echo, are storming the market. Voice user interfaces are helping to improve all kinds of different user experiences, and some believe that voice will power 50% of all searches by 2020.
Voice-enabled AI can take care of almost anything in an instant.
“What’s next in my Calendar?”
“Book me a taxi to Oxford Street.”
“Play me some Jazz on Spotify!”
All five of the “Big Five” tech companies — Microsoft, Google, Amazon, Apple, and Facebook — have developed (or are currently developing) voice-enabled AI assistants. Siri, the AI assistant for Apple iOS and HomePod devices, is helping more than 40 million users per month, and according to ComScore, one in ten households in the US already own a smart speaker today.
Whether we’re talking about VUIs (Voice User Interfaces) for mobile apps or for smart home speakers, voice interactions are becoming more common in today’s technology, especially since screen fatigue is a concern.

Echo Spot is Amazon’s latest smart speaker that combines a VUI with a GUI, comparable to the Echo Show.
What Can Users Do with Voice Commands?
Alexa is the AI assistant for voice-enabled Amazon devices like the Echo smart speaker and Kindle Fire tablet — Amazon is currently leading the way with voice technology (in terms of sales).
On the Alexa store, some of the trendiest apps (called “skills”) are focused on entertainment, translation, and news, although users can also perform actions like request a ride via the Uber skill, play some music via the Spotify skill, or even order a pizza via the Domino’s skill.
Another interesting example comes from commercial bank Capital One, which introduced an Alexa skill in 2016 and was the first bank to do so. By adding the Capital One skill via Alexa, customers can check their balance and due dates and even settle their credit card bill. PayPal took the concept a step further by allowing users to make payments via Siri on either iOS or the Apple HomePod, and there’s also an Alexa skill for PayPal that can accomplish this.
But what VUIs can do, and what users are actually using them for, are two different things.
ComScore stated that over half of the users that own a smart speaker use their device for asking general questions, checking the weather, and streaming music, closely followed by managing their alarm, to-do list, and calendar (note that these tasks are fairly basic by nature).
As you can see, a lot of these tasks involve asking a question (i.e., voice search).

Smart speaker usage in the US according to ComScore
What Do Users Search for with Voice Search?
People mostly use voice search when driving, although any situation where the user isn’t able to touch a screen (e.g., when cooking or exercising, or when trying to multitask at work), offers an opportunity for voice interactions. Here’s the full breakdown by HigherVisibility.

Real-time traffic updates are becoming a lot easier while driving thanks to Google Assistant and Android Auto.
Conducting User Research for Voice User Interfaces
While it’s useful to know how users are generally using voice, it’s important for UX designers to conduct their own user research specific to the VUI app that they’re designing.
Customer Journey Mapping
User research is about understanding the needs, behaviors and motivations of the user through observation and feedback. A customer journey map that includes voice as a channel can not only help user experience researchers identify the needs of users at the various stages of engagement, but it can also help them see how and where voice can be a method of interaction.
In the scenario that a customer journey map has yet to be created, the designer should highlight where voice interactions would factor into the user flow (this could be highlighted as an opportunity, a channel, or a touchpoint). If a customer journey map already exists for the business, then designers should see if the user flow can be improved with voice interactions.
For example, if customers are always asking a certain question via social media or live support chat, then maybe that’s a conversation that can be integrated into the voice app.
In short, design should solve problems. What frictions and frustrations do users encounter during a customer journey?
VUI Competitor Analysis
Through competitor analysis, designers should try to find out if and how competitors are implementing voice interactions. The key questions to ask are:
What’s the use case for their app?
What voice commands do they use?
What are customers saying in the app reviews, and what can we learn from this?
Requirements Gathering
In order to design a voice user interface app, we first need to define the users’ requirements. Aside from creating a customer journey map and conducting competitor analysis (as mentioned above), other research activities such as interviewing and user testing can also be useful.
For VUI design, these written requirements are all the more important since they will encompass most of the design specs for developers. The first step is to capture the different scenarios before turning them into a conversational dialog flow between the user and the voice assistant.
An example user story for the news application could be:
“As a user, I want the voice assistant to read the latest news articles so that I can be updated about what’s happening without having to look at my screen.”
With this user story in mind, we can then design a dialog flow for it.

The Anatomy of a Voice Command
Before a dialog flow can be created, designers first need to understand the anatomy of a voice command. When designing VUIs, designers constantly need to think about the objective of the voice interactions (i.e., What is the user trying to accomplish in this scenario?).
A users’ voice command consists of three key factors: the intent, utterance, and slot.
Let’s analyze the following request: “Play some relaxing music on Spotify.”
Intent (the Objective of the Voice Interaction)
The intent represents the broader objective of a users’ voice command, and this can be either a low utility or high utility interaction.
A high utility interaction is about performing a very specific task, such as requesting that the lights in the sitting room be turned off, or that the shower be a certain temperature. Designing these requests is straightforward since it’s very clear what’s expected from the AI assistant.
Low utility requests are more vague and harder to decipher. For example, if the user wanted to hear more about Amsterdam, we’d first want to check whether or not this fits into the scope of the service and then ask the user more questions to better understand the request.
In the given example, the intent is evident: The user wants to hear music.
Utterance (How the User Phrases a Command)
An utterance reflects how the user phrases their request. In the given example, we know that the user wants to play music on Spotify by saying “Play me…,” but this isn’t the only way that a user could make this request. For example, the user could also say, “I want to hear music … .”
Designers need to consider every variation of utterance. This will help the AI engine to recognize the request and link it to the right action or response.
Slots (the Required or Optional Variables)
Sometimes an intent alone is not enough and more information is required from the user in order to fulfill the request. Alexa calls this a “slot,” and slots are like traditional form fields in the sense that they can be optional or required, depending on what’s needed to complete the request.
In our case, the slot is “relaxing,” but since the request can still be completed without it, this slot is optional. However, in the case that the user wants to book a taxi, the slot would be the destination, and it would be required. Optional inputs overwrite any default values; for example, a user requesting a taxi to arrive at 4 p.m. would overwrite the default value of “as soon as possible.”
Prototyping VUI Conversations with Dialog Flows
Prototyping designers need to think like a scriptwriter and design dialog flows for each of these requirements. A dialog flow is a deliverable that outlines the following:
Keywords that lead to the interaction
Branches that represent where the conversation could lead to
Example dialogs for both the user and the assistant
A dialog flow is a script that illustrates the back-and-forth conversation between the user and the voice assistant. A dialog flow is like a prototype, and it can be depicted as an illustration (like in the example below), or there are prototyping apps that can be used to create dialog flows.

A sample dialog flow illustrating the intent, slot and overall conversation
Apps for Prototyping VUIs
Once you’ve mapped out the dialog flows, you’re ready to prototype the voice interactions using an app. A few prototyping tools have entered the market already; for example, Sayspring makes it easy for designers to create a working prototype for voice-enabled Amazon and Google apps.

Sayspring is a tool that makes it easy to prototype an Alexa Skill or Google Home Action
Amazon also offers their own Alexa Skill Builder, which makes it easy for designers to create new Alexa Skills. Google offers an SDK; however, this is aimed at Google Action developers. Apple hasn’t launched their competing tool yet, but they’ll soon be launching SiriKit.

Amazon’s Alexa Skill Builder, where designers can prototype VUIs for Alexa-enabled devices.
UX Analytics for Voice Apps
Once you’ve rolled out a “skill” for Alexa (or an “action” for Google), you can track how the app is being used with analytics. Both companies offer a built-in analytics tool; however, you can also integrate a third-party service for more elaborate analytics (such as voicelabs.co for Amazon Alexa, or dashbot.io for Google Assistant). Some of the key metrics to keep an eye out for are:
Engagement metrics, such as sessions per user or messages per session
Languages used
Behavior flows
Messages, intents, and utterances
Practical Tips for VUI Design
Keep the Communication Simple and Conversational
When designing mobile apps and websites, designers have to think about what information is primary, and what information is secondary (i.e., not as important). Users don’t want to feel overloaded, but at the same time, they need enough information to complete their task.
With voice, designers have to be even more careful because words (and maybe a relatively simple GUI) are all that there is to communicate with. This makes it especially difficult in the case of conveying complex information and data. This means that fewer words are better, and designers need to make sure that the app fulfills the users’ objective and stays strictly conversational.
Confirm When a Task Has Been Completed
When designing an eCommerce checkout flow, one of the key screens will be the final confirmation. This lets the customer know that the transaction has been successfully recorded.
The same concept applies to VUI design. For example, if a user were in the sitting room asking their voice assistant to turn off the lights in the bathroom, without a confirmation, they’d need to walk into the sitting room and check, defeating the object of a “hands-off” VUI app entirely.
In this scenario, a “Bathroom lights turned off” response will do fine.
Create a Strong Error Strategy
As a VUI designer, it’s important to have a strong error strategy. Always design for the scenario where the assistant doesn’t understand or doesn’t hear anything at all. Analytics can also be used to identify wrong turns and misinterpretations so that the error strategy can be improved.
Some of the key questions to ask when checking for alternate dialogs:
Have you identified the objective of the interaction?
Can the AI interpret the information spoken by the user?
Does the AI require more information from the user in order to fulfill the request?
Are we able to deliver what the user has asked for?
Add an Extra Layer of Security
Google Assistant, Siri, and Alexa can now recognize individual voices. This adds a layer of security similar to Face ID or Touch ID. Voice recognition software is constantly improving, and it’s becoming harder and harder to imitate voice; however, at this moment in time, it may not be secure enough and an additional authentication may be required. When working with sensitive data, designers may need to include an extra authentication step such as fingerprint, password, or face recognition. This is especially true in the case of personal messaging and payments.

Baidu’s Duer voice assistant is used in several KFC restaurants and uses face recognition to make meal suggestions based on age or previous orders.
The Dawn of the VUI Revolution
VUIs are here to stay and will be integrated into more and more products in the coming years. Some predict we will not use keyboards in 10 years to interact with computers.
Still, when we think “user experience,” we tend to think about what we can see and touch. As a consequence, voice as a method of interaction is rarely considered. However, voice and visuals are not mutually exclusive when designing user experiences — they both add value.
User research needs to answer the question on whether or not voice will improve the UX and, considering how quickly the market share for voice-enabled devices is rising, doing this research could be well worth the time and significantly increase the value and quality of an app.
Understanding the Basics
What is a tangible user interface?
A tangible user interface is one that can be interacted with via taps, swipes and other physical gestures. Tangible user interfaces are commonly seen on touchscreen devices.
What is a speech interface?
A speech interface, better known as a VUI (Voice User Interface), is an invisible interface that requires voice to interact with it. A common device that has voice recognition software is the Amazon Alexa smart speaker.
What does an Echo do?
Amazon’s Echo smart speaker uses voice recognition software to help users perform tasks using voice interactions, even if they’re across the other side of the room. Echo smart speakers are powered by a voice assistant called Alexa, and VUI apps called “Skills.”"
18,Building Voice-Enabled Prototypes in Framer,vui,https://blog.framer.com/building-voice-enabled-prototypes-in-framer-3c1c3e14938f,"I’d like to share how to make prototypes that can react to voice in Framer. My goal is to give you the tools to make VUI (voice user interface) prototypes quickly and easily. Don’t have Framer yet? Download a 14-day free trial to follow along with this tutorial.
In Framer, you can make any layer—or more accurately any layer’s property—react to voice. By react to voice I mean react to the voice input volume that the microphone receives.
Here’s a simple demonstration of the voice-enabled prototype. You can play around with it, change some properties, etc. It’s all up to you. 😉

How it works
To make all this magic happen, I used Web Audio API to handle all the audio-related stuff.
If you’re interested in diving into the topic further, I added links to helpful articles at the end of this post.
Please note: the prototype only works in browsers that support Web Audio API, which means that you’ll most likely need to run it on desktop.
I also added some comments to the code of the demo prototype so you can better understand how everything works.
How to use the prototype
Next, I’ll explain how you can use the code from the prototype in your own creations.
Let’s take a look at the respondToVoice function.

As you can see, this function takes layer as a parameter. This layer here is what will actually be ‘reacting’ to voice input and changing in the ways you want. In the example below, we’ll be changing the scale and border width of our layer according to input voice volume.
Make note of the following lines of code:

What we’re doing here is setting the property value according to the current input volume. How does this actually work? Our layer has two states, inactive and active, and we need to animate it between the properties of these states. To do this, I’ve used Utils.modulate function:
1. It takes incoming sound volume as the first parameter,
2. and maps it from [0, MAX_VOLUME] range to value range between properties of active and inactive states. MAX_VOLUME is the maximum value of the input sound volume.
Here’s an illustration that shows what’s actually happening:

Mapping Input Voice Volume Value to Layer’s Property Value
In the case above, I am mapping the scale and border width of the layer according to current sound input volume.
Now that we have our layer properties set, what’s next? Animation! 🎉🤘

Above you can see a chunk of code, which animates our layer according to the input volume. I’ve added some animation options like curve and time so the transition looks more smooth and fluid. You can edit these values to get other interesting results.
After creating the animation for our layer, we need to loop it because the voice input we’re getting is continuous:

One small note before I wrap up the article: don’t forget that you can play around with different variable values in the prototype. I’d pay attention to MAX_VOLUME variable, if you want the layer to be less ‘sensitive’ and ‘responsive.’
So yeah, that’s it.
Don’t forget to check out the prototype and create awesome things, including a full-fledged prototype of a voice-enabled personal assistant (you just need to add SpeechRecognition API).
I hope that I’ve saved you a little bit of time."
19,"Voice User Interface Design: New Solutions to Old Problems
Why voice? Why now? We examine why voice technology is leaping from a decades-long slumber to mainstream success.",vui,https://medium.com/microsoft-design/voice-user-interface-design-new-solutions-to-old-problems-baa36a64b3e4,"In the past few years, voice user experiences have reached critical mass. Cortana. Alexa. Google.
Like many technologies that seem fresh off the presses (virtual reality, anyone?), voice user interfaces have been in the public consciousness for decades and in research circles even longer. Bell Laboratories debuted their “Audrey” system (the first voice controlled UI) in 1952, predating even Star Trek’s aspirational voice controlled computer!

Voice recognition systems have been a reality for more than half a century. (Photo: AndroidAuthority)
But speech scientists have long known the magic of transforming analog signals into digital meaning would take a scope of processing power that far outstripped its early humble roots. It is only recently, in the era of ubiquitous cloud computing, that consumers have access to enough processing power that their own voices can be heard and interpreted in real time.
A New Frontier
As user experience designers, we were most likely trained in crafting experiences designed for graphical output and physical input. I know that voice interfaces were far from the imagination of the academics of my time — during my senior projects, we were enamored of the Palm Pilot, and of handwriting input that foreshadowed today’s touchscreen UIs.
And yet, just as we adapted the skills we’d learned for the brave new world of input beyond the mouse and keyboard, so it is time for some of the designers of today to expand our skill sets to include voice input and the resulting output layers.

Touch and pen input, as seen in the Palm Pilot’s Graffiti input language, was once a quirky backwater of design exploration. Voice user interfaces have emerged from this phase.
In the last few years, a small but growing number of user experience designers have become full-fledged voice user interface (VUI) designers. Though it may seem a quirky specialty skill, so was mobile design 10 years ago. Voice user interface design will soon become a key strategic skill for a new generation of designers.
Our Oldest Interface
Humans have been developing the art of conversation for thousands of years. It is a skill adults draw upon instinctively, every day, for most of their lives.
Speech is one of the first skills we acquire in childhood — and one of the last we lose in our sunset years, long after our vision and motor skills begin to fade.
The deeply instinctive nature of speech presents specific constraints and new challenges. Our brains are fundamentally wired to interpret the source of speech as human. With few exceptions, we also expect a spoken response when we speak to someone. Thus, a device that speaks to us is tapping into a deep river of psychological adaptations, and subject to a set of assumptions a pixel-based UI will never encounter.
This is also why — at least for the moment — designing for voice user experiences is inherently different from conversational user interfaces, which at the moment are synonymous with text-based chat bots. Our thousands of years of speech-based perception and psychology don’t (yet) interfere with our ability to enjoy written conversations.
Today’s Voice UX: Command and Control
But let’s be super clear: the voice user experiences consumers are learning to use today are usually FAR from conversational. We are still in early days.
Though some players use “voice UI” and “conversational UI” interchangeably, in my observation there are no truly conversational spoken user interfaces yet. It’s still a bit more accurate to simply call Alexa, Google Home, and Cortana “natural language” voice control systems, but the distinction currently rests in the types of tasks we ask our voice-based assistants to complete. In fact, the key is the word “task”. These devices are all specialized for allowing customers to complete TASKS using their voice.
By way of example, the “natural language” way to turn off a light isn’t deeply conversational. You wouldn’t turn to your spouse and say, “Isn’t it a chilly night? I’m feeling a bit cold. Turn the thermostat up, won’t you?” (Unless you’re in an Oscar Wilde play, perhaps.) You’d probably just blurt out “Turn the thermostat down.” Less of a conversation, more of a request.
Furthermore, the way you complete simple tasks is almost always the same, regardless of emotion, mood, or context. Perhaps you might add “please” if you’re having a good day…
That doesn’t mean there isn’t quite a lot of complexity in getting this voice UI right — but as opposed to truly conversational UI, which paints in adjectives and nuance, command-and control voice UI deals in simplicity and robustness.
At present, voice user interface designers often spend a significant amount of design time focusing on how to help customers along when things go wrong. What happens if someone just says “Set an alarm” without specifying a time? Or if the system misheard “AM” instead of “PM”? By understanding how a voice interface can fail, VUI designers can find ways to turn those failures into eventual successes.
Adapting Your Design Instincts
My time working on VUI for Windows Automotive, Cortana, and Alexa gave me an appreciation for the differences in the design process between visual and voice-based UX, and a passion for sharing that knowledge as it was shared with me by some esteemed coworkers along the way (thank you Lisa Stifelman, Sumedha Kshirsagar, and Stefanie Tomko, amongst others).
As a result of that passion, I was honored to debut my workshop Giving Voice to Your Voice Designs at Interaction 17, a global design conference sponsored by the Interaction Design Association (IxDA).
In my #Ixd17 workshop, we started with a primer on key terms and concepts that relate to the speech science component of voice UI: how an analog voice “utterance” is converted into a digital system’s representation of a customer’s “intent”. Usually, this interpretation process spans multiple disparate but connected systems, which is why cloud computing smashed VUI doors wide open.
We explored common situational constraints and some simple guidelines to set them up for success in the final phase of the class, where we walked through an end-to-end design process with design deliverables for a third-party voice skill.

Walking workshop participants through the process of building an interaction flow for a 3rd party voice feature at #IxD17 — ironically, in a studio in NYC’s School of Visual Arts. Photo credit Malika Chatlapalli.
My participants really impressed me with their thoughtful questions that drove at some much deeper challenges facing voice UIs, like contextual awareness and “memory” over time. (A later article will deal with a few of these concepts.) These practitioners are a clear indicator that many of today’s designers can transfer their existing design skills to voice with some simple reframing and a bit of added subject matter expertise.
Voice Input Changes Lives
Even though current voice UIs are a bit more simplistic than the dreamers amongst us would like to see, we can’t lose sight of the very real benefits voice experiences provide, even simplistic, when done correctly.
The biggest and most impactful benefit voice user experiences provide is vastly improved accessibility. Looking for inspiration? Go read the reviews of the Amazon Echo. There are so many stories from mobility-impaired customers, vision-impaired customers, and customers with cognitive impairments about how the device has changed their life at home.
That’s the real quantum leap here. Voice user interfaces don’t solve any NEW problems… yet. But they solve existing problems in novel ways that significantly improve life for many individuals.
Setting alarms, getting answers to informational questions easily found on Wikipedia… yes, we could do these things before on our smartphones and our computers. But we had to turn our attention to a device to do so. And in that moment, we exchange a bit of our humanity, temporarily, for that exchange of service.
Voice UIs allow us to remain fully human in our interactions. They allow us to remain more connected to the other humans in the room. And these VUIs are life-changing for those who can’t easily adapt themselves for traditional computer use.
So the need for voice user experiences — even today’s crop of control-focused, less conversational UIs — is real, and these experiences change lives. You might not be replacing your existing experience, but even adding voice UI to extend an existing experience can have a major impact on your customers.
Find Your Own Voice
Inspired? I hope so. I challenge every designer to start looking at voice input as an important new way of connecting with customers. Are there unseen opportunities that could transform the way customers use your product? Even better, transform their lives?
And even if you’re a “traditional” designer, don’t be immediately intimidated. Many practitioners started just as you did, in a traditional visually-oriented world. Designers are inherently curious and mentally resilient. You can reframe your thinking with some new knowledge and a few adapted skills.
But there’s so much more to the world of voice user experiences. In my next post, we’ll talk about conversational user interfaces, a hot topic that surfaced repeatedly at Interaction 17. And we’ll talk about the how the voice user interfaces and text-based conversational user interfaces of today may soon begin to intersect.
May the voice be with you."
20,"How Voice User Interface is taking over the world, and why you should care",vui,https://medium.com/@goodrebels/how-voice-user-interface-is-taking-over-the-world-and-why-you-should-care-54474bd56f81,"User interfaces, or UI, are what allow us to interact with machines. They encompass everything, from those things we tend to take for granted, like keyboards and the screens of our desktop computers, to technologies that are more complex, like the movement based UI the Xbox Kinect is built upon. As new technologies are introduced, their adoption rate is entirely dependent on the development of efficient, human-centric UI design.

Different types of User Interface
Voice-user interface, or VUI, has exploded in popularity over recent years. VUI uses speech recognition technology to enable users to interact with technology using just their voices. Virtual assistants like Siri and Alexa have brought VUI into the mainstream, with corporate giants like Google and Sonos following their lead. Companies like Synqq and Nexmo have also taken advantage of VUI technologies in order to develop devices that allow for real-time translation and transcription. However, it’s virtual assistants which have really captured the corporate imagination.
VUI allows for hands free, efficient interactions that are more ‘human’ in nature than any other form of user interface. “Speech is the fundamental means of human communication,” writes Clifford Nass, Stanford researcher and co-author of Wired for Speech, “…all cultures persuade, inform and build relationships primarily through speech.” In order to create VUI systems that work, developers need to fully understand the intricacies of human communication. Consumers expect a certain level of fluency in human idiosyncrasies, as well as a more conversational tone from the bots and virtual assistants they’re interacting with on a near-daily basis.
We’re not in Westworld just yet but it’s clear that robotic assistants are here to stay. With that in mind, it’s important to understand all the potential pitfalls and positive opportunities that come along with this newly popular technology; so let’s explore the good, the bad, and the downright ugly side of VUI.
The Good
In order to create good VUI, brands need to understand their consumers, what they want from a virtual assistant and, more importantly, what aspects of interacting with Artificial Intelligence (AI) drive them to the absolute brink. There a number of benefits to a VUI that other user interfaces cannot provide, namely:
Personality and tone — with voice-based virtual assistants there is more opportunity for brands to inject a little personality and humour. Ask Siri to beatbox for you and she’ll do just that, call her by the wrong name and she’ll return “Very funny. I mean, not funny ‘ha-ha,’ but funny.” Google Home is totally au fait with pop culture references, from Star Trek to Sir Mix A Lot, as is Amazon’s Alexa. A more personable tone helps users to forgive those moments when virtual assistants are unable to complete tasks or answer questions that an actual human would have no problem with. A personable VUI also helps to increase brand affinity — you’re more likely to use a particular device or service if it’s more entertaining and ultimately more ‘human’ in nature.
Efficiency and convenience — VUI requires nothing other than a vocal command to carry out tasks or answer questions. No longer will amateur chefs be forced to scrub up to set a timer lest they smudge the screens of their very expensive smartphones. Now they can just ask Alexa and she’ll set it for them. Users can quickly check the weather forecast on their way out of the house, add an item to their grocery list without scrounging around for a pen, or skip a song on Spotify without lifting a finger. VUIs are more likely to exist within devices that are online and connected all day long, devices which may one day prove integral to our daily lives.
A more ‘human’ experience — accurate and efficient speech recognition software allows for a more a ‘human’ kind of conversation than can be had using any other device. We shouldn’t underestimate the value of human interaction; if you’ve ever had a long and tedious phone conversation with an automated customer care centre then you know it’s not always easy to get VUI right, or any kind of conversational user interface for that matter. However, with advancements in machine learning and natural language processing, interactions with brands and devices through a VUI are rapidly becoming more ‘human’ and less robotic. Implementation of a VUI-based technology demonstrates real commitment to a culture of human centricity.
The Bad
As discussed, the implementation of VUI is not without its roadblocks. Problems that arise during the conceptualisation and design process are often the result of an insufficient understanding of human psychology. In order to prevent issues around adoption and consumer frustration with VUI based devices, we should consider the following:
Discovery and retention — while Amazon has made it very easy for third party developers to come up with their own skills for the Amazon Echo, only 31% of those 7,000+ skills have more than one review, an indication of low usage. This issue is not unique to Amazon. In order to increase the rate of adoption, developers need to convey to users what they can and can’t do from the very start, while working all the time to ‘humanise’ the VUI systems that these virtual assistants are built on.
Understanding limitations — when a machine and a human are engaged in conversation, we need to adapt the way in which we communicate — humans aren’t used to following strict, unwavering linguistic law, especially when it comes to speech. If users understand from the very start the ways in which their device is limited, they’re less likely to feel disappointed when their assistant fails to complete a task or answer what might seem like a very simple question.
Natural Language Processing — we’re not currently capable of developing a VUI with an inbuilt, natural and complex understanding of human communication, not yet. Regional accents, slang, conversational nuance, sarcasm… some humans struggle with these aspects of communication, so at this point can we really expect much more from a machine?
Visual feedback — including an element of visual feedback helps to reduce the level of frustration and confusion in users who aren’t sure whether or not the device is listening to or understanding what they’re saying. Alexa’s blue light ring, for example, visually communicates the device’s current status e.g. when the device is connecting to the WIFI network, whether or not ‘do not disturb mode’ has been activated, and when Alexa is getting ready to respond to a question…etc.
The Ugly
In recent months, user privacy has become an even more contentious issue; following the Cambridge Analytica scandal, accusations that devices like Google Home and Amazon’s Alexa might be listening into private conversations and in the run up to the introduction of GDPR. Consumers are beginning to ask themselves; what’s being recorded, what’s being stored, and how is my private data being used by corporations? As privacy concerns continue to grow, trust in virtual assistants and in the IoT in general is lessening. In order to regain trust, developers and tech manufacturers must find ways to reassure their consumer base that their privacy is of the utmost priority, while at same time trusting that in time consumers will become more comfortable with the technology.
The Future of VUI
The aim of systems based on VUI is to provide users with a fully immersive experience; nuanced, complex and more human in nature. We’re not there yet, but advancements in technology which allow us to develop more complex algorithms and software more apt to simulate human behaviours, have opened up more opportunities for growth, both in the home and in the workplace.
Business — in late 2017 Amazon announced Alexa for Business, designed to help employees manage their schedules, keep track of their to-do list, dial into conference calls, and make voice calls on their behalf. Alexa for Business allows meeting attendees to control the equipment in their conference room using just their voice, notify IT about a broken printer, and recall the latest sales data or inventory levels. Within the working environment, the virtual assistant becomes the virtual secretary.
Smart Houses and the IoT — Most virtual assistants fall into the category of smart home devices. The more we focus on compatibility between smart home devices, the closer we get to a completely interconnected household. With Google Home, users control every Google device in their home via their Android Smartphone. Google Assistant can control more than 1,000 smart home products including kettles, microwaves, robotic vacuums and thermostats. With Apple’s HomePod users can even use a catchall phrase like “Good Morning” to turn on multiple smart home devices at once.
Find your voice
The main barrier preventing widespread implementation and acceptance of VUI systems is the fact that developers are forced to adapt the rules of communication in order to accommodate the limitations of the device. For users, these limitations can make interacting with these devices pretty tedious. Often there are so many different, possible answers to a query, virtual assistants find themselves in an uncomfortable loop listing endless options for Italian restaurants or breeds of dog. Many developers have attempted to remedy this by limiting the initial number of options presented to three, and then asking the user if they want to hear more. To improve the quality of the user experience, we need to develop machines capable of comprehending context, tone of voice, and attitude, with a better understanding of user intention based on historical data and the observation of previous patterns of behaviour. We need to go beyond a pre-programmed script.
“Interfaces to digital systems of the future will no longer be machine driven. They will be human centric,” explains Werner Vogels, Amazon’s chief technology officer, “We can build human natural interfaces to digital systems and with that a whole environment will become active.”"
21,A Real Conversation with Alexa,intelligent personal assistant,https://medium.com/@leobottary/a-real-conversation-with-alexa-a244d5643f0c,"Alexa is an intelligent personal assistant developed by Amazon, made popular by the Amazon Echo and the Amazon Echo Dot devices developed by Amazon Lab126. The Alexa device is capable of voice interaction, music playback, making to-do lists, setting alarms, streaming podcasts, playing audiobooks, providing weather, traffic, and other real time information, such as news and much more. Alexa can also control several smart devices using itself as a home automation system.

In November 2014, Amazon announced Alexa alongside the Echo. Alexa was inspired by the computer voice and conversational system on board the Starship Enterprise in science fiction TV series and movies. The name Alexa was selected because it has a hard consonant with the X making it recognizable with higher precision.
Several of our guests on the Year of the Peer podcast have talked about Artificial Intelligence (AI) and what it will mean for humans in this fast changing world. Many of us already engage the likes of Siri, Alexa, Google Assistant, and Cortana to answer questions and help us perform a wide range of tasks. So we thought, if who you surround yourself with really matters, then it’s not that far fetched to include AI devices in the mix and explore what they can really teach us — not only about how to properly engage them, but also about how we as humans can interact with one another more effectively. For these insights, we invited Alexa to join us as our special guest!
Enjoy what is largely a tongue-in-cheek podcast where, with the help of Alexa and my producer Randy Cantrell, we learn more about WHO she is and deliver some simple and important reminders. Alexa, welcome to the show!"
22,The future of driving — BMW NZ engages FaceMe to showcase its intelligent personal assistant.,intelligent personal assistant,https://medium.com/@uneeq/the-future-of-driving-bmw-nz-engages-faceme-to-showcase-its-intelligent-personal-assistant-352cdf5dda00,"The age of ‘smart cars’ has arrived, and the latest evolution of these is being showcased by BMW NZ who has engaged FaceMe to develop a digital human concept for its intelligent personal assistant."
23,An intelligent personal assistant for sales staff,intelligent personal assistant,https://medium.com/@Emily_Wood/an-intelligent-personal-assistant-for-sales-staff-82ee0cf6eb1f,"Artificial intelligence era is coming into our work and life. Whether Apple Siri, or Kasisto released this year, in addition to the existing intelligent assistant, we have to also assume the probability of realizing the idea of intelligent operating system in the movie “Her” in 15 years, gradually changing the way we live and work.
CtrlCRM helps sales staff manage personal connections by analyzing emails and phone calls among clients and employees. It gathers relevant information such as email address, activity calendar, contact name, phone number and company name. All these information will be automatically organized in order of frequency of contact, company or organization.
Product Features
Set up new program based on principles of neural network algorithms.
Control and supervise work process with support of mass data.
Report exception and provide solutions on advice of marketing model.
Improve professional skills of sales staff quickly by self-learning system with artificial intelligence."
24,Examples of Intelligent Personal Assistant Built on Top of Cirkus,intelligent personal assistant,https://medium.com/@vlad.k/single-user-interface-in-examples-df1791488e9d,"In this article I will identify some real examples of intelligent personal assistant built on top of Cirkus when used for dealing with any particular user’s situations. The screens shown below are not complete nor perfect; however, the main point is to illuminate core principles of a personal assistant. Cirkus is an open environment, thus everybody is capable to build a perfect assistant in the future.
The main purpose of a personal assistant is to enable a real person (user) to be in permanent contact with his online identity, representing his online presence. Everyone’s and everything’s activities are fragmented into very detailed situations in Cirkus environment. Therefore, the main thing which is always shown in a personal assistant is a user’s current situation and its related service. Current user’s situation can be recognized or established by various ways. For example, the user can state or say his current need (or there can be used a mind reading approach in the future also). Another option is automatic detection of a user’s situation depending on his current circumstances. Also, the user’s situation changes as he performs actions offered by services.
Let’s have a look now at how a personal assistant can be used, for example, to order a taxi. We let a user write his requirement for a taxi simply into the text input here.

Personal assistant passes this query on the server where data of user’s online identity is stored. Next, it finds out from the knowledge, that in case of a “need a taxi” query, the user is probably in a “taxi customer” role and in a situation, when he wants to order a taxi. Additional information that we get from the knowledge is when situations of others match to a user’s current situation. In the case of a taxi ordering, matching situations are those representing available taxi drivers. Another piece of information obtainable from the knowledge is that we can order a taxi ride if we have a taxi driver. All of this information we got from the knowledge is then mashed-up together with users’ data and returned back to a personal assistant in the form of a situational service.

All the situations appear in personal assistant in its textual form. Possible actions appear in the form of controls like buttons or menu items. Actions, when performed, change current user’s situation and possibly the situation of other users. The knowledge configures what types of different situations should be simultaneously created for different roles. Thus, when the user press the “Order” button from our example, he reaches a new situation: “I ordered new taxi ride”.

Simultaneously, the taxi driver gets into the new “I got new taxi ride” situation.

Similar to the previous example, if I want to write a blog post I could just send a query “write a blog post”.

Given by the knowledge, it means I’m in the “blogger” role and the situation when I want to write a blog post. Among the other information, there is a blog post entity with its properties configured in the knowledge for this type of situation. Thus, personal assistant shows input form to fill in all the necessary blog post content.

In a “I have a blog” situation, the user can see his blog posts and a button which leads to the creation of new blog post. More precisely, it leads to the same situation (“I want to write a blog post”) which we got into with the previous “write a blog post” textual input.

Personal assistant can also suggest future situations and actions automatically dependent on the current user’s state, his current role and situation, location, time, etc.

Personal assistant also enables us to see other online identities and it’s relations in the same way as we browse common websites. For example, there is a configured a Cirkus Environment Reader role in the knowledge and its possible situations and actions towards Cirkus Environment and related online identities. Textual information represent properties of these identities, while navigation items represent possible Cirkus Environment Reader’s actions.

Application of the knowledge to our data and its interpretation in a personal assistant brings us plenty of new possibilities in terms of seamless interactions amongst users. We will cover another interesting examples in detail in some of next articles."
25,Contribution of an intelligent personal assistant in Our Daily Life,intelligent personal assistant,https://medium.com/@namejacob16/contribution-of-an-intelligent-personal-assistant-in-our-daily-life-4b45b36836e0,"An intelligent personal assistant is also known as Automated Personal Assistance. It is a program that can perform tasks as though it were a human. The task to which IPA will respond is based on an input and location recognition. IPA is also capable of fetching the information from a variety of online sources like weather updates, stock prices, latest news etc. AI Personal Assistant technology is accredited by the concoction of API, Smart Phones and the escalation of mobile apps. However, it designed in such a way that it can perform any task given by an individual but the limitation of this system is that it can perform one task at a time by hearing the voice instructions of the user. One of the key features of an AI Personal Assistant is that it organizes information according to an individual’s needs, including the management of calendar events, emails, to-do-list, files, music etc.
Most people are unaware that they are using an intelligent personal assistant in their everyday life in the form of applications like LG G3’s Voice Mate, Samsung’s S Voice, Google’s Google Home, Amazon Alexa and many others. These software solutions are the best example of IPA and help us to manage a proper schedule of tasks in our busy life. We all are aware of Apple’s Siri, which is also an AI Personal Assistant that helps us in doing things like sending text messages, checking the weather, and reminders setting, playing music list as well as it also answer the question about geographical locations through GPS system. For this, you have to only ask Siri for the things you want to do and rest she we will arrange things according to your comfort and need.
AI Personal Assistant not only reminds you of what to do at a certain time but it will also remind you what to do at certain place.. And when you want to send text messages, you don’t need to lay your fingers on the Smartphone, you only need to tell the message and sender name, and then automatically AI Personal Assistant sent it to your recipient. All these small-small things that we use in our daily life in our mobile phones become hi-tech and helps us to save our time in many other ways. With the help of the evolution of IPA, now there is no need to hire a personal assistant for maintaining and scheduling personal and professional activities because it can do anything that one personal assistance can do for you, even it is smarter, faster and secure than human being and resolve any problems within a seconds.
In India, there are many software companies thatare working on AI Personal Assistant designing and provide Free CRM Software that helps in engulfing broad set of applications to manage the data of customer for business purpose. It also very helpful in maintaining the proper interaction between company and customer, access business information, marketing, automate sales, marketing and customer support."
26,Intelligent Personal Assistant Market Booming Across the World,intelligent personal assistant,https://medium.com/datadriveninvestor/intelligent-personal-assistant-market-booming-across-the-world-352fa7a99f23,"Report Consultant’s analysts forecast the global intelligent personal assistant market is expected to grow at a CAGR of +36.7%.
DDI Editor's Pick: 5 Machine Learning Books That Turn You from Novice to Expert - Data Driven…
The booming growth in the Machine Learning industry has brought renewed interest in people about Artificial…
go.datadriveninvestor.com
An intelligent personal assistant is a system with artificial intelligence (AI) which imitates human interaction in order to carry out particular tasks. This system is being increasingly used due to machine learning along with other advancements made in AI technologies. Intelligent personal assistants have a large capacity to learn, reason and understand which are the three essential components required to emulate the problem-solving abilities of a customer service agent.
Request for Sample Report @
https://www.reportconsultant.com/request_sample.php?id=5496
Top Key Players of Intelligent Personal Assistant Market:
· Apple, Inc.
· Google LLC
· IBM Corporation
· Oracle Corporation
· Microsoft Corporation
Type Outlook of Intelligent Personal Assistant Industry: Speech Recognition, Test-to-Speech Recognition
Service Outlook of Intelligent Personal Assistant Industry: Customer Service, Marketing Assistant
Application Outlook of Intelligent Personal Assistant Industry: BFSI, Automotive, IT & Telecom, Retail, Healthcare, Education, Others
End-Use Outlook of Intelligent Personal Assistant Industry: Small and Medium Enterprise, Large Enterprise, and Individual User
Ask for Discount / Customization on Report @
https://www.reportconsultant.com/ask_for_discount.php?id=5496
According to the research report, the global intelligent personal Assistant market has gained significant momentum over the recent past. The accumulative recognition, the increasing demand and the growing necessity for this market’s products are revealed in the study. Along with this, the features functioning their acceptance among consumers are also mentioned in this report study. This market report offers a comprehensive analysis of the overall scenario in the global market."
27,The intelligent Personal Assistant,intelligent personal assistant,https://medium.com/@mrtomconnelly/in-1997-my-father-purchased-some-voice-recognition-software-called-naturallyspeaking-by-dragon-5f454a3b380e,"In 1997 my father purchased some voice recognition software called NaturallySpeaking by Dragon Systems. It was my first introduction to voice recognition technology and although it wasn’t 100% accurate (it really was quite useless), I was amazed that it was possible.
These days all you have to do is say “OK Google” or “Hey Siri” and your phone is ready to take your command. Setting reminders, sending text messages, and making calls is only a taste of what can be actioned via voice commands.
Voice recognition technology has come a long way.
I’m an avid user of Google’s intelligent personal assistant (IPA) service Google Now aka “OK Google”. The thing is, I don’t like using it in public. When I do, I try to keep it discreet until I give up and switch back to the ol’ fingers and thumbs. Friends and family I’ve spoken to say they feel the same way.
I feel like this technology is going through its teething stage where we’re not sure if it’s cool yet. This is such a great tool but it must be embraced in order for it to improve and be used more across more applications. What can be done to help this technology be adopted by the mainstream and socially accepted?
Allow custom wake up commands
I don’t want to walk down the street saying “Ok Google”. Why can’t I create my own activation phrase? I’d be much more comfortable using my IPA in public if I was able to say something like “Are you ready?”. As far as I’m aware, S Voice for Samsung is the only IPA that allows custom wake up commands.
Make it so I can “call” my intelligent personal assistant
I want push a button, put my phone to my ear and hear a voice say “How can I help?”. Imagine it, you could be rushing to work talking to your IPA on the phone, sending texts, writing emails, learning your agenda for the day and more. Voice recognition works best when your mouth is directly next to the receiver anyway. Why not hold it to your head like you’re on a call?
Flip it on its head – Dictaphone style
I often turn my phone upside down and speak into it like a dictaphone. This allows me speak directly into the receiver and glance down to my screen for more information. The problem is my phone doesn’t automatically invert the screen when the phone is the wrong way around. This idea impacts the phone’s operating system and the possibly the phones design rather than functionality of the IPA software.
“Is there anything else I can help you with today?”
When I make a voice command and it’s answered, the IPA should continually ask me “Can I help you with anything else?”. This would eliminates the need to repeat the voice recognition command and facilitate multiple commands.
This powerful technology won’t be applied to more apps and tools until it becomes adopted by the mainstream. For this to happen, I believe the way users engage with IPA software must be matched with existing technologies that they are familiar with and feel comfortable using."
28,Apple Inc. To Replace Intelligent Personal Assistant Siri With Viv,intelligent personal assistant,https://medium.com/@praneethrover/apple-inc-to-replace-intelligent-personal-assistant-siri-with-viv-6ee3772b3149,"Apple Siri is gonna expire soon. Dag Kittlaus and Adam Cheyer, Inventors of Siri, have invented a new Artificial Intelligence Technology called ‘Viv‘. It is a powerful AI invented by Siri’s creators from APPLE which can be able to learn from the world to improve upon its capabilities.
Apple is the world’s largest IT company by revenue and 2nd largest mobile manufacturing company. This AI bot is more advanced than Siri and is super smart than any human being or any virtual assistant which can be able to carry out any complex tasks by mimicking the knowledge data-base of a human assistant.
Actually, “Viv” was named after the Latin root meaning of ‘Life’. It is more advanced than Siri, Google, Cortana or Alexa. According to the inventors the Viv is supposed to be more flexible in its own way and providing the need or feel of a human."
29,BMW Introduces Intelligent Personal Assistant for its Cars,intelligent personal assistant,https://medium.com/@techstoryin/bmw-introduces-intelligent-personal-assistant-for-its-cars-9e0320af4add,"Gone are the days when cars were only used to go from one place to other. Today, they are a mark of luxury, comfort, and convenience. And the manufacturers are also not keeping it loose in providing the best and effortless experience for their customers. Taking a step further in this direction, BMW has introduced an Intelligent Personal assistant (IPA) for its cars to keep the driver free from doing anything.
The IPA will hear you when you say, “Hey BMW,” and the name can be changed to anything you want it to be called. Once it hears you, you can ask it to manage or change any functions of the car including the navigation, climate control lighting, infotainment system, headlight settings, and almost every other thing that is electrically controlled.
Also Read: BMW Thinks Fully Driverless Cars are Impractical
The IPA will join the driver as a stranger with basic settings at first, then it will keep learning about the driver’s preferences and routines. “BMW’s Personal Assistant gets to know you over time with each of your voice commands and by using your car,” BMW’s senior vice president Digital Products and Services, Dieter May, said. “It gets better and better every single day.”

The Personal assistance will also be able to react to natural human commands, so you could say, “Hey BMW, I’m cold,” and it will adjust the climate control accordingly. BMW says the IPA is the ideal co-driver, as you could ask it to search for nearby gas stations, restaurants, available parking spots, etc.
Also read: 2019 BMW Z4 Breaks Cover at Pebble Beach with a Soft Top–
BMW has recently partnered with Amazon for the voice assistance technology. While the IPA will make your driving experience better, Alexa will look after your shopping, your smart devices at home and office, and all other tasks. By integrating these two, you can have a seamless movement during your day without having to carry or remember any information.
BMW has had a long partnership with Microsoft with its cloud platform Azure. The latest BMW Intelligent Assistance is also built on top of Azure and conversational technologies. BMW has also integrated office 365 and Skype with its IPA, so, the driver can manage the work and attend skype conferences along the way. It is expected that Microsoft’s voice assistant Cortana will also be joining the BMW cars soon.
The BMW Intelligent Personal assistance will be rolled out from March 2019 as standard in the new X5, Z4, and the 8-Series models, an over-the-air software upgrade. The IPA will also be available in the 3-series but as an option and the service will be offered for three years only."
30,Intelligent Personal Assistant Recommendation for College Students,intelligent personal assistant,https://medium.com/@luhan_91776/to-be-continue-5863317aaf1c,"Introduction:
This recommendation report will evaluate and compare the most popular three intelligent personal assistants Apple Siri, Google Assistant, Microsoft Cortana, from the technology aspect and application aspect, regarding their speech recognition system and users’ experience. The report will use different research methods for two aspects. The technology aspect will focus on the speech recognition system while the application aspect evaluates Intelligent personal assistants’ performance to implement the popular and useful functions that customers value the most.
Background:
With the development of AI technology, one of its applications, intelligent personal assistant, has become more and more popular. Installed on laptops, smartphones or other electronic devices, the intelligent personal assistants take over tedious things that people used to manage on their own and help. More and more college students start to use intelligent personal assistants as well for study or entertainment. Intelligent personal assistants integrate data from apps to make information easier to process. Useful functions are taking voice notes, finding study guides and materials, playing music, checking emails, notifying essential events, making schedules, etc.
The evaluation will be among Apple Siri, Google Assistant, Microsoft Cortana. Siri is the Intelligent personal assistant launched by Apple, it is released as part of the integrated system of IOS and thus could be only used in Apple products. Google Assistant, firstly launched and installed on the Google Pixel and Pixel XL phone, is also compatible with android, Snoy and IOS systems additional to Google devices. The Microsoft Cortana is available on Windows 10 desktops and mobile devices as well as other systems like android and IOS. Amazon Alexa is not in comparison as the selected Intelligent personal assistant all have mobile used.
While customers sometimes complain that intelligent assistants are ‘not human enough’ due to the limitations of AI technologies, others think that designed functions are cool but not useful. I would conduct comparisons based on the technology and application aspects between these three products. The report will use different research methods for two aspects. The technology aspect will mainly analyze graphs or statistics from companies who own the products and research papers from third-party experts, while the application aspect used the stats from the online survey to find out the popular functions users do with their Intelligent personal assistants. Then I would use a personal test to determine how each Intelligent personal assistant perform under different scenarios.
Technology aspect (Speech Recognition System):
Speech recognition is a type of algorithm that enables the program to identify human words and convert them into information that is readable by machines. It’s an essential feature for Intelligent personal assistants to record and analyze users’ verbal commands and implement further instructions. On the other hand, the misunderstanding of commands and wrong-capturing of words would increase inconvenience for Intelligent personal assistants’ users.
Speech recognition works in the usage of algorithms via acoustic and language modeling. Acoustic modeling shows the connection between linguistic devices of speech and audio indicators while language modeling fits sounds with phrase sequences to assist distinguish between phrases that sound comparable. The performance of speech recognition is evaluated by accuracy and speed. Word Error Rate(WER) could show the accuracy of how the word is identified in transcription but cannot analyze the error that occurred.
In August 2017, researchers of Microsoft Artificial Intelligence & Research group announced their latest result that the speech cognition technology has achieved 5.1 percent of word error rate on the 2000 Switchboard evaluation set (W. Xiong) due to ‘a series of improvements to the neural net-based acoustic and language models’ said Microsoft technical fellow Xuedong Huang. With the same speech cognition ability as a human professional transcriber (daily mail), Cortana is placed in the line with professions with the aid of the latest speech cognition technology.
Two types of Convolutional Neural Nets (CNN) are used in the system to increase the accuracy to perceive the words. CNN is a class of deep artificial neural networks, a computational model based on the structure and functions of biological neural networks that could learn how to complete tasks by considering examples and analyzing visual imagery. (W. Xiong)
ResNet CNN uses the powerful highway connections to linearly transform inputs to output while the LACE model extracts simple local patterns and complex patterns at the same time to enable the process of a large amount of vocabulary running through the system. And last but not least, the new additional CNN-BLSTM model greatly improves accuracy by separating the words into pieces based on the time difference and cutting the same command into more pieces and examine each section more detailed. As a result of the improvements in acoustic models, Cortana reaches the low word error 5.1 %.(W. Xiong)
Another group of researchers from Montana State University experiment on the performance of the speech recognition system of Apple Siri and Google Speech Recognition (GSR) under different network conditions. While enhancing different applications in real-life scenarios, the speech recognition system would have message delays caused by packet loss and jitter, which led to low accuracy and speed for the information transmitted. (Mehdi).
In the experiment, the researchers set up two models to evaluate the message delay of Apple Sir and GSR under different packet loss and jitter respectively.

In the experiment, the same 26.4-second client’s voice package was transmitted through the netem box (traffic shaper in the graph) to vary the network conditions and imitate the packet loss and jitter. After GSR and Siri receive and translate the voice data, the results are sent back to the clients. The roundtrip time (the time after GSR and Siri receive the text) shows message delay and the accuracy, measured by comparing the match percentage of the original string in the voice package with the final context from the speech recognizer. (Mehdi).
The results showed that both packet loss and jitter impact the performance of speech recognizer., but Siri had much more delay than GSR. The following graphs demonstrate the impact of packet loss and jitter on the delay of GSR and Siri. Siri caused delay by 3.5 to 5.0 second when packet loss is 1% to 5% while GSR had less than 3.0-second delay even under the highest value of packet loss. Regarding the impact of jitter from 20ms to 180ms, GSR also has a lower average delay than Siri. (Mehdi).


After comparing the obtained results with the Walch t-test, the result found out the mean difference between the two speech recognizer systems is 1.666s (p-value < 2.2e-16), which indicates a significant difference. The majority of the delay of Siri was caused by the algorithm that allows Siri to start to recognize voice only after receiving the entire string. On the other hand, GSR processed the information after receiving the first voice package. It adjusted transport and application layers to keep the result accurate under huge interference with high values of jitter and packet loss. Combining results of the experiment with the standardized industrial tests, the researcher found out that the word error rate for Siri is 5.0% (L) while GSR has a lower word error rate of 4.9% (E).

Graph 1: Comparison of Voice Recognition Systems among the three Intelligent personal assistants
Conclusion for technology aspect:
Siri, Google Assistant, and Cortana all have a powerful speech recognition system with a comparatively low word error rate (< 10%). To sum up, the speech recognition error rate of Siri is 5.0 %(L), with the lowest error rate of Google Assistant as 4.9%(E) and the highest error rate of Cortana 5.1%. So, if looking solely at word error rate in the speech recognition system. Goggle Assistant is the winner.
Yet, the Intelligent personal assistants with extraordinary speech cognition system still have limitations, as the researchers of Cortana said, the ultimate goal of speech cognition system is not only to accurately capture the words but also to understand the content. But anyway, the former is the foundation that prepares Intelligent personal assistants to be more intelligent and Google assistants with GSR win this competition of word rate error.
Application Aspect (User experience):
The second aspect evaluates the ability of Siri, Cortana and Google Assistant to implement useful functions regarding user experiences. Whether Intelligent personal assistants are useful depending on their performance to implement the popular functions that customers value the most. I would carry on a personal test to three products to determine how each Intelligent personal assistant performed on the popular situations that people would use Intelligent personal assistants.
The statistics represented the results of an online survey conducted from 1040 respondents in the United States in April 2017 and showed that people may use their Intelligent personal assistants mostly in the following situations. For college students especially, the stats (HigherVisibility) also found that they prefer functions like manage alarms, play music, check weather reports and loop up news headlines, highly matched the stats in Graph 2.

Graph 2: Situations where people would use virtual assistants(Statista)
Based on the rank of 6 popular tasks, I would test the three Intelligent personal assistants under specific scenarios and evaluate their performances respectively. I used iPhone 6s for Apple Siri. For Google’s Assistant, I used Google Pixel XL. I used ThinkPad X200 to test Cortana on Windows.
The following are the performances of the three Intelligent personal assistants in six situations.
Music
Scenarios: I asked each assistant to play 10 songs from different ages. Some are not popular and in foreign languages. Google Assistant got 9 out of 10, Siri and Cortana got 7 out of 10.
Siri works with Apple Music mostly while Cortana is with Groove Music developed by Microsoft but is little-known. The supported media with Google Assistant is more diverse from Spotify, Google Play Music to YouTube increases the possibility to find the specific songs wanted by users. Google Assistant scores 2 and Siri, Cortana 1.
2. Weather
Scenarios: I asked ‘what’s the weather like tomorrow?’ followed up by ‘what about Goleta?’ and ‘Should I bring an umbrella?’
All three assistants could answer the first question but for the second question, Google Assistant correctly interpret the second question and pulled out similar information while both Cortana and Siri didn’t understand the questions and had to ask direct questions about the weather at Goleta. When I asked ‘should I bring an umbrella?’ Cortana directly responded ‘That’s probably not necessary.’ then told me the temperature and the weather like today with detailed information. Siri did the same. Google Assistant score 3, Cortana scores 2 and Siri scores 2.

Cortona (left), Siri (right)
3. Calendar/Schedule
Scenarios: I asked, ‘What’s my schedule tomorrow?’, ‘set a schedule for essay tomorrow at 4PM’, ‘Change the time of product recommendation appointment tomorrow.’
All three assistants could pull out the information about my schedule tomorrow and easily create an appointment, but Siri who can directly interface with the phone’s calendar makes it easier to modify the events’ time or delete events. Cortana asked me to specify the reference to the meeting while modifying it and couldn’t create or modify the appointments through voice alone. For Google Assistant, it’s smooth at set up events but cannot modify afterward.
Siri scores 3, Cortana scores 2 and Google Assistants scores 1.
4. Email
Scenarios: I asked ‘Send an email to Lu Han’, ‘Do I have new email?’ and ‘Read my recent emails’.
Siri prefers to work with Apple Mail while Google Assistant prefers Gmail. But both could recognize the name, write the message through users’ voice and send the email. In contrast, although Cortana can achieve deep voice interaction like Siri and Google Assistant, its recognition of voice names is poor and the error rate is high. Siri did the best job at the check and read new emails. At the scenario interaction level, Siri understands the semantics and correctly calls recently unread mails; on the functional level, Siri can also fine-tune recently unread mails in chronological order/sort order to display or read. Google Assistant displayed a random email from months ago and didn’t link to the Gmail app. Cortana brought up a browse page and opened Bing search for the question. Siri scores 3, Google Assistant 2 and Cortana 1.
5. News
Scenarios: I asked, ‘What’s the news today?’ ‘Tech News?’
Google Assistant could identify the users’ requests and return the correct query results. For the third-party integration, Google News assistant supports the most extensive search from BBC, VOA, Fox News to Bloomberg. Siri also understood my request for today’s news and collected news from NYTimes to VOA. Cortana pulled up top stories for today and read the first headings for me, integrated news mostly from Fox News (nearly all the top news is Fox News). Both Cortana and Goggle Assistant could pull up the latest Tech News Compared with other intelligent virtual assistants, Google Assistant has integrated a large number of third-party news media as news sources in news topics and greatly enhanced the user experience. Google Assistant scores 3, Cortana scores 2 and Siri scores 2.
6. Calling Uber
Scenarios: I asked, ‘Call me an Uber.’
Siri can interact with the user in destination confirmation, model selection, and vehicle booking and help users to achieve path planning and riding cost estimation through the integrated map function. Because Google Assistant and Cortana does not integrate third-party taxi applications, it can’t interact in this area and couldn’t identify this semantic recognition. Siri scores 2. Google Assistant and Cortana scores 1.
Data Processing:
After collecting all the scores, I made a decision matrix to describe the multi-criteria decision analysis problem and evaluate the strength and the weakness of the Intelligent personal assistants based on the criteria in Graph 2. The first column of the table is the functions with the ranking of popularity. The scores (1 to 3) are given for each Intelligent personal assistant to show their performance in particular functions. The score would be the same depends on how Intelligent personal assistants performed. The final evaluation considered both the share of respondents (refer to graph 2) and Intelligent personal assistants’ performances. The former would be the weight of the latter and the Intelligent personal assistant performed well in the popular functions would have the highest score.

Table1: Comparison of Intelligent personal assistants using decision matrix
Conclusion for application aspect:
From the evaluation, Google Assistant scores the highest with Apple Siri behind. Microsoft Cortana scores the lowest.
In the test scenarios that have been implemented and integrated with other Google applications like YouTube and Gmail, Google Assistant performed extremely well. It also showed the semantic understanding stronger than other virtual assistants, in the scenarios of asking the weather, sending voice email and selecting the news. However, Google Assistant still has problems in the integration of phone-based applications like Calendar. But with its advanced technology to implement great semantic understanding in users’ requests, Google Assistant keeps being the most promising Intelligent personal assistant with high developmental potential.
Apple Siri did a good job to integrate phone modules and third-party applications. With the advantages of phone manufacturers, it performed well in setting and adjusting the schedule, writing voice email and calling Uber, the functions that relate basic phone functions and system feature. Although its in-depth semantic understanding of individual scenes is slightly weaker than that of Google Assistant, Apple Siri performed better functional modules after understanding the scenarios than other intelligent virtual assistants due to its integration of third-party applications.
Compared to the other two intelligent virtual assistants, Cortana is slightly inferior. Its incomplete establishment of an intelligent knowledge base led to the semantic misunderstanding of user’s requests and can only call Bing search processing. Its ability to integrate third-party applications is weaker than the other two Intelligent personal assistants. Cortana still has a long way to go, regarding the establishment of the semantic knowledge base, as well as the building of functions, the integration of third-party applications and so on.
Recommendation:
I recommend Google Assistant based on the overall evaluation given from both technical aspects and application aspects, regarding Intelligent personal assistants’ performance of the Speech Recognition System and User Experience. Google Assistant shows the highest precision in the Speech Recognition System with the lowest word error rate of 4.9%. it also scores 5.13 in the decision matrix charts, implied its ability to understand users’ requests, integrate the third-party applications and implements the functions with high efficiency and precision, which makes it the most useful Intelligent personal assistants encountering most of the common scenarios."
31,Utah Fishing Alexa Skill Helps Find Hot Fishing Spots,personal digital assistant,https://medium.com/@utahgov/utah-fishing-alexa-skill-helps-find-hot-fishing-spots-e03263a727da,"Nothing beats good advice when you’re looking for a great day of fishing. Now Utahns can simply ask a personal digital assistant for some of the best advice from state’s best experts. The Division of Wildlife Resources announced the launch of the Utah Fishing skill for the Amazon Echo. The Alexa skill gives full fishing reports for fishing spots across the state of Utah.
“It’s incredibly fun to play with,” said David Fletcher, Utah Chief Technology Officer. “But then it becomes clear that it is a great tool to learn about some of Utah’s best fishing holes.”
Utah Fishing will help anglers find the best fishing spots across the state. The Alexa skill tells fisherman the status at the Utah fishing spots, the conditions, bait that has been working, and what species are biting. The reports are updated regularly by local experts who have the best information.
“If you’ve ever had a friend that kept you up-to- date on fishing hotspots, that’s what it’s like,” said Mike Fowlks, Division of Wildlife Resources’ interim director. “Except you have all the expertise of the DWR.”
To use the skill for Alexa users simply:
— Tell Alexa to enable Utah Fishing
— Ask, “Alexa, open Utah Fishing.”
— Then say, “Alexa, what’s the fishing report for Strawberry Reservoir?”
Or any of the 148 fishing spots cataloged in the Alexa skill.
For an interactive fishing reports map visit: https://wildlife.utah.gov/hotspots/"
32,Roger Samara | How to Identify and Remove Computer Virus,personal digital assistant,https://medium.com/@rogersamara/roger-samara-how-to-identify-and-remove-computer-virus-efb3b245657e,"Has your personal digital assistant been infected by a virus? Viruses and at variance malware can pose a suited security risk to your data and personal information, and both have a drastic effect on your computer’s performance. Here Roger Samara describes concern and removal of Computer Viruses.
1. Trojan Horse Virus
2. Worm Virus
3. Macro Virus
4. Rootkit Virus
5. Bootsector Virus"
33,iPhone X is clearly a little ghost of the Palm Pre,personal digital assistant,https://medium.com/@knatham/iphone-x-is-clearly-a-little-ghost-of-the-palm-pre-ea55a2988db4,"The mobile revolution started in many ways by the Palm Pilot way back in 1997. The Palm Pilot is a Personal Digital Assistant that created a new market segment for hand held devices. Apple entered the hand held devices space many years later but is still seen as the inventor of mobile technologies like touch screens. Many Apple fans seem to miss the basic fact that touchscreens existed a long time ago, and Palm was a pioneer in this space. Palm’s smartphones had touchscreens a long time before the first iPhone was introduced in 2007. And Palm’s Pre smartphone that was introduced in 2009 had another revolutionary user interface — the deck of cards activity design interface for switching between apps using simple gestures. When the 3rd generation iPhone was launched, Apple cautiously used a version of the cards interface by displaying just the application icons instead of the shrunk version of the application snapshot. Apple also used the center button interface to invoke its version of cards. But the iPhone X has clearly infringed on Palm’s patented cards activity design. With the removal of the center button, Apple has used the same gestures to invoke the cards interface. This is the exact mechanism that was once made popular by the Palm Pre.
2018 could very well be the year where the smartphone community finally acknowledges Palm’s creativity in this space. I say this because Qualcomm, which now owns some of Palm’s patents on the deck of cards, has filed a patent infringement case on Apple. In November 2017, Qualcomm filed a case with the district court for the Southern District of California, asserting that Apple infringed on 5 of its patents — 4 out of which were purchased from Palm. Regardless of the outcome of the case, as Palm’s ex-employee and long term fan, I’m happy that Palm’s patents have passed the test of time and hopefully it will finally get recognition for its inventions."
34,It Is All About the Timing! The Revival of Failed Concepts…,personal digital assistant,https://medium.com/venturesight/it-is-all-about-the-timing-the-revival-of-failed-concepts-5a8f60f8f440,"It is starting to feel like we are seeing it over and over again. The Personal Digital Assistant, or more commonly known as PDA(even with cellular functionality), came out years before an iPhone. While many Japanese automobile manufacturers produced electric vehicles before Tesla came into play. Timing these days seem to be even more crucial in a rapidly changing environment, hitting the market little too early could spell disaster or hitting the market too late could mean you’re the laggard business in the market.

Source: Mircosoft, HoloLens 2 offers new ways for surgeons to plan operations
Although a number of other factors did play a part in the success or failure of these innovations, the timing did play a large part and it can be possible to foresee it. So what does timing mean? What it really means is, are the important success factors for your new business opportunity in place and ready for it.
For example, is your market already signaling adoption of your offering? are regulatory institutions prepared to address your offering?
… So what is next to for a revival?…
An interesting case is AR glasses, although one of Google’s huge flops (released 5 years ago), they had good reason to believe that this was a potential future. They did lack seeing how ugly they looked and failed to realize potential concerns that people have with privacy, among other things.
The rise of augmented reality, with growing support systems like releases of developer ARKits and technological development in laser-based displays can lead to a revival of these AR Smartglasses products.

Source: https://www.youtube.com/watch?v=5dsVwXzOIjk
Focals by North, seem to be making a good impression, although the price is still a deterrence with essentially expecting to pay around $600-$1000, and the technology still has some way to go to reach the same future Google had originally envisioned. Microsoft’s new release of Hololens 2, and Magic Leap are said to be working on their next model suggests there is a significant interest in this area. The ecosystem will definitely push the AR glasses forward, so this is an emerging industry to watch out for in the coming years.
What other products/services do you see today, that are actually too soon?
Are there any other revivals going to happen within the next 3 years?
Comments or thoughts…"
35,Did Facebook just escalate the battle of digital assistants to enterprise?,personal digital assistant,https://medium.com/@vishrutshukla/did-facebook-just-escalate-the-battle-of-digital-assistants-to-enterprise-bb1dccec3eec,"Although rather nimbly, Facebook yesterday unveiled its take on a personal digital assistant for iOS and Android users. Dubbed “Facebook Here” by TechCrunch, the low-publicized “feature” lives inside Notifications area on the app. Here adopts the now fairly standard card-based UI to show information such as your friends’ birthdays, life events, upcoming sports and events, TV shows, what’s trending and geo-specific content such as nearby places, movies/theatres, local events and news etc.
With this, Facebook finally gets a level playing field to take Google’s recently announced Google Now on Tap contextual service and Microsoft’s Cortana personal assistant head-on for consumers. Consider this:
· Facebook has an extremely rich social profile of yours — which Google and Microsoft lack. It exactly knows your friends, strength of your relationship with them, your messages, activity updates, places you’ve visited, content you read and share and much more
· By mining your social graph, Facebook can surface things which interest you. On the contrary, Now and Cortana can surface in the moment factual information unless they explicitly enquire about your tastes and preferences to learn over time
· Users already spend a significant share of daily screen time on the Facebook app — especially hooked to the Notifications tab. Importantly, almost every notification row elicits a user action. Showing relevant, timely content on the same tab just feels natural!
But wait a minute… Is this “me-too” assistant the only card that Facebook has up its sleeves? Perhaps not.
Later yesterday, Facebook announced a huge win for its FB@Work enterprise collaboration and productivity offering: Royal Bank of Scotland adopted the platform for its 100k employees. Superimpose this big announcement on top of the Here “feature” launch and new possibilities start to emerge!
Can Facebook Here — in its current form — be a precursor to a “smart, fully enterprise context-aware digital personal assistant living right inside FB@Work?”
With 300 businesses already using FB@Work within 10 months of its public unveiling, Facebook appears serious in this enterprise play largely seen as an attempt to secure an alternate source of B2B revenue in addition to its ads-based monetization. Competitors Google and Microsoft both already have years to deep enterprise reach (and hence, real enterprise user and activity data) with offerings such as Google Apps for Work, Microsoft’s Yammer and SharePoint. To add to it, interactive services such as Google Now are ready to surface relevant work-related content/notifications/reminders to enterprise workers.
Here appears fit to fill this gap for Facebook. There is nothing that beats showing enterprise information workers (IWs) in-the-moment, relevant and actionable cards straight within their company’s social network. If we’re reading the signs correctly, the Here+FB@Work combo certainly doesn’t seem like a far-fetched dream. It’s almost here!"
36,Let Cortana Translate For You,personal digital assistant,https://medium.com/@Onecooltip/let-cortana-translate-for-you-e48bd40f7c8d,"arlez-vous Français? Cortana, Microsoft’s personal digital assistant, has been updated in Windows 10 to offer translations in 38 languages Users can ask Cortana to translate via verbal or written requests and Cortana will answer with audio and written replies. To try this yourself, ensure that Cortana is active in Windows 10, then either type your request or say “Hey Cortana”, then “Translate “Word, phrase or sentence you want to to translate” in ‘Language you want to translate to’. Of course, change the request to something like “Translate ‘Where are the bathrooms?’ in Spanish’”. This new capability adds the recently introduced translation capabilities in the Microsoft Skype audio/video chat service. As Cortana improves and expands, this service will be more useful in mobile devices. Enjoy!"
37,Ask a UXpert: How Do You Design for Headless Interfaces?,headless interfaces,https://medium.com/thinking-design/ask-a-uxpert-how-do-you-design-for-headless-interfaces-1b0392075ae4,"User interfaces (UI) have changed dramatically over the last few years. As we access the web on a myriad of devices, they need to adapt to our needs. UIs need to take into account slow connections, older browsers, and the wildly varying specs of the devices we use. Some of them don’t even have a screen.
Screenless — or headless — interfaces are becoming increasingly popular. The most common examples are Apple’s Siri, Windows’ Cortana, Google Assistant, and, of course, Amazon’s Alexa and its Echo device. We interact with them using our voice, but, as a lot of the principles we’ve become accustomed to from working on graphical user interfaces don’t apply, how do we design for them?
We asked four experts what they say we need to consider when designing headless user experiences and how to prepare for a future without pixels.
Transcend device boundaries

Headless user experiences are certainly a key step in the evolution of more humanistic design, but humans are inherently adaptive, and we expect our devices to be adaptive, too. Today’s headless user experiences fall short on adaptivity, as they often operate in a bubble. What if I want to switch to a computer or phone? Can I start an experience on one device and continue it on another?
To design the next generation of headless experiences, we need to think beyond a single device. How can we enable an interaction to follow a customer across a variety of devices and contexts? The first step is understanding what scenarios and customer states should transcend devices. What “slots” had the customer filled via a spoken exchange before switching devices, and can we share that state via the Cloud? What can we infer by a customer’s change in context? Once you’ve identified these key elements, don’t shy away from getting involved early in product design and architectural discussions to ensure you can share that customer state across devices.
Once we’ve begun tackling those transcendent scenarios, we turn our eyes towards the design process. Successful design for headless or multimodal user experiences is heavily dependent upon context. How are you capturing customer context in your research and deliverables? It’s often not enough to just share scripts or flows. Storyboarding can be critical in communicating the richness of your customer’s lived experience at the moment they speak to your product. High-fidelity prototyping may require video or audio components.
– Cheryl Platz, principal designer at Microsoft and owner of Ideaplatz
Find the thin line between science and magic

We’ve started to treat designing for headless UIs like being an illusionist, in the vein of Houdini or Kellar. I know that sounds odd, but it’s a great mindset. The goal is to maintain the magic and not show our hand. Being clear about the bifurcation between automation and interaction — when it’s automated or templated, we want that to be clear. But, when programmed logic or deep AI is behind the communication, it has to feel natural and conversational. We work to find that thin line between science and magic, where the user feels like they still have control. The biggest hurdle facing headless UIs is the literal act of letting go of controlling the interaction.
So, our headless UI design always starts with content — which is how all design should start, to be honest. Our focus is on getting the tone, consistency, and the big little details right. That means a lot of upfront planning, documents, and building blocks. We take the same approach that we do to build design systems. Instead of setting up a grid, we set up a scaffold of content to make sure that the bones are in place for consistency. What has to remain rigid is sorted out first. Then we move on to what can flex, somewhat, around the rigid parts. Then we build for the exceptions and things that can give and bend. Going back through it all in testing, we act like detectives, trying to find the inconsistencies and the gaps. We’re working hard to stay out of the uncanny valley and, again, either give a clear appearance of automation or a magic experience that feels human.
Design systems are far more important for headless UIs, since breaks in voice and tone are more noticeable than visual breaks. That may seem odd, but it’s true. Most of us are professional enough to keep things like buttons, borders, and fonts consistent, so obvious gaps in visual tone aren’t apparent. But the very minor things stand out much more when it comes to headless, from the level of familiarity or colloquialism in the prompts to how verbose or succinct the UI works can be enough to give away the trick. And that’s when it frustrates the user. Then, they want to skip the headless UI and take control through a graphical user interface (GUI) or talking directly to a human. Our goal is to build workers that can handle it all, with pleasure, and let the user feel just as in control as when they have access to a GUI or human intervention.
– Brad Weaver, partner at The Banner Years
Use markup to make sense of the content

Our first priority with any interface, headless or not, should be the content. It’s important to have clear, well-written copy. Does it speak to the audience using the words they do? Are the instructions and labels clear? Are the sentences easy to follow?
Once the copy is in good shape, I don my markup hat. I’ve got a lot of HTML elements at my disposal and I use them to further clarify the meaning of the content. Articles, paragraphs, lists, emphasis, abbreviations, all of these elements illuminate the content. They give structure to our documents and make them easier to read.
Once my first markup pass is done, I loop back to see if there are instances where the prose makes assumptions about how a user is interacting with the site. For instance, a link to “read more” can make total sense when it’s preceded by an article teaser. If a user is navigating the document via links, however (which happens often when they are using assistive technologies like voice-based interfaces) hitting one — or worse, several — links that offer little context as to their purpose can be incredibly frustrating. In cases like this, inserting visibly hidden text can keep the visual experience light while providing more context about the link to folks who may not have it. I might choose to label the link “Read more about designing for headless interfaces.” I could do that via the “aria-label” attribute or I might use CSS to hide — accessibly, of course — the portion of the link text I don’t want shown.
It pays to look to how HTML (and CSS and JavaScript) can reduce barriers for non-sighted users. After all, in a headless context, we’re all non-sighted.
– Aaron Gustafson, web standards and accessibility advocate, Microsoft
Start with user research, prototype, and test with real people

I think it’s always important to start with an understanding of users and what they need. Do some user research first. It’s even more important when designing for screenless experiences.
Rather than approaching it as designing an interface, I’ve realized it’s about designing a conversation. To do that, I need to understand the range of words and utterances used when we naturally talk about the topic.
Changes to content and words in a GUI are seen as low-cost and often (wrongly) left late in development. It’s the complete opposite for spoken interfaces — words are the interface. The sooner you can prototype a conversation and get testing with real people, the better informed your UI will be.
Initially, the prototype can be very low fidelity — simply recorded conversations and discussions. Compared to designing a traditional GUI, it would be the equivalent of a paper sketch — quick to do and fast to change.
There are lots of tools out there to help you make sense of this research: flow diagrams, word clouds, and bot simulators. And post-it notes are easy to create, sort, and change, all before any code is written."
38,The use of Chatbots and Headless Interfaces within the Public Sector,headless interfaces,https://medium.com/digital-leaders-uk/the-use-of-chatbots-and-headless-interfaces-within-the-public-sector-b499731de210,"Chatbots are becoming ever more prevalent, from banking bots that allow people to report a missing bank card and order a new one, to bots that help you book a taxi, or even order a pizza. And the public sector is quickly following suit.
The issue of how such tech can benefit the public sector was the subject of a Capita Software Services workshop held as part of Digital Leaders Week (19–23 June).
Representatives from a wide range of public sector organisations joined the workshop, held at Capita’s Gresham Street office in London and facilitated by myself and Dave Kearns, technical solutions manager for Advantage Digital.
The session discussed chatbots and headless user interfaces, what is meant by these terms and what kinds of solutions they encompass.
Chatbots can help support individuals who are digitally reluctant, and whose current preferred method of contact is still to use a council or social housing organisation contact centre. To offer a real-time and authentic feeling customer service experience, they have the ability to simulate human conversations. Someone using a chatbot simply types what they want in the messenger app and the bot brings it to them, talking to them as if it were a real person.
Headless interfaces involve software capable of running on a device without the need for a graphical user interface. The most common examples in recent times have been Apple’s Siri, Windows Cortana, Google’s Assistant Home and Amazon’s Alexa.
Specifically looking at the public sector, the session explored the potential this innovative technology offers organisations in transforming customer services and supporting digital initiatives via real examples and working projects, giving fuel for discussion and tangible food for thought for all attendees. Delegates were given a live demonstration of the potential of the tech to transform public sector services, including solutions built by Capita using chatbots or voice controlled interfaces to enquire about basic services such as refuse bin collections, or when your next housing association payment is due.
From the outset of the workshop we tried to emphasise the fact that both chatbots and headless user interfaces are still in the “emergence” stage within the public sector. While this tech is becoming increasingly mainstream in the daily lives of many people — Gartner has predicted that by 2020, 30% of all internet searches will be done by headless user interfaces — there remain a number of obstacles to overcome before it starts to have a real impact on public sector operations.
Issues raised by delegates included concerns over privacy of personal data and the ability to prove how the initial cost of tech can result in tangible benefits. While we recognised the fact that there are some challenges remaining to be resolved, they urged organisations to be part of the conversation from the outset.
Now is the time for your organisations to recognise that, if they are committed to digital transformation to better meet the needs of their customers, they should be on the frontier of recognising this rapidly evolving space and meeting the needs of the increasingly tech-savvy public whom they serve.
Among the workshop delegates was Simon Penaluna, assistant director of IT, Peaks and Plains Housing Trust.
“I really enjoyed the session,” he explained. “It was refreshing how the people who attended were from different organisations, so we could discuss strategies and possible benefits for headless tech and chatbots around a different customer base and identify different obstacles that could prevent or limit its use.
“The group discussions brought to light a range of issues that I think will help Capita to develop solutions in the future. I am excited to see where this might lead.”"
39,A Conversational Agent for Data Science,conversational agents,https://medium.com/hackernoon/a-conversational-agent-for-data-science-4ae300cdc220,"Science fiction has long imagined a future in which humans converse with machines. But what are the limits of conversational interfaces? Agents like Siri or Cortana can help us with simple things such as getting directions or scheduling an appointment, but is it possible to apply these agents to more complex goals? Today, our group at Stanford is excited to announce a few ideas we have been exploring through Iris: a conversational agent for data science tasks.
In comparison to the domains targeted by most conversational agents, data science is unusually complicated, with many steps and dependencies necessary to run any kind of useful analysis. When interacting with data through analyses and visualizations, you can’t simply rely on a set of hardcoded, standalone commands (think of Siri commands such as doing a web-search or placing a call). Instead, commands need to build upon and support one another. For example, when analyzing some econometrics data, you might take the log-transform first, then run a statistical test.
Iris supports interactive command combination through a conversational model inspired by linguistic theory and programming language interpreters. Our approach allows us to leverage a simple language model to enable complex workflows: for example, allowing you to converse with the system to build a classifier based on a bag-of-words embedding model, or compare the inauguration speeches of Obama and Trump through lexical analyses. To make this concrete, consider the following example:

Figure 1: Here we use Iris to download some tweets and analyze them with Empath, a text analysis library. (Note: these GIFs are large and may be slow to load in some browsers)
In the rest of this post, we’ll explore the specific interactions that Iris enables, why these interactions matter, and how developers can extend the system through a conversational domain specific language (DSL).
Basic functionality: how users interact with Iris
The Iris interface is a cross between a chat application like Facebook messenger and a development environment like RStudio (Figure 2). In the bottom left, users issue commands to the system, which will then appear in the window above, along with system responses. Data structures created in the course of a conversation appear in the upper right. There is a collapsible pane on the right sidebar for function search and documentation.

Figure 2: An overview of the Iris interface. Conversation and analysis occur in the left side window, while supporting metadata appear on the right.
One thing we learned early in the course of user testing is that people want to see, in advance, what command will be executed under their current input query. Iris displays hints above the text input pane, the leftmost of which will be executed on hitting enter (Figure 3). These hints allow users to reformulate queries when the proposed command does not match their expectations, resulting in fewer system errors. Hints also signal other elements of system state, for example, whether a user input argument parses correctly or matches the required type.

Figure 3: Hints allow for command search and query reformulation
Once a user hits enter to execute a command, Iris will automatically extract whatever arguments it can from the text of a request. Iris will then resolve any remaining arguments through a series of clarification requests with the user. For example, if you ask Iris to “take the mean of the column”, Iris might respond, “What column of data do you want me to use?” Once all arguments have been resolved, Iris will execute the command and display its output in the main window.
Commands as building blocks: sequencing commands
When working with data, it is rarely the case that a single command or function will accomplish everything you are interested in. More commonly, in programming a short script, you will chain together a series of commands. For example, first you might transform some text data into a bag-of-words feature representation, and then you might fit a classification model on these features. In Iris, any command in the system can be combined with any other command (subject to type constraints). This allows data scientists to use Iris to construct more complex workflows.
One intuitive way of combining commands is through sequencing: executing one command, and then using its result as the argument for some other command. This style of combination is similar to writing an imperative program, which Iris supports in three major ways. First, the result of any Iris command can be referenced in a follow-up command through pronouns such as “that”, “it”, and “those”. Second, Iris enables what is known in programming languages as assignment: storing a value in a named symbol. You can ask Iris, for example, to “save that result as my_array,” and a new variable named my_array will appear in the environment, where it can be referenced later in the conversation. Finally, many Iris commands save named variables in the course of their execution, which will similarly persist in the enviornment for future use.
Using these strategies, data scientists can chain together commands in sequence. For example, we might create a logistic regression model, then valuate it under cross-validation (Figure 4):

Figure 4: Here we use Iris to build a classification model to predict flower type, then evaluate it under cross-validation.
Commands as building blocks: composing commands
While sequencing commands is powerful and intuitive, it is not always the best way to stitch commands together. Consider how people speak, and you will often find that they craft meaning through a composition of statements. For example, if someone asks you, “what is the variance of that?”, and you respond, “sorry, what data are you talking about?”, then they might say, “I meant the first column of data’.” In programming terms, this conversation would compose a command to compute variance with another command to select the first column of data:

Iris supports composition through the ability to execute any other command when resolving some top-level command’s arguments. This style of sub-conversation can be nested upon itself to form conversations of arbitrary depth (in practice, exchanges that are more than two levels deep become somewhat confusing). For example, we can ask Iris to run a t-test, and interactively compose that with a log-transform (Figure 5):

Figure 5: Here we compose a command to run a t-test, with a command to log-transform columns of data
Like an interpreter in a dynamically typed programming language, Iris will check whether a composed command returns the type of value that an argument requires (for example, a command that takes the mean of some data might expect a composed command to return an array value). If the type returned by the composed function doesn’t match, Iris will reject the value and ask the user for further clarification.
Extending Python functions with conversational affordances
User contributions are an important part of any programming ecosystem. One of Iris’s goals is to allow expert users to extend the system with missing functionality. For this reason, all of the underlying commands in Iris are defined as Python functions, transformed by a high-level DSL into Iris commands with conversational affordances. For example, here is the implementation of an command to compute a standard deviation:

Figure 6: An Iris command implementation that computes the standard deviation of an array of data. The logic of the command is simply a Python function, annotated with natural language and type annotations.
The core logic of the function is wrapped in a class that defines metadata such as the title of a function (how it will appear in a hint), example user requests that execute the function, argument type information, and descriptive information. The DSL uses these metadata to transform the class into an automata that users can interact with conversationally.
While this DSL is relatively compact, we found in user testing that even expert users preferred a GUI to help with command creation. Iris has a command editor that supports command creation and modification dynamically within the environment, where changes can be saved to persist across sessions. We have also discovered, in practice, that the ability to compile and test commands directly within the enviornment leads to much faster iteration and debugging of new functionality.

Figure 7: Iris includes an editor that allows users to create new commands and edit existing commands. Any modifications to command can then be immediately tested through conversation in the environment.
Beyond text: working with mixed modality interfaces
Conversation may be an efficient way to create and execute programs, but data science tasks often rely on non-textual abstractions. For example, researchers often inspect and organize data through spreadsheets or write code that produces visual output, such as charts or graphs. Iris supports these modalities as a complement to its conversational interface. For example, when selecting a column from a dataframe, Iris will present the dataframe in a spreadsheet-like view, where users can click on the title of the column to populate the conversation request field.
Similarly, visualizing data is an important part of many data science workflows, and Iris supports this through the vega-lite library (a high-level wrapper around d3.js). Whenever an Iris command produces vega-lite schemas as output, for example, scatter plots or bar charts, these will be rendered as visual elements within the conversation.

Figure 8: Iris integrates with vega-lite to produce data visualizations
Launching an open source community
Iris is a research project, but it is also open source code, and as development continues to progress we hope that others will contribute. By building a community around this project, we aim to bootstrap an open dataset of natural language interactions that will allow us to push the boundaries of the kinds of tasks that conversational agents can support. We are planning to launch a desktop client for OSX later this summer, so if you are interested in helping us debug a beta release, check out our arXiv paper, follow the Gihub project or sign up on our mailing list at irisagent.com."
40,How chatbots will benefit businesses,conversational agents,https://medium.com/the-guild-journal/how-chatbots-will-benefit-businesses-2803e4e59da5,"The topic of conversational enterprise typically focuses on customer service in B2C environments. However, with the level of sophistication chatbots and virtual assistants exhibit today thanks to machine learning and natural language processing, companies are looking to deploy them internally to drive operational efficiency.
A virtual assistant is a software application that can perform tasks for an individual. Many people refer to them colloquially as chatbots or bots.
Integration with enterprise applications
Most of us are already familiar with popular virtual assistants programmed into our smartphones or speakers such as Apple’s Siri and Amazon’s Echo (Alexa). On the enterprise side, we already see leading SaaS companies integrating popular messaging bots such as Facebook Messenger into their applications. Major ERP providers such as SAP and Oracle are developing virtual assistants for their user interfaces as well.
TechCrunch reports that Salesforce is on the leading edge of this trend. It continually purchases startups that help its customers automate tasks and aims to democratize access to AI for companies of all sizes that use its services.
Salesforce is known for designing SaaS solutions for salespeople, but is now expanding into customer facing tools. Last year, it introduced its chatbot, ‘Einstein Bot’, to help improve operational efficiencies in customer service. Businesses that use the chatbot can customize the way it interacts with their customers.
Einstein Bot can answer customer questions and query the fulfillment system to pull up order information. Like other customer service bots Einstein Bot helps tackle the high volume, repetitive questions and allows customer service people to focus on more complex, nuanced requests. This in turn increases operational efficiency, improves customer engagement and fosters higher job satisfaction for customer service personnel.
Salesforce also points to research about The State Of Chatbots, to discover user preferences and potential use cases, as shown below:

Project management
With advanced AI and cognitive computing capabilities, chatbots can track internal data and customer interactions to make inferences that can assist internal operations and project managers.
Large-scale projects typically span multiple internal departments across different geographies. Stakeholders often spend a lot of time tracking status updates from their counterparts, customers and vendors.
For example, a project manager in Los Angeles may need a milestone update from their counterpart in Bangalore, and might wait up to 24 hours to receive a response. However, if the relevant information is stored in internal systems, a chatbot may be able to retrieve a response instantaneously.
While bots may not solve major production delays or strategic challenges, they can certainly cut down on response time and email volume that can create arbitrary delays.
Accounting
While virtual assistants may not reach the more complex areas of accounting, they can certainly help automate repetitive, simple tasks. In their current state, virtual assistants will assume more of an administrative support role in accounting.
They can also help with simplifying workflows and cutting down on administrative tasks. People working in accounts receivable can ask a chatbot straightforward questions such as a customer’s credit limit, to query a D&B report or the status of a payment.
With further advancements in AI, chatbots will evolve beyond purely transactional tasks and be able to provide advice given accounting data stored internally.
Human resources
In the digital era, the hiring process is becoming increasingly data driven. With hundreds of applicants sometimes applying for a single position, a chatbot can help with the screening process by asking some of the preliminary background questions required for a given position.
This kind of automation would be a great help for companies that hire lots of seasonal or temporary workers, such as retailers and distribution centers.
Chatbots can also assist with fielding HR FAQs for employees. Some may even feel more comfortable asking sensitive HR questions to a chatbot versus approaching a manager.
Enrolling in benefits can be one of the most frustrating parts of HR. Employees often spend a lot of time trying to sift through the information and understand the requirements. Chatbot queries may help clarify some of the convoluted areas of enrolling in benefits.
HR might actually prove to be one of the most difficult area to implement chatbots, due to the personal nature of HR and legal sensitivity of the information shared. While they may help increase operational efficiencies, companies should proceed with caution.
Faster employee onboarding
Depending on the experience level and position type, it can take weeks or months to train a new employee. This costs the equivalence of their overhead plus the additional resources it takes to get them up to speed.
ADP found that the average US company budgets $308,000 annually for new employee onboarding. Big bucks, and a cost that chatbots could help to reduce.
Most questions new employees have typically regard locating information, systems operations or simple industry knowledge. Many new employees may hesitate when asking questions in fear of interrupting their new peers or bosses who are busy in the middle of other tasks.
With virtual assistants, new employees can get their simple questions answered faster, feel more confident in their new roles, cut the training time down and start adding value.
Self service for simple IT
Reducing tactical IT work is a big priority for many organizations. IT resources don’t come cheap. You don’t want your IT staff spending hours on end helping employees resetting passwords or figuring out task manager.
For non-IT employees, stopping work to wait for someone to come fix a simple problem can interrupt time-critical tasks. Similar to customer service bots, IT bots can help increase the job satisfaction of IT workers tired of running around hitting Ctrl+Alt+Del. They can spend more time working on larger, strategic projects and contribute their skills to higher value tasks.
At least one European bank is using chatbots specifically for identity management and knowledge management. More use cases will no doubt emerge.
Nearly all industries can benefit by implementing virtual assistants for internal use. Major consulting firms and tech journals routinely cite banking, hospitality, travel, healthcare and logistics as some of the best candidates for bot deployment.
Chatbots can eliminate some of the tedious and redundant tasks of every position from attorneys to customer service reps to accounting managers.
Companies should start looking — right now — at the kind of virtual assistants that will increase productivity in the near future, while potentially reducing overheads and improving overall employee satisfaction."
41,Natural Language Understanding — Core Component of Conversational Agent,conversational agents,https://towardsdatascience.com/natural-language-understanding-core-component-of-conversational-agent-3e51357ca934,"We are living in an era where messaging apps deal with all sorts of our daily activities, and in fact, these apps have already overtaken social networks as can be indicated in the BI Intelligence Report. In addition to this clear point, the consumption of messaging platforms is further expected to grow significantly in the coming years; hence this is a huge opportunity for different businesses to gain attention where people are actively engaged.
In this age of instant gratification, consumers expect companies to respond to them quickly without any delay, and this, of course, requires a lot of time and effort for the company to hire and invest in their workforce. Thus, it’s now the right time for any organization to think of new ways to stay connected with the end-user.
Many organizations undergoing a digital transformation have already started harnessing the power of Artificial Intelligence in the form of AI-assisted Customer Support System, Talent Screening using AI-assisted interviews, etc. There are even numerous conversational AI applications including Siri, Google Assistant, personal travel assistant which personalizes user experience.
General Overview
There are two approaches to create conversational agent namely Rule-based and Self Learning/Machine Learning. For the Rule-based approach, you can use the power of regular expression to create a simple chatbot.
In this post, I will demonstrate to you how to use machine learning along with the word vectors to classify the user’s question into an intent. In addition to this, we shall also use a pre-built library to recognize different entities from the text. These two components belong to the Natural Language Understanding and are very crucial when designing the chatbot so that the user can get the right responses from the machine.
Natural Language Understanding
NLU (Natural Language Understanding) is an important part of Natural Language Processing which allows the machine to understand human languages. This has two important concepts namely Intent and Entity:
Example Utterance: I would like to book my air ticket from A to C.
Intent: In this case, the objective of the end-user is to reserve a flight from one location ( A ) to another location ( C ). Therefore, the purpose or intent of this question would be “reserve ticket”.
Entity: There are two concepts/entities in this query; Departure City (A) and the destination city (B).
To summarize, an intent “reserve ticket” has the following entities:
Departure City
Destination City
We can now use this information to extract the right piece of response for our user.
As part of this tutorial, we will be using Chatbot Corpus(https://github.com/sebischair/NLU-Evaluation-Corpora) consisting of 206 questions, which were labeled by the authors. Data comes with two different intents (Departure Time and Find Connection) and five different entity types. In this post, we will be using questions along with the intent to perform intent classification. However, when it comes to entities, we won’t train our custom entities, instead, we will utilize a pre-trained named entity recognizer to extract entities from the text. Custom entities can, of course, be trained using different algorithms but we can look into it later on.
Part 1: Intent Classification
To classify the user’s utterance into an intent, we can make use of regular expression but it works well when rules are simple to define.
We shall now head towards the machine learning approach to classify the user’s utterance; this is a supervised text classification problem where we will make use of training data to train our model to classify an intent.
An important part here is to understand the concept of word vectors so that we can map words or phrases from the vocabulary to vectors of real numbers such that the similar words are close to each other. For example, vector for the word “glacier” should be close to the vector for the word “valley”; two words appearing in a similar context have similar vectors. Thus, the word vector can capture the contextual meaning across the collection of words.

Please use this link to play with : http://projector.tensorflow.org/
We will make use of Spacy package in Python that comes with the built-in support for loading trained vectors. Trained Vectors include word2vec, glove, and FastText.
If we talk about word2vec, it comes in two flavors, Continous Bag of Words(CBOW) and Skip-Gram Model.

http://idli.group/Natural-Language-Processing-using-Vectoriziation.html
Continuous bag of words (CBOW) tries to guess a single word using the neighboring words, while the skip gram uses a single word to predict its neighboring words in a window.
Brief Detail
Input is fed to the network in the form one-hot vector.
Hidden layer is the magic part here and that is what we want. Hidden layer weights matrix are the word vectors that we are interested in, and it just acts as a lookup table because when you apply dot product between one-hot vector of input word and weight matrix of the hidden layer, it will pick the matrix row corresponding to 1.(Note: Dimension of the word vectors usually range between 100 to 300 depending on the vocabulary size)
The output part is just the Softmax function which gives us the probabilities of the target words.
We won’t go into much detail related to this.
Training Intent Classifier using pre-trained word vectors
Step1: Load Data
df=pd.read_excel(“Chatbot.xlsx"")
queries=np.array(df['text'])
labels=np.array(df['intent'])

Step2: Load pre-trained Spacy NLP Model
Spacy is a natural language processing library for Python that comes with the built-in word embedding models, and it uses GLoVE word vectors of 300 dimensions
# spacy NLP Model
nlp = spacy.load(‘en_core_web_lg')

n_queries=len(queries)
dim_embedding = nlp.vocab.vectors_length
X = np.zeros((n_queries, dim_embedding))
Step3: Preprocessing — Word Vectors
Right now if we see the data containing in our queries section:
print(queries)

To train the model, we will need to covert these sentences to vector using the Spacy pre-trained model.
for idx, sentence in enumerate(queries):
   doc = nlp(sentence)
   X[idx, :] = doc.vector

Now that we have converted sentences into the vector format, it can be fed to the machine learning algorithm.
Step 4: Training Model using KNeighborsClassifier
#########Splitting Part###########

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.30, random_state=42)

#############Training Part###########
from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=1)
neigh.fit(X_train,y_train)
Let’s evaluate it.
###########Evaluating##############
print(""Accuracy on Test Set: "",np.count_nonzero(neigh.predict(X_test)==y_test)/len(y_test))

Let’s check on sample query

Note that this is correctly identifying an intent which is FindConnection. You can try different variants to test this part.
Congratulations, we have successfully built our intent classifier which can understand the purpose of the user’s utterance. Now that the machine knows the purpose of the user’s question, it needs to extract the entities to completely answer the question user is trying to ask.
Part 2: Entity Recognition
Entity Extraction helps us in figuring out the right field so that the user can get the right response from the machine. For example, the query “from olympia einkaufszentrum to hauptbahnhof” classified as FindConnection, has two entities: olympia einkaufszentrum(Start Location) and hauptbahnhof(Destination Location).
After identifying intent and entities from the text, we can generate a query that can fetch the response from the database.
EXAMPLE SQL to FETCH from the database:
 SELECT ___ FROM CONNECTIONS
 WHERE START_LOCATION=‘olympia einkaufszentrum’
 AND END_LOCATION=‘hauptbahnhof’
 __________
 FURTHER CONDITIONS IF REQUIRED
 ___________
This is just an example of SQL to show you the importance of intent and entities to extract the right response for the user.
To recognize entities, we will make use of Spacy named entity recognition that already supports entity types like PERSON, FAC(Buildings, airports, highways), ORG( Companies, Organizations), GPE(Counties, States, Cities), etc. To see further entity types, refer to this link: https://spacy.io/api/annotation
doc = nlp('from olympia einkaufszentrum to hauptbahnhof')
for ent in doc.ents:
    print(ent.text,ent.label_)

As you can see, Spacy NER has identified these two entities from the text. However, this library only supports basic entities such as PERSON, LOCATION, etc. In our case, olympia einkaufszentrum should be marked as start location and hauptbahnhof as end location.
We can also train custom entities to support our problem. There are various entities extractor available such as CRFEntityExtractor, MitieEntityExtractor, EntitySynonymMapper, etc through which you can train your custom entities. You can even train Spacy NER with custom entities. This I leave up to you.
Conclusion
In this post, we saw an important concept of Natural Language Understanding which can deduce what the end-user mean. More precisely, it is an understanding part of Natural Language Processing that can digest, understand the text to output relevant responses to the end-user. Intents and Entities are the two crucial components of NLU, and we saw how to perform Intent Classification and Entity Recognition using a simple approach with the use of Spacy and scikit-learn. There are various chatbot platforms available that support NLU including RASA NLU, Dialog Flow, IBM Watson, Amazon Lex. RASA NLU would be a good starting point to start with to extract intent and entities from the text.
In addition to Natural Language Understanding, several other components are important in building AI-Assisted Chatbots. You need to explore the Slot filling section, memory element and much more to create a nice working conversational agent."
42,TALKO- A reception desk conversational agent,conversational agents,https://medium.com/design-with-code/talko-a-reception-desk-conversational-agent-29f0d65d47ab,"The reception area, located on the 2nd floor in N5 campus of the Srishti Institute of Art, Design, and Technology, is like the help desk for students for a variety of reasons. The receptionists make sure to solve issues such as managing ID cards, solving students’ queries about different faculty or courses, informing about the locations or changes, etc. For someone who is new to N5, this can be a little baffling. Moreover students can find it very confusing if there are no receptionists around to answer their queries or inform them about something or the other. Sometimes, some might even prefer to not talk to another person because of their own personal issues and insecurities. Thus, with the use of a conversational agent, can the reception area be made more user-friendly? Can the space, thus be used, even in the absence of a receptionist and serve some sort of purpose to users even if the receptionists are around and they choose to interact with it? Exploring this, the following concept has been finalized upon, after coming up with a few different ideas.
Concept note
TALKO is the conversational agent on the reception desk at N5 Campus in the Srishti Institute of Art Design and Technology. It works on both voice and text based control systems in order to be inclusive and allow the user a choice of their preference. This screen is projected onto the desk and appears only when needed. Projection Mapping has been used to map the buttons that a user presses on the screen. Projection Mapping also ensures that a user does not have to constantly be cautious while he places his arms on the desk, or is talking to the receptionists if they are there, or be wary like he’d have to if there was a screen embedded in the desk instead.
To activate it, the user has to speak into the microphone of the headphones that are connected to the desk and are placed at a side for access to the users. Once the words “Hi TALKO” are registered, the start up screen gets projected, allowing the user to use the conversational agent accordingly.
The conversational agent opens up to a screen asking the user to pick the category they’d like to talk about including- Faculty, Information, Admissions, Wellness, etc. Given that people aren’t too open about talking to the wellness team or even talking out loud about needing some sort of help, hopefully talking via text to the conversational agents about their whereabouts should be of some help. The Wellness option facilitates a user to talk to the conversational agent and fix an appointment with any of the counsellors available, in a time slot comfortable with the user. The Faculty option notifies the faculty if a student wants to meet them, but otherwise answers queries about locating some faculty or the other, knowing which faculty or administration person to approach for what, etc. The Information option is basically like an online library of sorts which contains all information about different majors, new notices about other miscellaneous things, etc. Admissions specifically caters to new people in N5 Campus of Srishti, who are there to enquire about admissions and aren’t students yet. The process of understanding the process of how Srishti works and how the admissions go can be simplified by talking to someone instead of by reading a brochure or pamphlet. Using the Equipment option, one can enquire about availability of equipment like cameras, or speakers, etc and also receive information on the procedure to acquire it. Other than this, there will be an Other option, for any other queries, where you can talk to TALKO about anything else that these categories seem to not cover, and TALKO shall respond accordingly.
The headphones with the microphone, are preferred over speakers, given that the reception area usually has someone or the other in the vicinity. What a user needs to know, or speaks about, to the conversational agent, doesn’t need to be a public declaration. The agent also does not need to be addressing everybody in the vicinity by having the answer being broadcasted through speakers. Hence, headphones and the microphone, along with the text option, are attempts at ensuring confidentiality for the users.
Buttons of a tick ✔ and a cross ✖ are always present at the side of the screen. To end a conversation the user has to click the tick icon, after which the user is asked for feedback on whether they are content with the conversation or not. The user can then answer via the tick or the cross again, and TALKO asks if they’d like to have a conversation about anything else. If there has been no conversation from the user’s end for two minutes, TALKO shuts by itself showing a default message.
This is to be implemented at the N5 Campus reception in an attempt to save a user’s time in case of the absence of a receptionist, or to talk about something they seem to not be able to, with the receptionist, thus making the reception area user-friendly."
43,Bots Are NOT About Conversation,conversational agents,https://medium.com/bots-for-business/bots-are-not-about-conversation-96cb5042de5a,"Most bot developers seem hung up on the conversational aspect of bots. Natural language processing (NLP) is a novelty for many (most?), so this obsession is understandable. But it’s one thing to get stuck with some technical challenge, and quite another thing to declare that very challenge as being the end goal.
If bots be essentially all about conversations then apps are essentially all about graphics. You see the error in that kind of thinking? Yes, apps rely on graphics when interacting with humans. And bots rely on text when interacting with humans. But neither graphics nor text define the purpose of those software products.
What Is The Purpose Of Bots?
The true meaning of the word bot is digital expert. These digital experts serve the purpose of delivering expertise that users expect. Do I have an example, you ask? Sure. Let’s look at digital experts in existence today. One such digital expert (or bot) is Homebrew.
This bot provides formidable expertise to all users who need to install stuff. Typical install utility expects a human operator to be the expert when wishing to install software. Homebrew is different. This bot lowers its expectations when interacting with humans. It assumes that humans are extremely bad at doing the legwork needed for installing software. Homebrew only expects humans to tell it what they want to install, and then it goes away and does the rest. Flawlessly!
So we see that the purpose of a bot is to serve humans. A bot needs to be able to amortize the chores. A bot needs to be able to do that while listening to and understanding human commands.
Conversational interface is merely a side effect of this important interaction with humans. Anyone who has used Homebrew or Git (or similar bots) had realized that text is much more expressive than graphics. Text is where bots thrive, and full service delivered by bots is where humans thrive."
44,Reading “Managing Context in a Conversational Agent”,conversational agents,https://medium.com/@yixingjiang/reading-managing-context-in-a-conversational-agent-e21d3bb3ef83,"Today I’m reading “Managing Context in a Conversational Agent” by Claude Sammut, here is a link to the paper: https://pdfs.semanticscholar.org/29b8/17100d8a873f3c91ae840264494bfd62b01e.pdf.
This one is not very hard to read. The main idea is authors developed a bot named “ProBot” which is based off “pattern => response”, which authors call scripting..

Then for conversations, we can have a stack based policy scripts to return responses.
Other thoughts from this paper:
how do we usually evaluate conversational agents?
This paper gave two approaches, paper evaluation, or collect responses from museum where bot is deployed and examine them.
This paper also mentions a spreading activation model, which is interesting, it has a Wiki page: https://en.wikipedia.org/wiki/Spreading_activation
I did not spend a lot of time on this but I took it as a way to search nodes in networks, when we have visited one node, we decay its value. In a conversation scenario, this means if one pattern is repeated multiple times, then the user was not happy about the previous output and asked for result again, by decaying values for the previous output we can get a new output."
45,Toward a fully Context-aware Conversational Agent,conversational agents,https://medium.com/@smartly_ai/toward-a-fully-context-aware-conversational-agent-787d23d279ca,"I was recently asked by my friend Bret Kinsella from voicebot.ai for my predictions on AI and Voice. You can find my 50 cents in the post 2017 Predictions From Voice-first Industry Leaders.
In this contribution, I mentioned the concept of speech metadata that I want to detail with you here.
As Voice App developper, when you have to deal with voice inputs coming from an Amazon Echo or a Google Home, the best you can get today is the transcription of the text pronounced by the user.
While It’s cool to finally have access to efficient speech to text engines, It’s a bit sad that in the process, so much valuable information is lost!
The reality of a conversational input is much more than just a sequence of words, It’s also about:
the people — is it John or Emma speaking?
the emotions — is Emma happy ? angry ? excited ? tired ? laughing ?
the environment — is she walking on a beach or stuck in a traffic jam?
local sounds — a door slam? a fire alarm? some birds tweeting ?.
Imagine now the possibilities, the intelligence of the conversations if we could have access to all this information: Huge!
But even we could go further.
It’s a known fact in communication that while interacting with someone, non-verbal communication is as important as verbal communication.
So why are we sticking to the verbal side of the conversation while interacting with Voice Apps ?
Speech metadata is all about the non verbal information, wich is in my opinion the immerged part of the iceberg and thus the more interesting to explore!
A good example of speech metadata is the combination of vision and voice processing in the movie Her.

With the addition of the camera, new conversations can happens, such as discussing the beauty of a sunset, the origin of an artwork or the composition of a chocolate bar!
Asteria is one of the many startups starting to offer this kind of rich interactions.

I think this is the way to go and that there would be a tremendous amount of innovative apps that will be unleashed by the availablily of the conversational metadata.
In particular, I hope from Amazon, Google & Microsoft to release some of this data in 2017 so we the developers can work on a fully context aware conversational agent."
46,"“Hey Alexa, What’s Up?”: Studies of In-Home Conversational Agents Usage.",conversational agents,https://medium.com/@coolime96/hey-alexa-whats-up-studies-of-in-home-conversational-agent-usage-5a78edcb0b1b,"What is this research paper about ?
Purpose :
The purpose of this research paper was to analyze the behavior patterns of Alexa users and seek for the space to make Alexa better.
Method and Sample Size :
It presented two complementary studies , one quantitive data study and interview study. The first one is collecting history logs from 75 Alexa participants in household living (278,000 commands). By google chrome extension of Alexa history page, it was possible to download participants’ logs and participants got a chance to exclude any content that may be too sensitive to share (but majority didn’t exclude anything). The other part was the interview with 6 households who have used Alexa for at least more than 6 months.
Commentary
It was really interesting research paper that looked deep on actual patterns and categories of conversational usage of Alexa. Some interesting outcomes that I found fascinating was that the most said command is ‘stop Alexa’ and households who have already bought Alexa tend to buy more and more Alexa by approximately 1 year. Larger size of households also tend to have higher satisfaction.
It would be also more compelling research also about behavioral pattern based on different age groups. Also, actually interviewing children could bring more insights, not only the parents’ reactions towards children’s usage of Alexa. But definitely, since children are the generation growing up with Alexa, rather than ‘using’ Alexa,
Based on the experience of making skills on Alexa during the last class, I totally understood the point of really limited usage of Alexa compared to the capability of Alexa. I think voice interaction agent like Alexa definitely needs more familiar, flexible on-boarding. Then it leaves another question, as long as Alexa is only responding to ‘Alelxa,’ how can Alexa introduce new skills or features to users?"
47,How we created the conversational agent for the RMI,conversational agents,https://medium.com/kunstmaan/how-we-created-the-conversational-agent-for-the-rmi-4434455862ce,"Either it’s too wet, or too cold, or waaaaay too hot.
It turns your barbecue party into a washout.
It makes sure you’re frozen to the bone when you go out for a run.
It has you sweating like a pig in your winter coat while waiting for the bus.
Surely it would be handy if you could ask someone about the weather, at any given moment. And that you would be given answers, bearing in mind your specific location and your personal situation.
We gathered all our experts in service and conversation design, copywriting and development round the table with the RMI, the Royal Meteorological Institute of Belgium. Together we designed an extension for your Google Assistant that tells you all about the local weather, tailored to your specific needs.
At Kunstmaan | Accenture Interactive, the process of creating a conversational agent starts with our UX strategists.They work with the client to define the end users needs & wants and design the whole human experience. The Senior Software Architect works on the available information in the backend systems of the client and what is technically feasable. The digital Copywriter writes out conversations based on the tone of voice defined in the human centric designs and splits it into user intents: Making sure the Assistant really understands what the user actually wants when asking something. Finally a Developer starts setting up the fullfillment layer and configures the Dialogflow project. Our Testers have been trained to write test cases for this new kind of interface.
Let’s dive a bit deeper and explain how we actually — technically — created the RMI conversational agent.
This is where Actions on Google comes in. Actions on Google is the platform on which anyone can publish Actions for the Google Assistant. The recommended way to develop for Actions on Google is the conversational agent tool called Dialogflow. Dialogflow wraps the functionality of the Actions SDK, and provides additional features such as an easy-to-use IDE, Natural Language Understanding (NLU), Machine Learning (ML), and more. Note that Dialogflow is actually owned by Google. It also allows you to create conversational agents for several other platforms as well (Facebook Messenger, Alexa, Slack,…), but that is out of the current scope.
A conversational agent on Dialogflow consists of several concepts, which are explained below:

Pictured above: the hobby entity in the RMI Dialogflow project, mapping several synonyms or related hobbies to the same value. The resulting value is used for populating our request to the RMI API endpoint.
Entities are sets of variables that are limited to certain values, so you can compare them to Enums. Dialogflow provides a set of predefined entities that are either a limited set (such as @sys.address, or @sys.flight-number), or unlimited sets (such as @sys.last-name), but it’s also possible to create your own entities. These entities are recognised by Dialogflow in user queries, and the resulting values are sent up to any webhooks if specified.
In our RMI project, several alternative wordings for watersport (surfing, sailing, swimming) are defined, so Dialogflow will all match these to the same value. The RMI API endpoint takes an integer as parameter for “hobby type”, which is why the value for the watersport entity is set to the value 3.
Similar to hobbies we use entities for clothing types, cities/locations, datetimes, and more.
Be sure to populate your custom entities in all supported languages though!

Pictured above: sample training phrases for the hobby intent. There are several ways to state the same intent, and recognised entities are clearly marked in different colors. A Dialogflow intent matches a user’s intent to do something, in this example the user wants to know if the weather is good for a certain hobby (in particular, the types of hobby defined in our hobby entity). You can define several ways the user could state this intent, with or without certain (optional) parameters. After providing these training phrases (in all languages), Dialogflow will be able to match user queries with the correct intent, even if they do not match completely by using Natural Language Understanding (NLU).

Pictured above: The list of entities used as parameters in our hobby intent. The hobby entity is marked as required, causing the conversational agent to automatically prompt the user if this is missing.
For each intent, you can provide a list of entities that might be mentioned in the user query, and mark some of them as required. Marking an entity as required will cause the conversational agent to automatically prompt the user for this value. As soon as all required entities in an intent are known, Dialogflow will resolve the intent. It’s possible to resolve the intent directly within Dialogflow (by providing a default static reply), or to trigger a webhook. In both options you will have the inputted entity variables available to use in your response, and in the case of a webhook these can be used in your business logic.

Pictured above: the “clothing” follow-up intent for the weather context.
Intents can provide an output context in which they can store certain variables. Other intents can be made triggerable only when certain contexts are active (as seen above), which makes them ideal for follow-up questions.
The easiest way to explain contexts are by using a real-world example. Imagine the following conversation:
What’s the weather this Thursday in Brussels?
RMI agent replies with the weather info for Brussels on Thursday.
Will I need sunglasses?
RMI agent replies to the question based on the weather for Brussels on Thursday
As you can see, the follow-up response takes into account the context of the conversation, as all weather intents store information such as location and date into the weather context. Follow-up intents can then utilise this context to fill in the gaps.

Pictured above: A basic text response for the exit intent. It’s also possible to use variable entities from the intent in the response.
For simple responses that don’t require any business logic it it possible to define them straight in Dialogflow. It’s also possible to provide specific visual responses for Actions on Google such as cards or carousels on phones or other devices with a display.
However, it’s also possible to trigger a webhook request for the response, which Dialogflow calls Fullfillment. These webhooks will do a POST request to an endpoint of your choosing. Note that the endpoint is the same for all intents, so the first thing you’ll need to do in your business logic is to differentiate between the different intents.
There are several ways to build your own webhook endpoint. Google’s recommendation is the Actions on Google Node.js library, which we’ve also used for this project. For other internal projects we’ve built prototypes in Java as well, so it doesn’t really matter which backend language you use, as long as you can easily work with JSON objects.
On completion of an intent that has Fullfillment enabled, Dialogflow will do a POST call to your endpoint with the intent, all relevant entities, and contexts. Your endpoint should be responsible for translating the Dialogflow JSON objects to valid API calls for your business logic, and format the response from the API calls back into response JSON objects for Dialogflow. All of this is required to take less than 5 seconds RTT, since the Google Assistant will mark your conversational agent as non-responsive if any response takes longer than 5 seconds.

Hooking up your Dialogflow project with your business logic via a fulfillment webhook completes your conversational agent. You can see an image above of this complete architecture, of which we’ve only scratched the surface. With Dialogflow as your conversational agent tool, you’re not just limited to the Google Assistant. It provides integrations with all major plagforms, including Facebook Messenger, Alexa, Slack, and others. On top of that, there are several SDK’s available for multiple platforms so your conversational agent can be fully integrated within your web-app, mobile app or even watch app."
48,Designing a chatbot with IBM Watson Assistant,conversational agents,https://medium.com/ibm-garage/designing-a-chatbot-with-ibm-watson-assistant-7e11b94c2b3d,"Chatbot, also known as “conversational agent”, is a trending technology. Chatbots are changing the business landscape. Its emergence in the enterprise has several implications that require some thought. Building a bot is not a hard task; with the rise of many platforms, it’s now easier than ever to develop and deploy one. The challenge with chatbots lies in delivering a good user experience, and they only present opportunities if done right. But, designing a conversation that meets consumer needs and returns real business value requires a nuanced strategy and in-depth considerations. The experience you are creating for your clients is paramount. Before moving quickly to development, figure out the problem areas that you, your users, and your employees are struggling with, and if a chatbot is the best fit to meet everyone’s needs. *
Deploying chatbots in the enterprise raises a host of potential issues that inevitably affect the deployment of enterprise software, including performance, scalability, and especially security. Enterprise chatbots may require access to user credentials, profile information, and enterprise data to perform useful functions. Any chatbot initiative must comply with enterprise cybersecurity standards. *
In this post, we’ll be taking a look at addressing some of these concerns and show how to design a chatbot using IBM Watson Assistant.

Figure 1: IBM Watson Assistant meet IBM Enterprise Design Thinking
What is exactly a chatbot?
In general terms, a chatbot is an artificial intelligence (AI) conversational agent, which conducts conversations via text or voice commands in a natural language conversation. It communicates and performs basic tasks such as answering questions or placing product orders. Chatbots’ primary purpose is to streamline interactions between people and services through messaging applications, websites, mobile apps, or through the telephone and interactive voice response (IVR). Other options include:
Used for service or as a marketing tool for engagement
Provide content, facilitate a purchase, or connect with consumers
Combine the ability to scale and personalization
Provide support
Suggest product recommendations
Leveraged for conversational marketing campaigns
The goal is to have chatbots written in a way to mimic spoken human speech to simulate a conversation with a real person performing any task. Chatbots can learn. Should you feed it a large number of conversation logs, the bot could be trained in a way to understand what type of question needed and what kind of answers to answer. Eventually the bot can be trained enough where you could not tell it is a bot.
“A computer would deserve to be called intelligent if it could deceive a human into believing that it was a human” — Alan Turing
These conversational bots are getting smarter. They are less prone to errors based on how they are trained. Hence, they can provide better customer experience and can help establish a better brand. There are tons of services to help you build chatbots, each with varying degrees of control, and these services have also become increasingly sophisticated. Unlike as seen in countless movies with “science fiction” depicted as awkward speech with robotic cadence, chatbots have come a long way! It’s safe to say that chatbots are still growing. Bots are available 24 hours a day, 7 days a week, 365 days a year and are capable of answering customers’ questions much quicker than human agents can.
Is chatbot the right solution?
Don’t build a conversation for building’s sake. Stall the idea of a conversation agent as a solution before clearly identifying what is it that you’re trying to achieve. First, ask yourself a few of these questions:
What is the focus area of the business that needs improvement now?
Who are all the users and stakeholders involved in this area, and how are they interacting with each other?
What’s not working for them? Why are they struggling?
What is the desired business outcome if we can change that?
If you already have a chatbot/conversation agent operating today, ask yourself:
Are they performing up to expectations? Are they meeting your KPIs?
Are users getting helpful answers?
Is the conversation designed well to serve the users needs?
Also think about potential pitfalls of chatbot:
Can bots put people at risk?
How are the bots trained and do they contain biased information?
Align together towards the vision
Once you have a clear business fit for a chatbot, then invite the stakeholders and sponsor users to a design thinking workshop. In this workshop, have the participants go through convergent and divergent activities to understand, explore, and define how chatbot fits into improving user experience. Use activities like Empathy Map and As-Is Scenario to understand the pain points experienced today. Identify areas where a conversational agent can help to improve the experience; look for areas where the user is asking the most questions and are the most confused and frustrated. These areas will be good candidates for where a chatbot can help. Then, prioritize with the team on which of the identified pain points will be the most important and feasible. Using the prioritized pain points as a guide, discuss and identify content, information, questions, and answers that could alleviate the pain points. Identify top priority areas that are the most import to your personas.
Dialogue design
In this blog, we will use IBM Watson Assistant to design a conversation. The archetypal chatbot consists of 3 specific concepts: intents, entities, dialog, as described in detail below. Let’s design a chatbot for your business and understand what it takes to create a purpose-built one that also delivers a superior conversational experience.

Figure 2: A typical approach used when deploying Watson Assistant
Intents
An intent is a purpose or goal expressed in a customer’s input, such as answering a question or a pressing a bill payment. By recognizing the intent expressed in a customer’s input, the Assistance service chooses the correct dialog flow for responding to it.
An intent is a category that defines a user’s goal or purpose
These categories are trained using representative examples
Recognizing the intents does not require knowing the specifics of the user request — it is a way to guide the dialog flow in the appropriate direction
Entities
An entity represents a class of object or a data type that is relevant to a user’s purpose. By recognizing the entities that are mentioned in the user’s input, the Assistance service can choose the specific actions to take to fulfill an intent. It’s also Watson’s way of handling significant parts of an input that should be used to alter the way it responds to the intent.
Entities are the subjects of intents
Entities are specific values that clarify user intent and trigger fine-tuned actions and responses
For each value, you can include a list of synonyms to capture the possible varieties in user expression
Entities represent information in the user input that is relevant to the user’s purpose.
System entities
System entities are common entities created by IBM that could be used across any use case. They are ready to be used as soon as you add them, such as:
@sys-date
@sys-time
@sys-number
Dialog
The dialog component of the Assistance service uses the intents and entities that are identified in the user’s input, plus context from the application, to interact with the user and ultimately provide a useful response. The dialog is represented graphically as a tree, so create a branch to process each intent that you define.
The dialog node is made of a trigger and a response, where the trigger is the condition
Dialog matches intents (what users say) to responses (what the bot says back)
Each dialog node contains, at a minimum, a condition and a response
The dialog response defines how to reply to the user

Figure 3: Depiction of a simple dialog
Slots
Add slots to a dialog node to gather multiple pieces of information from a user within that node. Slots collect data at the user’s pace. Details the user provides upfront are saved, and the service asks only for the details they do not. Slots…
Reduce development time
Get the information you need before you can respond accurately to the user
Answer follow-up questions without having to reestablish the user’s goal
Identify the type of information you want to extract from the user’s response
Our experience designing a chatbot
For a delightful user experience, you can’t run away from asking these core questions at the heart of your design, and each question needs to be carefully and meticulously considered with the different facets:
Is my product well-suited to be a bot?
How to design a meaningful conversation?
Will the chatbot understand the messages it receives?
How to create a simple design for an immediate and direct solution to a person’s problem?
Will the design solve the problem reliably?
What are the logical conversation path(s) for users to follow?
What are the integration points?
Can the bot handle more complex queries and tasks?
What user pain point does it solve?
What type of friction will it remove from the current process?
How to prevent the chatbot from asking the wrong questions and collecting unnecessary information?
When to override the bot (and allow for a graceful failover)?

Figure 4: Steve Jobs’ quote
Chatbot conversational flow
A chatbot conversational flow works like a decision tree, which gives you a comprehensive list of decisions, events, and outcomes as depicted below:

Figure 5: Basic transactions for a banking use case.
Prerequisites
To follow along, you are required to have these accounts and tools in place:
Start with an account on IBM Cloud. It’s free of cost to start with.
Setup Watson Assistance
If you want to follow along, the full solution can be found at the link: Create a banking chatbot with FAQ discovery, anger detection, and natural language understanding.

Figure 6: Watson banking chatbot
Steps to creating successful chatbots
Identify the problem to be solved and use case
Choose the channel for your bot (e.g. Facebook Messenger, Skype, Slack)
Choose the right use case for your bot
Choose the services you will use to build your chatbot (in this case we are planning to use IBM Watson)
Emulate conversations to train and retrain bot.
Test
Launch and learn
Included components:
IBM Watson Assistant: Build, test, and deploy a bot or virtual agent across mobile devices, messaging platforms, or even on a physical robot
IBM Watson Discovery: A cognitive search and content analytics engine for applications to identify patterns, trends, and actionable insights
IBM Watson Natural Language Understanding: Analyze text to extract meta-data from content such as concepts, entities, keywords, categories, sentiment, emotion, relations, and semantic roles, using natural language understanding
IBM Watson Tone Analyzer: Uses linguistic analysis to detect communication tones in written text
Node.js: An asynchronous event driven JavaScript runtime, designed to build scalable applications
Conclusion
In recent years, AI and machine learning have changed the way we go about our day-to-day business. Chatbots have conquered the market. No longer a nascent technology, chatbots are now mainstream. Its business impacts include breakthrough across different industries, such as financial services, retail, oilfield services, hospitals, and insurance companies. The chatbots market was worth USD 1274.428 million in 2018 and is projected to reach USD 7591.82 million by 2024, registering a CAGR of 34.75% over the period (2019–2024). According to IBM in 2017, 265 billion customer requests are recorded per year and businesses spend nearly $1.3 trillion to service these requests. Using chatbots can help them save up to 30% of this.
On a closing note, if you asked me the question, “Are chatbots a must-have?”, you can extrapolate and figure out on which side of the track that I’m seating! Chatbots are here to stay!
Food for thought: Intelligence of Chatbot
Can bots communicate intelligent insights?
Is Human Intelligence underrated? Is Artificial Intelligence overrated, an actual breakthrough, or still in its infancy?
Will bots eventually be equipped to deal with complex issues such as geoeconomic and geopolitical problems?
Attribution
Special thank you to the IBM Garage Singapore team: Oliver Senti, Practice Manager; Eunice Shin, Sr UX Designer; Ma Li, Architect / Data Scientist; Iris Tan, UX Designer and Thanh Son Le from the Cognitive Solutions Engineer, IBM Watson and Cloud Platform Expert & Delivery Services.
References
For more details on designing and building chatbots, please refer to the links below where you can find many read up on AI-powered chatbots:
Chatbot Best Practices
Introduction to Watson Assistant
Watson Assistant Tooling Overview
Webinar — Building a ChatBot using IBM Watson Conversation Service
How to build dialog in your chatbot with Watson Assistant
Chatbot Tutorial | AI in Marketing
Conversational A.I. for the Enterprise
The Future of Banking: A Financial Concierge for Everyone
Chatbot application Life cycle
Chatbot Architecture
What We Learned Designing a Chatbot for Banking
Design Framework for Chatbots
Chatbot Design Canvas
How chatbots can help reduce customer service costs by 30%
Bring your plan to the IBM Garage.
Are you ready to learn more about working with the IBM Garage? We’re here to help. Contact us today to schedule time to speak with a Garage expert about your next big idea. Learn about our IBM Garage Method, the design, development and startup communities we work in, and the deep expertise and capabilities we bring to the table."
49,Conversational interface for chatbot & voicebot: the French touch,voicebot,https://blog.prototypr.io/conversational-interface-for-chatbot-voicebot-the-french-touch-28a1d5522ec3,"Now a new form of expression is available to brands: they can express their image through a specific and more friendly tone of voice, so users can feel more included and emotionally linked.
Can you explain to us what a conversational experience is?
It’s a basic service: a virtual assistant that helps people with daily tasks, or to quickly obtain information.
A conversational interface enables users to interact with this service, or with a brand, in an easier and a more natural way. Customers only have to write a few lines or only have to speak: “What’s the best deal for a trip to Australia?”, “I want to book an Uber”, “How much is an electric bike?”. This conversation can be accessed from a website or a mobile app, and also via typical daily channels (e.g. Messenger, Google Assistant, WhatsApp, Slack, etc.).
These days, more and more devices are dedicated to a user vocal experience (smartwatch, speakers, GPS, etc.), fitted out with microphones and AI, like Google Assistant for Google Home or Android devices. A lot of brands choose IoT so they can offer their clients a more natural and direct service with vocal input.
These kinds of products offer customers new options which are evolving very fast. As designers, we have the responsibility of fully exploring their potential and we need to make sure we design experiences adapted to user’s needs.
How can we explain the popularity and fast development of these tools?
With a 100 % live vocal interaction non-digital people can use a service like anyone else.
A more human-like interaction: Users can express their needs, or ask their questions in any way they like, in any order they like. The bot will adapt and respond.
Accessibility: It’s a great opportunity for non-digital people, especially the visually impaired, to access a digital service! Currently their only option is a mechanical synthesised voice reading out all screen content. With a 100 % live vocal interaction they can use a service like anyone else.
More efficiency, fewer constraints: The conversational experience helps to decrease the cognitive load and will ease the user journey. On a typical website, the user has to carry out a certain number of steps including: interface analyse, comprehension of the website, product identification and selection. Through a conversational experience, the user’s cognitive load is significantly reduced thanks to AI.
Time saving: Long waiting times on the phone, endless redirections, and repetitive tasks, can now be avoided!
I guess there are business advantages too?
Brand identity: Now a new form of expression is available to brands: they can express their image through a specific and more friendly tone of voice, so users can feel more included and emotionally linked.
Increase key knowledge: Brands can combine these new tech tools with their user base to create a better customer experience. It can save valuable time for sales people and customer services, and free them up to focus more on a good quality service.
Users shouldn’t have to look for information by themselves anymore.
A bigger connexion: Today, I think e-commerce companies should be more connected to their clients via the daily channels they use (Messenger, Slack, etc.). In my opinion, users shouldn’t have to look for information by themselves anymore. That’s why, with their authorization, the OUI.sncf bot sends personalized train times directly to the passengers, when it knows they need it at a specific time, so the users don’t have to request the information at all.
What are you currently working on? What are your challenges?
I currently have the opportunity to explore different sectors, like transportation with OUI.sncf, the medical field with the DeepOP start-up and service delivery with the SMARTLY start-up. I have designed customized vocal experiences and bots for platforms like Messenger, Google Assistant, Google Home and Alexa.



These projects enabled me to approach vocal use in several completely different contexts. For example, a nurse, who gives medical data to a virtual assistant, will not have the same conversation as a customer buying a train ticket. Also security and legal challenges aren’t the same. A conversational experience for family connected speakers shouldn’t be designed in the same way as a personal device.
Do you see differences between B2B and B2C?
A crucial part of the job is to work on the bot persona, to define its personality, tone of voice and specific goals.
Yes! For B2C applications, the main challenge is convincing users to adopt this new tool; to be successful a tool must be relevant, especially if you want users to change their habits. The key is the trust users are willing to give a brand, and the emotional attachment between them. A crucial part of the job is to work on the bot persona, to define its personality, tone of voice and specific goals.
For B2B, if you are designing an HR or a legal conversational tool for example, the challenge is more about efficiency.
I know you worked on mobile apps, interactive kiosks and websites. What do screen content design and conversational design have in common?
The design approach is just the same! You have to know end-users very well. My work is also based on personas and on the study of customer needs and pain points.
However, we have a lot more knowledge about website or app design. For conversational design, especially voice interaction, there is no design pattern and no best practice exists. I had to start from scratch.
After a lot of testing, I have been able to identify some best practices to make information processing easier. In a vocal experience, humans have a very short-term memory, so if a voice bot gives three or four precise pieces of information in the same sentence like train times, users can usually only remember half of them.
How do you work?
After the user research phase, I design the “happy path”, i.e. the conversation when everything is going well, then the other cases (e.g. the user says “I don’t know” or “no”, etc.). Finally, I have to focus on “repair conversations” which is equivalent to the error messages and redirections on a website.

Happy path example for Oui.sncf — Google Assistant
The more you anticipate specific cases, the better the experience will be.
A huge part of the work is to analyse and identify contexts in order to adapt the design to typical conversations in a given setting. I use macrographs to represent the whole user experience, which gives an immediate overview of the project and also helps to define product scope and goals. The other main specific deliverables are Context map, Intelligence rules and Dialog flows. The more you anticipate specific cases, the better the experience will be. In particular, we pay great attention to non-response scenarios.
Finally, I also include a plan to measure success (KPIs), including giving users the opportunity to say if they are pleased with their experience via pre-set answer suggestions (“Thanks”, “Great!”).

Context mapping
What advice would you give to UX designers, who want enter this field?
First of all, do not change the classic design approach! Like in any project, do a lot of co-design workshops with your client to define your persona bot, identify and prioritize your goals, and choose your wording very carefully.
Above all, test and test again, I will never stop repeating that. Your creation is worthless if it hasn’t been tested on real end-users. Feedback from user testing is essential and irreplaceable: “I never have a problem buying my ticket, the problem is finding the train”, “I had to interrupt the bot to give it more medical information”, “I feel I am forced to choose from only a small range of products. I want to have more choice”. You never know in advance if you are going to have to change your project strategy at any given time.
Don’t hesitate to invite your client to assist in test sessions. In my experience, a “Wizard of Oz” method of VUX (Voice User Experience) testing thrilled a lot of my clients because they could see how their product could be used by the users once developed.
Finally, if you are not good at writing, never hesitate to work with a copywriter!
What do you see in the future for conversational experience?
I believe in the automation of recurrent tasks to relieve humans of boring work. Bots can help humans in a non-intrusive way (notification) to remember or become aware of myriad of things.
Most of my clients choose these new platforms for business reasons. My goal is to make them realize that without a positive experience it’s hard to guarantee the adoption of a conversational tool. Given that GAFA offer conversation templates adapted to e-commerce, client service, etc., we can focus on the value and quality of the service we want to deliver.
How do you rate France in relation to conversational tool design and development? Are we leaders, good students or only beginners?
I’m optimistic so I would say we are good students. Tech is here to stay and companies are aware of the UX approach. There is no doubt, in a near future conversational experiences are going to be more and more popular!

Amina Esselimani is a conversational UX designer. She graduated from L’École de Design de Nantes Atlantique in 2011. She’s passionate about user research and testing applied to chatbots and voicebots (B2B and B2C). Putting AI at the the service of the user experience is her daily challenge."
50,How We Built Our First Voice Bot: The Business Story,voicebot,https://chatbotsmagazine.com/https-medium-com-hellohaptik-how-we-built-our-first-voice-bot-the-business-story-4441315b218e,"2017 kicked off the dawn of the Voice Assistant a.k.a the Voicebot.
Alexa, Google Assistant and Siri, all won a place for themselves in customers hearts, homes and retail shelves everywhere.
Their popularity keeps growing by the day and the reason behind this hype is quite obvious. They make life so convenient! All you need do is talk, which is naturally easier than typing or tapping buttons on your phone to get things done.
But how can a company use a voicebot? What difference is this going to make for businesses?
Here’s our story from the forefront of this new vocal revolution that should answer what a voicebot can do for you.
What’s a Voicebot?
A voicebot is a type of chatbot. Being a Conversational AI company, we had over 4+ years of experience building bots for a text-based interface.
But building for Voice was fairly different and a challenge that we were raring to work on. That’s when we received our first client request to build a voicebot for a fast food chain.
Types of Voicebots
There is no well-defined manual to follow when you’re developing new technology. Based on the interface, voice bots fall into the following two types:
1) Voice + Text Bots — Hybrid Voice + Chat Support Model

If you’ve used Siri on your iPhone or Google Assistant on your Android phone, you know about Voice + Text bots. These are text-based bots with a layer of voice on top of it, and the input form is speech as well as text.
A lot of companies have lately started including voice support on their text-based bots, like Axis Bank’s customer support app: ‘Axis AHA’.
2) Voice Only Bots — Voice Controlled Devices

Alexa on Amazon’s Echo speakers and Google Assistant on Google Home devices are the most popular voice assistants in the market. These beautiful products can handle everything from simple tasks like setting alarms and playing music to more complex tasks like controlling your gadgets and turning your house into a smart home.
Our client required both these types of bots to deploy as an Alexa skill and a voice supported text-bot for their website and mobile apps.
Building The Bots
1) Voice + Text Bot — Hybrid Voice + Chat Support Model
We realised at the nascent stages of our first ‘voice project’ that text-based bots can’t directly be converted to a voice + text bot. Thinking voice-first was the key to laying the groundwork.
Challenges
The biggest challenge was taking a ground-up approach to designing chat flows. Adding a voice layer on top of a text-only bot doesn’t mean simply adding the tech support for it, i.e., it’s not just a one-line code change to your bot.
❌ Bad Experience Design
When the voice script and chat script are the same. It’s easier for the user to read the text rather than wait for the bot to stop speaking in some cases. The voice script needs to be less verbose and to the point.
✅ Good Experience Design
Keep the voice script different from the chat script. Having the voice script more precise and to the point helps enhance the experience.

Final Results
In our case, we already had a chatbot platform which was text-based and it was simply a matter of adding a layer of voice on top of it.
Here are our favourite Speech to Text and Text to Speech APIs:
1. Bing API — It offers a punctuation feature which isn’t there in Google’s speech-to-text API, which makes a distinct difference.

2. Amazon Polly — With Alexa dominating the voice bot world, we didn’t have to think twice about which API to use. It also helped that we’ve been using Amazon Polly for quite a while on our app! Read more about it here.
2) Voice Only Bot — Voice Controlled Device
In layman’s terms — an Alexa Skill is to an Alexa device what an iOS app is to an iPhone. As of March 2018, there are more than 30,000 Alexa Skills in the U.S alone.
While building an Alexa skill or Google Home command, the User Experience structure is very different from that of a text-based bot. This applies to voice + text bots too. This is because of the missing visual interface. The Echo Spot is the only Echo device with a screen, which has a low market share as of now.
Challenges
Using only voice as the mode of communication can be tricky for the product designers in some ways. Taking information like addresses can be very easy for text-based bots but doing this on a voice controlled device just doesn’t work. The industry-wide practice is to take this kind of data using the companion app of the voice device. For example, the Zomato Alexa skill directly pulls a user’s address from his Zomato account. This can be managed through Alexa’s companion app on their phone.
Since this is not great UX, this process is not to be used too frequently. The point of having a voice-controlled product is lost if a user needs to use his phone too often.
❌ Bad Experience Design
No guidance for the next steps or re-prompt in case of no reply.
✅ Good Experience Design
It’s very important to guide the user in the right direction and help them fulfil their final goal. Following up with the user if there is no reply is essential. This mirrors ‘Quick Replies’ on most Messengers where you can simply tap a suggested option to send a message.

Final Results
Our team designed an Alexa Skill ground up for our client that can collect orders, describe the menu and even tell you funny chicken jokes!
We used our backend to make the Alexa bot which consumed all the APIs and converted that into information which can be presented to the end user. For instance, we utilize the Menu API and communicated an information-heavy food menu in a way which feels conversational and minimalistic to the end user.
Key Takeaways
Voice support for a text-based bot is not an ON/OFF switch
Even if you’re converting a chatbot into a voice bot, think voice-first bot from the beginning of the project.
The communication that you use should be different
You don’t speak to a person and write to a person in the exact same way. Don’t use the same chat script and voice script.
The user should be able to understand the capability of the bot
Making your product easy-to-use is essential. I didn’t start using Siri until I saw someone else use it correctly — here is the link to that ad if you’re interested.
Give hints on the next steps in the user’s journey
It is very important to communicate the usability of the platform to your end user throughout the conversation.
Alexa only gives you 8 seconds
If there is no interaction between the user and Alexa then the device automatically exits your skill. A prompt is sent after 8 seconds, the device exits the Alexa skill if the user fails to respond again.
Which type of voicebot should your business have?
Voicebots suit the modern consumer’s lifestyle extremely well. Making your services available on these platforms is a sure-fire way to reach a bracket of customers who wish to get things done as quickly and conveniently as possible. So if your business use case is something which can be conversational, like ordering food/groceries or even customer support, then certainly consider getting a voice bot.
Voice + Text Bot — Hybrid Voice + Chat Support Model
Your user wouldn’t have access to your Alexa device while their travelling and would need to use your phone to complete a task. Also, if your bot is expected to communicate a lot of information to a user, like a menu from multiple restaurants or a long list which is information heavy, then you should leverage the visual interface of voice + text bots. The Echo Spot is evidence that a screen can be leveraged to enhance user experience.
Voice Only Bot — Voice Controlled Device
If you’re looking to add another channel through which you can do business then a Google Home command or an Alexa skill is the right step for you. It most definitely helps to have a large user-base. Uber, Zomato and Dominos are some companies who have seen great success with their Alexa Skills and Google Home commands because discoverability wasn’t a problem for them.
It helps to maintain a presence on both these platforms if your service is likely to be used both on-the-move and at home.
Conversational interfaces have been around for a while now, and voice bots are a great way for businesses to use automation and connect with the world at a human level.
We’ll soon be talking about the tech side of building our first voice bot. So, stay tuned. Considering a bot for your business or want to know about implementing a voicebot?"
51,Voicebot development insights from the winners of the Actions on Google Developer Challenge,voicebot,https://medium.com/@memerunner/voicebot-development-insights-from-the-winners-of-the-actions-on-google-developer-challenge-e3fade1d8ac3,"This winter it was as if CES was trying to prove to the holiday season that it could create the most buzz around voice assistants. So at least heading into 2018, it looks to be an exciting year for the #VoiceFirst movement.
And Google recently announced winners of the Actions on Google Developer Challenge, with awards and cash going to what they judged were the best voicebots on the Google Assistant platform in 2017. It’s about time for someone to launch an awards show called the Bottys.
At any rate, it’s worth looking into the winners and seeing what they’re doing. While we don’t have hard data to indicate if what they’re doing is working, at least they’re getting Google’s nod of approval.
So I’m going to analyze the first, second and third place winners to see what we can learn from them.
And without further ado, here they are.
100 Years Ago: An app that travels back in time 100 years and lets you listen to an interactive radio show, including breaking news and the latest hit songs circa 1917
Credit Card Helper: Helps users find the best credit cards quickly and avoid traps.
My Adventure Book: Storytelling game that lets users navigate their own adventure.
There are 4 areas we’ll assess to better understand how these apps work.
Invocation
User interface
Landing page content
Developer comments
Invocation
The invocation is what you say to Google to get to the app. It’s the name. Now, voice apps aren’t that easy to discover. It’s not like people surf Alexa or Home like they might the web. Currently, users take a linear path through the medium, and typically have a clear idea of what they want to do or find out, if not specifically where they want to go. Categorically, audio information is consumed more linearly than graphic information.
The invocation is no small part of an apps success. The easier to say and remember, the more likely people will be to actually use it.
Here are the invocations for the three winners.
100 Years Ago
“Talk to 100 Years Ago”
Credit Card Helper
“Ask credit card helper what the best credit card is”
“Ask credit card helper what the risks of credit cards are”
“Ask credit card helper what to do if my card is stolen”
“Ask credit card helper what type of credit card i should get”
“Talk to credit card helper”
My Adventure Book
“Talk to My Adventure Book”
Credit card helper is using a range of invocations to touch on different user interests. Consider the difference between “what the best credit card is” and “what to do if my card is stolen”. The first one could drive users directly into a new customer funnel, and the second helping an existing customer’s make account changes. Both great features to offer, addressing people at different stages of the customer experience path.
So developers and marketers will want to use terms that are easily remembered, easy to say, distinct from other voice apps, and offer directions to specific sections of the app, where possible.
User Interface
Once the user passes through to the app, it’s imperative to create a positive experience as quickly as possible. Like websites and mobile apps before them, voicebots will likely succeed or fail based on the first 30 seconds of the experience.
And here’s how each app greets you after a successful invocation. These screens were grabbed while interfacing with Google Assistant on the phone, as it displays the script the Voicebot speaks for easy reference here.
100 Years Ago

Credit Card Helper

My Adventure Book

You can see the relative simplicity of these apps. And of course that’s to be expected from first forays into new territory. However, even early web pages often provided link after link. This is an early indicator of what will likely be a challenge for the voice web for a while to come: The small amount of content that users can comfortably navigate and consume.
One UX issue I noticed is, when going back through an app repeatedly, it’s great when you can skip through large sections of the script that you’ve already been through. Especially at the intro. Credit Card Helper did this very well, and it removed a sense of tedium from the process. You just have to say “Hey Google, skip” and you’re on to the next section.
Here’s the main navigation structure for each app.
100 Years Ago
Hear the news
Listen to music
Speak to special guest
Credit Card Helper
Help finding a card
Browse by category
About our research
My Adventure Book
Single launching point with multiple paths to follow
It makes sense the simplicity of the navigation reflects the simplicity of the overall app experience, although Credit Card Helper has extensive content on each of the credit cards in its database.
A frequent discussion point in voicebot design is the optimal depth for a nav. I’ve heard more than one person say 3 is ideal. I think that might be where we are now, although I’m sure that will expand as people become more familiar with the technology.
Additionally, the AI technology behind the speech interface is also going to improve dramatically, increasing the accuracy and quality of experience.
App directory
Due to the invocation process for voicebots, one of the major marketing challenges is getting users to remember the exact invocation name. You might hear it on the radio, or see it in an ad, but when you go to visit the voicebot, via voice, you have to remember what’s likely to be a 3 word name. Not a simple task, especially when you consider marketers spend bazillions just to get consumers to remember their simple brand name.
To help with finding apps, Google has created an app directory where each voicebot has its own page. This is an opportunity to prep the visitor for their upcoming user experience.
Here are the Google Directory pages for each app.
100 Years Ago

Credit Card Helper

My Adventure Book

Firstly, you can see how the many invocation phrases on the Credit Card Helper stands out, offering the user additional ways to invoke and discover the app.
The description area offers brands a place to briefly (hopefully) summarize why users should visit the app. You can bet there will be a lot of discussions within marketing departments over what goes on this page. Suffice it to say that well written copy with a clear sense of the app’s mission is going to be critical.
I’d also guess that Google will expand on the media available for presentation on this page.
Developer Comments
Given the recency of this industry, there’s not much in the way of best practices or industry standards. Brands are going to need to use this first wave of apps for as much design and user experience information as they can get.
In addition to reviewing each app, I contacted the app creators to get additional insights about the app development process. So here are some of the highlights and challenges they reported.
100 Years Ago developer Jesse Vig used feedback from a previous app he developed to guide the direction for his winning voicebot. According to Jesse:
Prior to building 100 Years Ago, I built another action called Time Machine that reads headlines from the past and plays a brief time travel sound effect. Surprisingly, most of the positive feedback I got was about the sound effect. Based on this experience, I wanted to create a richer audio experience with 100 Years Ago.
Arun Rao, CEO of Starbutter, the company that developed Credit Card Helper, also has strategic advice for app developers:
On the design side, state your Action’s key objectives early and try to design a “Wow” experience around them. If you do too much or don’t have a clear objective, your Action won’t be interesting. The first part of “Wow” is to not do anything really dumb — which takes a lot of user testing to figure out.
I previously mentioned the challenges of designing information for a voice interface. Jesse Vig also addresses this, saying:
I was able to create a much richer experience when the action had access to a visual display, as in the case of Google Assistant on mobile devices. Reading is faster than listening, and obviously images cannot be conveyed through an audio interface.
Arun Rao has some good advice for approaching new apps:
For use cases, think about where voice or chat interactivity adds much more value than a current experience. Test this out with prototypes or videos before you build anything (BotSociety is a good prototyping tool to start with).
He also offers up a valuable technical recommendation for maximizing app performance:
On the technical side, go serverless and use Google Cloud Functions or Amazon Lambda. These are efficient and more scalable and error-proof for webhooks than having a real or virtual server.
Along a similar vein, Nick Laing, developer of My Adventure Book, suggested new developers use the existing tools.
My advice to any beginners is to start with DialogFlow, there is a lot you can do with that console alone. Once you get familiar with the platform you can write your own code to expand the functionality.
As the voice assistant industry enters it’s 4th year as a consumer gadget, there’s enormous potential on the horizon. These early examples are the tip of a large and growing iceberg. (And Earth needs more ice, right?) If one thing is certain, it’s that the devices, the apps, and the user base, are all going to evolve considerably over the next few years. It should be an exciting race.
Thanks to Arun Rao, Jesse Vig and Nick Laing, not only for their work but their generosity in providing guidance for other app developers.
I’ll be reviewing more voicebots in the coming weeks, so if you’d like to stay up on the latest creations, please follow me."
52,Voicebot.ai Interview with VoiceLabs,voicebot,https://medium.com/@marchick/voicebot-ai-interview-with-voicelabs-5906d2ea71a6,"What did you see in the market that motivated you to start VoiceLabs?
Adam Marchick: The first thing I want to do is give a lot of credit to my cofounder Alex Linares, the Chief Product Officer at VoiceLabs. I was skiing and on sabbatical because I hadn’t had a vacation in five years, and was thinking about doing something new. Alex was an Entrepreneur-in-Residence at The Chernin Group and gave me a call. “Have you checked out this Echo thing.” Alex is an amazing product person. He built the number one Facebook app. He built one of the first mobile apps to reach one million downloads from the App Store. Alex told me, “This Echo thing is amazing. Once I started using it, I’m never going back.” I’m an enterprise software nerd so I said, “Where is the AdMob, Flurry or Kahuna here.” They didn’t exist.
We saw that developers had no help with analytics or monetization. We started talking to people about their critical needs. I was fortunate enough to be on the Facebook growth team and had immersed in that emerging ecosystem. So, I asked similar questions to Alexa developers: “Do you know how users are experiencing your apps?” They said, “No. I tried trying to use mobile analytics but it didn’t work for voice.” When we dug in, we realized there were differences between voice, mobile and web that required specialized analytics.
What problem do you solve?
Marchick: Analytics for voice applications. A lot of this comes from my background in actionable analytics. I started asking questions, “What does my voice usage look like? Are people opening my Amazon skill and leaving delighted or frustrated? Are they opening the skills repeatedly and with what usage patterns? Are they getting the right answers? How are they navigating?” That is where the creation of voice pathing came from. VoiceLabs is the next generation of funnel analytics made for voice.

VoiceLabs’ Voice Pathing showing a successful voice session in a developer’s Voice app. “Successful sessions are the key to a delightful consumer experience,” said VoiceLabs CEO Adam Marchick.
Are you building Google Analytics for the voice web? What else do you have on the horizon?
Marchick: I like to think of this as more Mixpanel because we are going to be cross-platform and I really respect them, but Google Analytics is also a good analogy. It’s not just, “here are your metrics.” It is also, “What was your goal conversion rate and monetization?” It is voice-specific, which is unique. In mobile and web, the click-throughs matter a lot. In voice, it is lot about what is being said, the Intent and how Voice bots respond to spoken words.
On the web, you only care that they entered the email address. Great. Here [in voice] it matters a lot what they said and how Alexa responded. Looking at logs and pulling out the speech-to-text. It’s more than just looking at interaction metrics. We are also evaluating the responses provided by Alexa. That is where you really see the consumer experience play out. That is where the analytics are critical and the insights are really actionable.
You mention in a blog post that our devices will largely determine which assistants we use because their will be too much hassle to switch. Given that statement, which platforms are you betting on will have the biggest user base?
Marchick: Right now, it’s impressive that Alexa is on the Echo, FireTVs and all of Amazon’s Fire Tablets. By Q1, should be about 40 million devices. It’s no secret that Apple has 1 billion iPhones sold, so Siri will be big. Google Assistant will be big part of this story because of Android. Samsung, with the acquisition of Viv, is now important. Microsoft is smart, so something will happen with Cortana, and they have 20 million Xboxes so they have a built-in user base.
There are Access Points and Voice Interfaces. There is no question that Apple and Android have the largest number of Access Points, many with active voice usage (but not a majority). What is awesome about the Echo is that everyone with one actively uses voice.
How do you expect the introduction of Google Home to impact the consumer market?
Marchick: People are really excited about it. I think consumers have a lot of brand affinity for Google. We still don’t know exactly what it will and won’t do. What Amazon got right is streaming music and home automation. It will be exciting to see what Google gets right.
I am an advocate for the third party developer. Being part of building the Facebook and mobile ecosystems, I recognize that you have to make it valuable for developers to spend time in your platform. That speaks to an open ecosystem. By contrast, Google AdWords is not an open ecosystem but it is enormous. Google is the market leader in creating an open ecosystem, Android, and a closed ecosystem, AdWords. In this market, you need as many great people innovating as possible. So, I am assuming it will be open and developer-friendly whether they have a skill, app or some other model. It could be different than a traditional app model, but Google has told us what that different way is yet.
Many people view Alexa as the first real voice assistant at any meaningful scale, even though Siri would like to make the claim as well and Google’s variously named voice assistants may have a stronger case to make. How do you distinguish between what Siri and Google Now have done previously and Alexa, if at all?
Marchick: It goes back to this idea of an open ecosystem. If you look at voice searches, there is a distinction between “Call Mom” and “I want to order a pizza.” The idea that voice search leads to third parties getting involved with complex interactions is much more sophisticated than making a call that Apple can control completely on its own platform. The Amazon Echo worked with other people to make stuff happen. That is different and leads to a whole new level of innovation. I can get a great recipe from the Food Network while I’m in the kitchen. That is awesome. Siri or Google would return search results and you start clicking and are back to text based stuff, and that doesn’t work when my hands are messy with tomato sauce, because unfortunately I am not a culinary whiz.
Your presentation tonight to the NYC Alexa Meetup group suggests that sessions per day are the key metric that Alexa skill developers should focus on. Why sessions per day specifically, and not duration of session, number of interactions per session or number of users?
Marchick: It’s a metric of engagement. I wanted the talk to focus on doubling your engagement. We can also talk about the other metrics and that would be a different presentation. I’m a growth marketer and I learned from the best in the business by being lucky enough to work at Facebook — there other metrics that are just as important. I am just focusing on sessions per day for that talk.
Monetization or lack thereof is a topic that comes up in some of the Alexa developer forums. You mention in-skill advertising in your presentation. How do you see monetization playing out on Alexa and other platforms? Will it be advertising or another play, and why?
Marchick: There are still some structural issues to making advertising valuable. History repeats itself. Getting the ecosystem to scale is critical. Once you have that, you have to determine the best way to monetize. Scale makes the world go around. That is what we are helping people with. Until there is scale, it won’t be clear what is best. We actually have built a voice ad network, and it’s ready whenever the market is ready.
Who should be building for the voice web / voice ecosystem?
Marchick: Let’s look at it another way. Who shouldn’t be building for it — someone who just wants to have a placeholder in the voice ecosystem. They won’t be satisfied with what happens because distribution and acquisition isn’t that functional yet. You have to be committed to building something that is interesting and useful for consumers. Otherwise, you will have no installs or repeat users. In mobile, you could build a mediocre app and buy a million installs. You can’t do that here.
What is your favorite voice-enabled skill, application or utility?
Marchick: I use Amazon Music daily. There is no question, the first two billion-dollar voice applications are streaming music and home automation. That said, we are starting to see some innovation. I used the Yahoo Fantasy football skill this weekend and it was pretty fun."
53,"AI & NLP Workshop: Learn how to make an AI powered, NLP based Voice Bot",voicebot,https://becominghuman.ai/ai-nlp-workshop-learn-how-to-make-an-ai-powered-nlp-based-voice-bot-23b1d5575f0b,"Build a Marketing/Customer Service Voice Bot in our All Day Workshop
AI is no longer the future… it is now the present. AI has quickly gone from the innovation labs to being fully implemented from one department to the next. Take a look at the recent successes:
Chase: JPMorgan’s COIN, has saved over 360,000 hours of manpower! What does it do? COIN reads contracts and makes sure there are no errors.
HiCharlie: This bot keep track of your finances so you don’t have to. On average Charlie saves users $80 per week!
Sweedbank: Their chatbot, Nina, handles 40,000 conversations a month and resolves 81% of the issues.
Trim: Trim is Financial Assistant bot that can help you reduce expenses and even negotiate bills on your behalf. Trim has a 94% retention rate!
The AI revolution needs people that understand the technology and can take it to the next level.
Currently there is a $150 Discount for this Workshop but there are only a few tickets left. RSVP to claim the discount.

Learn how to make an AI powered, NLP based Voice Bot
In this workshop you will learn from industry experts on AI/ML/NLP and will be working on building a Customer Service or Marketing Chatbot which you can launch on Messenger, your website or on Google Voice or Alexa.

The Schedule

This is a full day workshop, that starts at 9am and ends at 5pm. Breakfast, coffee, and lunch will be provided ☕️

Let’s meet the instructors

By the end of this course you will have designed, built and launched an intelligent Customer Service Chatbot using SmartLoop.Ai technologies along with RASA’s open source NLP which is built on Tensorflow.
Ideal for
Anyone Interested in VOICE, AI, or Bots
Product Managers
Designers
Developers
Marketers,
Customer Service Department
Human Resources
Reasons to attend:
Learn the basics of Machine Learning and AI
Learn to collect data, label and classify
Easy build NLP, Deep Learning applications and get started in no time"
54,Converse.AI and Actions on Google enable every business to build a voice bot without writing code.,voicebot,https://blog.converse.ai/converse-ai-and-actions-on-google-enable-every-business-to-build-a-voice-bot-without-writing-code-b703e1f04dc6,"Converse.AI, in partnership with Actions on Google, have launched the easiest and simplest way to build a voice bot, for any business, without the need to write any code.
Converse.AI enables companies to easily build bots, capable of answering questions, holding conversations, managing workflow and integrating into external systems, using it’s unique Chatflow capabilities.
Actions on Google enables anyone to build for the Google Assistant. Your integrations can help you engage users through Google Home today, and in the future, through Pixel, Google Allo, and many other experiences where the Google Assistant will be available.
It is the first voice platform to be integrated with Converse.AI and the combination of both platforms enables any business to have their own voice bot, without having to use scarce developer resources.
The first Google Action launched on the Converse.AI platform, Bay Transport, enables the user to track their preferred route on the BART transport system, set their home stations and easily discover when the next available train will be arriving.

The interest in voice automated services is set to grow significantly in the coming months and the use cases are endless. The ability to build and rapidly iterate a voice activated bot, will soon be crucial for businesses wanting to increase engagement with their customers.
Image being able to check your bank balance find out the delivery schedule of your latest online purchase, finding out film listings at your nearest cinema and booking tickets, and much more.
This first Google Action from Converse.AI is just the first step in proving what is already possible, we are really excited to see what you build!
Get started by signing up or checking out our documentation!"
55,"Voicebots arrive in Colombia with Google Assistant
We interview Juan Nates, managing partner of Chatbot Chocolate in Colombia, to talk about virtual assistants and voicebots in Colombia.",voicebot,https://planetachatbot.com/voicebots-colombia-google-assistant-fd146832ae20,"The technological development that we have witnessed over the last few years has caused us to be accustomed to devices such as computers, smartphones, GPS's or others being part of our daily lives. In all of them, the weight of the written word is predominant leading us to deal with keyboards and touch screens since the beginning. However, in this unprecedented technological evolution, the weight of the voice gains ground as the years go by. Now, voicebots arrive in Colombia with Google Assistant.

Therefore, the true consequence of this union between technology and voice are the Smart Speakers. With its emergence in the market, not only the advances in Natural Language Processing are evidenced , but it also shows how the voice returns to take the leading role becoming the interface of the future. Analyzing the possibilities of these new environments is important but doing so without taking into account linguistic or cultural differences is a mistake. Therefore, from Planet Chatbot we have interviewed Juan Nates, managing partner of Chatbot Chocolate in Colombia, who through this interview details how the market for voice interfaces or voicebots is projected in the country.
1. Why has the voice managed to capture the attention of large companies?
Over the past few years we have witnessed a technological boom like no other. In a matter of a decade we have become accustomed to living by the hand of our mobile phones and not having an Internet connection seems catastrophic. This “visible” technological development has paralleled a hidden, but constant and gradual improvement in the processes of recognition of human natural language, giving the opportunity to generate new functionalities such as voice search on Google or voice to text transcription available on WhatsApp that we enjoy every day. However, the greatest result of these improvements in natural language processing has been the birth of virtual assistants such as Siri, Google Assistant or Alexa. Some of these assistants have been on our mobile devices for years, others like Google Assistant,

Juan Nates, managing partner of Chatbot Chocolate.
Be that as it may, more and more companies are betting on starting work in these environments, trusting that conversational interfaces via voice are the new front to exploit in order to communicate with their customers.
2. What is a voicebot ?
For all those who are not familiar with this term that will be increasingly common, a voicebot is a computer system capable of having a conversation with a human being using natural language via voice. These types of bots are normally implemented in environments such as Alexa (receiving the name of Skill) or Google Assistant (in this case they are called Actions), although they can also be implemented in websites or proprietary applications.
3. What opportunities does this new environment offer?
As I commented, the sudden emergence of Smart Speakers and virtual assistants offers companies the opportunity to reach their customers through voice, an interface to which we are fully accustomed. Solving doubts and questions, buying and selling products or executing commands such as turning on the lights are some of the actions that can be carried out with the use of these devices. If we give concrete examples, we talk about buying a pizza or booking a taxi simply through our voice. Given this, the interest of companies to be in these environments is obvious and undeniable. Although we have to be realistic since there are still many challenges to reach this ideal scenario, especially here in Colombia.

4. Indeed, there are many challenges to overcome to reach that scenario you present, could you tell us about it?
The main barrier to overcome is the acquisition of the device itself. Although the data is showing that it is increasingly common to see a Smart speaker in the living rooms of homes in the United States and Europe, so much so that according to the latest Gartner data 50% of American households have at least one device In the home.
Overcoming that barrier, there is no doubt that the learning process of using these devices is substantially less, since it is as simple as executing a command through the voice. However, there are other challenges such as the one derived from the joint use of these devices (that is, by several members of the same family) or security issues regarding the delivery of personal data (the balance of your bank account). Real situations that will be solved as this technology is positioned in the market and becomes a commodity for both companies and end users.
5. Chatbot Chocolate is a company specialized in the development of voice and chatbots, what have you learned after designing skills and actions for third parties?
As you know the official launch of Home Speakers in Colombia has not yet occurred. However, from Chatbot Chocolate we are already prepared for the change that is coming bringing to the country all the experience that we have acquired during the last months in Spain designing different pilots and use cases for third parties. In this line, last April we launched Elecciones.chat, a voicebot and chatbot with which citizens could consult the electoral program of the five main parties that attended the country's elections. A concept that we have worked here in Bogotá with the elections for mayor of the city. An initiative that is present in WhatsApp and Google Assistant and with which we highlighted the interest and settlement of voice interfaces in the Colombian market.

Thanks to the development of this and other use cases we have been able to test the real interest in this kind of booming environments and, above all, learn the great differences between the design and development of text bots and voice bots. A constant learning that allows us to be leaders in the market and be already working on different pilots here in Colombia.
6. Why is it important to start working in these new environments?
Ten years ago mobile apps became a trend, many companies opted not only to launch their own app but also that multiple companies were born with the aim of developing such interfaces for third parties. However, the market is currently saturated with unused applications, demonstrating that the winners in this market are the messaging apps that consolidate their hegemony every day.
Given this, it is time to try other environments, which are born to offer a complete service and user experience, without forcing the user to have the app of the company in question taking up space on your Smartphone. There is no doubt that conversational interfaces via chat and voice have arrived to stay and companies have a duty to adapt to the new situation that arises before it is too late. In this way, they can begin to gather information about how their real users use this type of environment, what kind of functionalities can be the most interesting and what is the best way to monetize the conversational channel taking into account the service they offer."
56,"The Chatbot Landscape, 2017 Edition
To help decision makers and users wade around the vast landscape of bots, this landscape gives a high-level overview of providers and tools.",voicebot,https://medium.com/keyreply/the-chatbot-landscape-2017-edition-ff2e3d2a0bdb#e083,"Why this landscape, now?
Since we started building bots more than 2 years ago, the landscape has seen massive interest and change. This makes it hard for companies and customers to figure out what’s really happening and what they should do if they really want to build a chatbot for their business.
Through this exercise, we deeply explored various bot platforms, bot use cases, and bot frameworks — and we’ve arrived at some interesting observations and insights that may be useful to you (hopefully 😄).
Obviously, there’s no way to squeeze everyone into the landscape, hence we selected those which fulfill these objectives:
Give readers an overview of the industry, such as the industry’s structure, notable examples, dominant providers, and tools widely used to develop chatbots.
Help decision makers understand and choose the most appropriate chatbot development strategies by crossing business needs and capabilities in the market.
The basics
The global market for chatbots reached US$88.3 million in 2016. The market will grow to 36% CAGR, to more than $1 billion, through 2023[1]. Other than this growth rate, the current state and structure of the industry is clearly useful to know better — it’s gotten pretty crowded since we started building bots more than 2 years ago!
Ready? Get ready for a long post 😹
Methodology
We began the mapping process by conducting secondary research, collating data from hundreds of sources, including published studies, companies’ websites, online articles, and personal interviews. In order to give a fair representation of dominant players in the landscape, the study spanned diverse geographical regions and 6 industry verticals, namely:
Commerce
Fashion & Beauty
Travel & Hospitality
Education
News & Entertainment
Finance & Insurance
Additionally, we validated the information presented on the landscape by seeking the opinion of established industry players.
Classification
To put everything into a coherent structure, we define the parameters and the terms as such:
Horizontal axis: The “marketing” function refers to a bot’s ability to drive exposure, reach, and interaction with the brand or product for potential and current customers. The “support” function refers to a bot’s ability to assist current customers with problems, and to resolve those problems for them.
Vertical axis: “Managed” refers to companies outsourcing the development of bots to external vendors, whereas “self-serve” refers to them building their bots in-house or with an 0ff-the-shelf tool.
From the inside out, the concentric circles represent:
Platforms: The messaging platforms that enable the existence bots through robust send-and-receive APIs, frameworks and ecosystems.
Brands: Companies which have launched and experimented with bots, in that particular quadrant (for example, Managed x Support).
Providers: Companies who have the capabilities to deliver exceptional work in that quadrant.
Tools: The supporting tools used by providers, brands, or developers of bots in delivering bot experiences.

Details & Explanation
Here are some of our observations about each of the concentric circles: platforms, brands, providers and tools.
Platforms
In this study, text is the main interaction mode we explore (we might explore voice bots in another study).
As Facebook Messenger is one of the leading chat platforms (with over 1 billion daily active users worldwide), with a strong push internally for Messenger bots, it’s understandable that companies and developers alike have been heavily investing in Facebook Messenger bots.
Clearly, everyone is waiting with baited breath for WhatsApp to open up a bot platform, but it’s still conjecture at this stage. SMS remains a baseline option for communication, and companies continue to use it to send automated reminders and information. As more messaging apps gain a foothold among their intended audience, such as Line and Telegram, their bot platforms will become more attractive for companies to invest in.
Another point to note is that platform adoption and use vary greatly among geographic regions, and certain platforms may work better than others. WeChat, for example, has a naturally huge user base, but may or may not work for your intended target audience.
By platforms, the type of bot content and interaction paradigm also differs. Line and Kik bots tend to be more brand engagement focused, and are more likely to be “loudhailer type” bots (i.e. Announcements and promotions mostly) than SMS or Messenger bots, which tend to be more varied across support and brand engagement.
Brands
Brands are companies who have launched their own bots, split by bot type in specific quadrants as defined above. Here we highlight some interesting bots and their functions, and their respective providers, if applicable.
Marketing x Managed
Universal Studio promoted their horror film ‘Unfriended’ with a Facebook Messenger bot speaking in the character of Laura Barnes[2], which was developed by Imperson — a conversational AI startup from Disney’s 2015 accelerator class.
Maroon 5 Bot[3] on Facebook Messenger, developed by Octane AI, improves the personal touch and interactions between the band and their fans.
The Gov.sg bot[4] on Facebook Messenger, built with KeyReply, helps the government of Singapore provide timely and accurate news and information to their citizens, and broadcast emergency news in the event they are needed.
Support x Managed
Citibank India’s virtual assistant, developed by Creative Virtual, is located in the Customer Service Center of their website and is designed to provide information to customer queries about Citibank products and services[5].
The Bosch service assistant, developed by Artificial Solutions, makes it easy for customers to troubleshoot an issue with an appliance or arrange a service call via its website[6].
NinjaBot [7](for Ninja Van, a fast-growing logistics company in Asia) developed with KeyReply, helps customers track and update their parcels before they reach them, deflecting queries to the support team about parcel status.
Marketing x Self-serve
Cheapflights, Kayak, Expedia, and more have launched bots of their own to give recommendations on travel products, help customers book flights or hotels, and send booking confirmations on Messenger[8].
Support x Self-serve
Marriott International launched Mobile Requests service (via its App) that includes an ‘Ask Anything’ concierge service and ‘Anything Else’ function that allows guests to chat directly with hotel staff, 72 hours before their stay[9].
Providers
Providers are companies who have the capabilities to deliver exceptional work in that specific quadrant.
Marketing x Managed
Conversable AI is a software-as-a-service (SaaS) platform for messaging and voice experiences across multiple platforms, including Facebook Messenger, Twitter, SMS, Amazon Echo, Google Home, and many others[10]. Some of the notable use cases are the Marvel Interative Story Bot, Pizza Hut Ecommerce Bot, and CES Twitter Guide Event Bot.
Assist started out as a local services chat assistant and developed into a chat provider that deploys bots on Facebook Messenger, Twitter DMs, Kik, iMessage and Telegram, among others [11]. They have served Fandango, 1800Flowers, Hyatt, and more.
Msg.ai develops chatbots for multiple channels and provides a dashboard to centrally manage the experiences[12]. They have deployed bots for Sony, CLEAR, and Signal, among others.
Support x Managed
KeyReply is one of the top AI bot providers in Asia for enterprises and governments. KeyReply’s AI capabilities aim to completely replace humans for some specific support tasks. KeyReply’s customers include the government of Singapore, Fortune 500 companies and Asian brands across multiple verticals[13].
Servicefriend provides hybrid bot technology for consistent messaging experiences at scale. They offer an “Interactive Text Response” (the text equivalent of IVRs, interactive voice responders over the phone) for companies such as Globe Telecom[14].
Kasisto enables banks to offer services through an automated “Virtual Specialist” so that consumers can use voice or text to assess financial information and perform transactions on their mobile banking application. It requires no coding by banking implementers and is designed for banks to private label their own brand[15].
Marketing x Self-serve
Chatfuel lets anyone build bots for Facebook Messenger. Their users include NFL and NBA teams, publishers like TechCrunch and Forbes, and millions of others[16]. Chatfuel is great for simple and focused bots, but may not work for complex chatbots requiring lots of customization.
Octane AI, focuses on allowing celebrity content creators to create tree-like stories that others can engage with on Facebook Messenger. Celebrities such as 50 Cents, Maroon 5, Lindsay Lohan, Jason Derulo, have used Octane AI to build their chatbots[17].
Gupshup is a bot-building platform that offers businesses pre-made bot templates. It offers both a “no coding required” version and an IDE for developers to build bots[18].
Support x Self-serve
Flow XO’s focus is to create an easy chatbot building experience[19]. Flow XO enables the deployment of bots to different platforms, such as web, Messenger and Slack.
Morph.ai is comes with built-in training and understanding modules to give a more accurate understanding of the business’s target audience. However, the platform only allows limited targeting for broadcasting[20].
Motion AI offers turn-key templates for many chatbot use cases including customer service, meeting scheduling and surveys[21]. However, it only operates on a multiple choice or single bot statement basis. It services global consumer brands including Kia, Fiverr, Sony, and Wix.
Some notes about the brands and providers above
Marketing bots tend to be largely campaign-driven, where it can be used effectively for driving engagement in short bursts. Longer-term marketing or sales efforts in the market are still mostly experimental, as it may be hard to define metrics for success without a strong indicator from the proof-of-concepts.
Support bots, however, have been around for much longer, and is a medium that customers are already used to. Metrics for deflection and customer satisfaction may also be more well-defined, hence there will be a “flight to quality” in this space to those providers who genuinely can deliver on their promises to answer customers well, not piss them off, and elevate the support experience.
Tools
These are supporting tools used by the providers and brands, or by developers of bots.
Marketing x Managed
Nuance has been developing highest functioning speech software in the world, perfecting the ability for machines to recognize and emulate the human voice[22].
Chatsuite offers app marketing experiences with chat [23].
Bot Metrics, a San Francisco-based company that specializes in metrics and analysis for chat bots has landed funding to help developers and early bot enthusiasts get a better understanding of their services and users[24].
Support x Managed
So obviously this is a quadrant that is pretty hard to characterize, because many companies won’t just reveal their tech stack where you can find them. Just based on our own exploration, we find that these are some of the more useful tools for machine/deep learning around the large datasets that come from brands, and there are other natural language processing tools and papers that contribute greatly to the endeavor.
Theano is the grand-daddy of deep-learning frameworks that many academic researchers in the field of deep learning rely on. Numerous open-source deep libraries have been built on top of Theano, including Keras, Lasagne and Blocks which attempts to provide a more intuitive interface as compared to Theano[25].
TensorFlow is created by Google with an aim to replace Theano. On top of deep learning, TensorFLow has tools to support reinforcement learning and other algorithm. One limitation is that TensorFlow runs slower than other frameworks like CNTK and MxNet[26].
Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is a well-known and widely-used machine-vision library, which is suitable for classifying and processing image. The use of Caffe is not applicable to other deep learning applications such as text, sound or time series data[27].
Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first. Torch comes with a large ecosystem of community-driven package in machine learning, computer vision, signal processing, parallel processing, image, video, audio and networking among others, and builds on top of the Lua community[28].
Marketing x Self-serve
Wit.ai enables developers to easily create text or voice based bots that humans can chat with on their preferred messaging platform. The platform offers support to 100,000+ developers, non-coders in Wit community to build applications and devices with its natural language processing capability[29]. Wit.ai is well-suited to support complex functionality but mainly works well for small-sized bots.
Api.ai, owned by Google, is a visual tool to build decision trees for a chatbot, popular for consumer-facing applications, but clients report limited functionality[30].
ChatScript is the next generation chatbot engine which manage dialog or NL tools. ChatScript is a rule-based engine, where rules are created by humans writers in program scripts through a process called dialog flow scripting[31].
Support x Self-serve
Microsoft Cognitive Services’ initial release with just four services (face recognition, speech recognition, visual content recognition and language understanding) has now been extended to over 20 APIs. Furthermore, in 2016, Microsoft launched the Bot Framework for building a conversational user interface which links to Cognitive Services[32].
IBM Watson is mostly known for its premium solutions that require you to build a chatbot using its services. It also has the Conversational Service, which is lower in price and offers a visual tool to build decision trees that use Watson to map intent. IBM is pretty open on language support if that is a concern[33].
Montreal-based Smooch empowers software developers to create conversational experiences across any messaging channel. Smooch.ai is a simple-to-use solution, with Slack support and offers a free solution when the user base is small[34].
Using this landscape
What does your business need: Marketing or support?
Look at the nature of the product/service. What is included in your product/service package? Does your business provide only presale-service or both pre-sales and post-sales support?
If your products require constant interaction with consumers to drive sales (e.g. companies selling clothes and accessories such as H&M or Victoria’s Secret), you should develop a marketing chatbot.
If your products require heavy customer support such as product warranties, assistance to resolve technical difficulties (e.g. electronics companies such as Apple), your company should focus on developing a customer support bot.
How do you do it: Buy or build?
Does your company have the resources and capabilities to build software in-house? If your existing resources (i.e. technical skills, software infrastructure) can be easily transferable to the creation of a bot, your business should consider developing a chatbot in-house, either for greater control or cost. For example, Skyscanner has a strong existing tech team and robust algorithms; hence, they can apply that to their flight search bot [35].
Does your company have the ambition, human resources or budget to cope with complex NLP and data science issues that might arise? If no, then you might be better off outsourcing the development of chatbots, reducing the risk of investment in complex NLP, which is not your core business.
Should you do it: Strategic or faddish?
If your value proposition involves providing convenient and fast service to your customers, then developing chatbots in-house may enhance your value proposition and strengthen your company’s competitive advantage in the long run.
For example, a bank’s value proposition lies in its ability to provide convenient and 24/7 support to its customers (such as transactions, transfers, bank loans, debit/credit card applications, etc). Capital One has built an automated bot that can communicate with the bank’s customers via text message (and Alexa) to give them information on their accounts and help them make credit-card payments whenever they need[36].
What do you want to invest: All-in or experimental?
Do you want or need to have a full control of conversations with customer or customer data, and already have a good idea what a bot should achieve? If yes, then find an enterprise-grade provider, or build it in-house with a specific team dedicated to the project for at least 3 months.
If you’re simply experimenting with bots, then it’s fine if you sandbox some of your flows or data and build a small use case on the cloud, working with a provider for a proof-of-concept, or hacking together a small bot internally.
What’s next
We’ll want to continue providing updates to this map as we go along. If you think there’s a brand, provider or tool that should be added, please let us know! If you’d like to argue for/against any classification, tell us as well: There are many ways to skin a cat 😸 (especially one as opaque and complex as an entire bot landscape), and we can debate on those ways.
Thanks for reading and don’t forget to share if you found this useful! 💌"
57,"DialogFlow: A Simple Way to Build your Voicebots & Chatbots
Understanding the basics of DialogFlow",voicebot,https://towardsdatascience.com/dialogflow-a-simple-way-to-build-your-voicebots-chatbots-27949a1ac443,"Nowadays, businesses, whether it is B2B or B2C, are heavily relying on chatbots in order to automate their processes and reducing human workloads. There are various NLP chatbot platforms used by the chatbot development companies to build a chatbot, and one of the best platforms among them is DialogFlow. The Platform was previously called as API.AI. It was acquired by Google in 2016 and renamed as DialogFlow.
DialogFlow is a Google-owned natural language processing platform that can be used to build conversational applications like chatbots and voice bots. It is a platform that provides a use-cHeease specific, engaging voice and text-based conversations, powered by AI. The complexities of human conversations are still an art that machines lack but a domain-specific bot is the closest thing we can build to overcome these complexities. It can be integrated with multiple platforms, including Web, Facebook, Slack, Twitter, and Skype.

DialogFlow Terminology:
Understanding the basics of DialogFlow
1. Agent:
DialogFlow Agent handles conversational flow with end-user. To get started with DialogFlow, first, we need to create an Agent using the Dialogflow console.
It is a top-level container for intents, entities, integrations, and knowledge. Agent translates end-user text or audio during a conversation to structured data that apps and services can understand.
2. Intents:
Intents are used to understand and process the user intentions and contexts to direct the conversation flow with end-users.
An intent contains some training phrases and their corresponding responses.
An intent gets invoked when it’s training phrases get matched with user inputs. And give output to end user’s as defined in its responses. If multiple responses are present for input, then responses will be shown to the user randomly.
If we have multiple intents with the same training phrases, then either the higher priority intent will match, or the intent with active input contexts will match.
Intents priority:
Intents priority can be set if there are chances that user input can match with multiple intents. The intent with higher priority will get invoked.

Fallback Intents:
Fallback intents are invoked when user input did not match with any intent.
When we create, an Agent two default intents get added in the Agent.
Welcome intent and default fallback intent.
Contexts are used to understand natural language user contexts. That is in which context the user wants information.
For example:
A person can give input “orange is my favourite.”
Now this orange can be matched with a colour intent or with a fruit intent. So which intent should be matched in this case.
To solve this problem, contexts are used in DialogFlow.
Contexts lifeSpan:
Contexts have some lifespan for which they remain active. The default lifespan is 5 requests, but it can be changed.
It means that the context will live longer for the next five matched intents.
Contexts are of two types:
a) Input Contexts:
An intent having some input contexts can be matched, only if its all input contexts are active.
For example:
We have two intents with the Same training phrase “Orange is my Favourite.”
But both intents have different input contexts. One contains colour as input Context, whereas others contain fruit as input context.
The intent for which input context is active will match with user input.

b) Output Contexts:
An intent having some output contexts will make its all output contexts active if it matches with user input.
For example:
A colour intent match with user input “Do you know about colours.”
which responds to the user by saying, “what is your favourite colour.”An Output context “colour” can be set active by the intent.
When the user says, “Orange is my favourite,” the intent having input context “colour” will match the user input.

4. Entity:
Entities are used to extract some useful information and parameters from end-user input. Entities can be either system-defined or can be developer-defined.
DialogFlow provides many predefined entities like date, time, colour, temperature known as system entities to handle most popular common concepts.
However, custom entities can also be defined by developers based on their requirements.
The extracted parameters from user inputs can be passed between intents to direct a conversational flow.
5. Responses:
Agents can give two types of responses to end-users.
a) Default responses.
b) Rich responses.
a) Default Responses:
Default responses are also known as Platform Unspecified responses. These responses are simple text responses shown to end-users. These can be used with any platforms including web, Facebook, slack.

b) Rich Responses:
Rich responses are also known as Platform specified responses. Rich responses are used to show buttons, cards, quick replies, links to users via Facebook, slack platforms.
However, to use rich responses with web applications, the chatbot needs to be customized.
Rich responses can be configured either with DialogFlow console or can be sent within webhook responses.

6. Webhook:
A Webhook can be integrated to give responses from our Application. Webhook integration is simple and can be implemented using a fulfilment option. A URL can be configured there, and webhook call needs to be set active for the intent for which you want to call the webhook.
Conclusion:
DialogFlow is a very simple platform to build quick chatbots, voice bots with minimum coding efforts. It can process natural language errors easily and can be integrated with multiple platforms. It is basically a tool that allows in building chatbots that clearly understand the human conversation and reply to them with an appropriate answer after parsing the conversation with relevant parameters. For more understanding, you can refer to this link."
58,Smoother chatbot conversations with multiple intents,voicebot,https://medium.com/@julian.harris/smoother-chatbot-conversations-with-multiple-intents-f270d5b7f72f,"I had what you’d call a status quo conversation with a bank voicebot today. It went something like this:
Bot: You said your date of birth is 1 Jan 1919. Is that correct?
Me: No
Bot: Please tell me your date of birth:
Me: <proper date that coincidentally doesn’t make me nearly 100 years old>
To be really fair: having free-form voice like this just a few years ago was nothing short of astonishing, and still is in many parts of the world.
But we’re in 2018. We can do better than this awkward to-ing and fro-ing. Surely?
So, yes. the next step is being able to handle a response and another question immediately after that. In conversational computing terms is called “multiple intents”. Once you get a taste of a voice system that can handle more than one request at once (turns out quite common in everyday life), “single intent” systems quickly become infuriatingly inflexible! What actually happened in my bank conversation was more like this:
Bot: Your date of birth is 1 Jan 1919 is that correct?
Me: No, it’s 1 Jan 1970
Bot: please say yes or no
Me: Gngngnggnngng
At CogX18 I held an Advanced Conversation Design session and with Hans van Dam from Robocopy.io, John Taylor from Action.ai, Nikola Mrkšić from PolyAI, Rachael Rekart from Autodesk, Alexander Weidauer from Rasa along with CognitionX’s own Bogdan Vrusias, Martin Lambert from eXvisory and Duncan Anderson, and the insights from that session will be shared in CognitionX’s Chatbot Research Subscription (sign up for free trial here)
John’s Action.AI demo was really slick
Relevant to this particular theme John’s live Action.AI taxi order demo with Google Assistant really stood out in supporting the ability to both change the subject when responding and give multiple requests. It was both jaw-dropping in how smooth and natural it made the conversation, and at the same time, astonishing that we somehow accept less than this today.
Rasa.ai offers this now too
Rasa is an open source chatbot conversation development product and is very interesting for lots of reasons, and the most relevant for this piece is that as of 0.12 Rasa NLU (the understanding component) released a few weeks ago, it also support multiple intents.
What else supports it?
It’s my view that it’s inevitable that all conversation development systems will support this. Google Assistant US supports multiple commands but I’m still trying to understand whether it supports multiple commands in responses. I’ll update this when I get the low-down."
59,Google Pixel Buds teardown and thoughts on humanizing computer interaction,virtual personal assistant,https://medium.com/@justlv/google-pixel-buds-teardown-396183cbbc18,"Google recently launched their wireless earbuds as a bold new step to enable Google’s “virtual personal assistant” through a wearable product.

I’ve talked previously about the promise of voice interfaces, and I believe in creating a more seamless human-computer interface to humanize technology, so I applaud this effort.

Her (2013)
But to deliver the best AI-focused services even a software giant would need to break into the real world with a device one would feel comfortable wearing. But putting technology on one’s body is tough to get right — so it’s interesting to see how two leading technology companies approach this.


Jewelry vs clothing. Apple AirPods (left) Google Pixel Buds (right)
Generally wearables that try imbue the essence of something we’d actually wear end up facing a choice of looking like either jewelry or clothing. The Apple AirPods have been commented on as feeling like earrings (with the hanging microphone, their shiny metal accent, and needing to be delicately removed and placed into their case after use), and hence may fall into the jewelry category. If so, the Pixel Buds have taken a decidedly more “clothing” approach, with a soft fabric case, round button-like earpieces, and a string with a thoughtfully placed bead, which can be pulled on either end to snugly adjust the fit like a drawstring.
Meeting both the design and voice assistant requirements would require some thoughtful hardware engineering to get right, so naturally I got hold of a pair to teardown over the holiday, and to see how Google scored on these fronts…
Unboxing
Nothing too outlandish with the packaging to write about here, just some pulp trays neatly nesting a fabric clamshell case, holding the earbuds.



Be safe. Did they somehow know I was about to break out the heat gun, pliers and scalpels? 👨‍🏭
The earbuds are meant to rest outside the ear canal, and so there is no need for a tray of interchangeable silicone tips which one always ends up losing. But to go this route relies on the earbuds fitting and staying in your ear. The adjustable string loop was surprisingly effective at this, and kept the earbuds in securely during a run, with the orange bead cleverly acted as a limit to how far the cord could be pulled. Overall the cord seemed like a really neat design feature, adding to the soft material feel of the product, but had two suprising drawbacks:
It looked so much like regular string, some users reportedly tried to cut the string and so Google had to explicitly warn people not to, and probably why they refer to it as a “cord” and not a string. In the teardown below you’ll see why cutting this is a bad idea…
They don’t quite hang around your neck. The Pixel Buds are too end-heavy, with the supple string not giving much resistance, resulting in them frequently slipping off your neck on either side. The Beats X got this right, with a semi-rigid loop to fit around your neck, and in-ears that securely clip together magnetically when hanging around your neck. Product idea: an after-market leather/fabric collar that can go across the back of your neck and the cord can clip through to prevent them sliding off…
While most are not comfortable with PDA (public digital assistant, hat-tip to @karaswisher for the term), you can hear yourself and your surroundings well while wearing these, making it feel much more natural to talk while wearing. But not having a Pixel phone, there was not much testing of the assistant I could do beyond controlling music and Siri.
Being eager to take a look inside, I wasn’t going to wait till I could test these with a Pixel phone. So, onto the teardown…
The Teardown
One could assume the earbuds were held together with some sturdy glue or adhesive, which would reliably seal the product and provide some level of waterproofing, so to open up would require highly localized heat. My weapon of choice? A hot air rework station. This is essentially similiar to a small hairdryer (but should never be used as one!) and is primarily for applying controlled heat onto specific areas of PCB’s, allowing one to remove or replace electronic components by melting the metal solder attaching them. They typically come with fairly accurate temperature control, so you can scale this way down to find a temperature that won’t melt the plastic, yet will soften any stubborn adhesive.
I decided to start with the left earbud, predicting that without microphones or a touch interface, this would be the lower tech side and so would have less to damage with my first attempt…

After heating around the joint, the plastic cover could be carefully pried off with a scalpel and wedge
Prying open the cover, one can see what looks like the battery. Being glued in itself, this will need some more carefully applied heat and surgery to pry away from the plastic to see what lies beneath.


Careful here: Too much heat + Lithium battery = 🔥🤕
Pulling away what we can now identify as a 120mAh Li-ion battery, and unplugging the flex cable gives us a better view of the PCB with some basic components, and some wiring.


The battery is hogging most of the space in this earbud
From this view we can assume that this left bud is in fact just the battery, leaving little space for any other advanced electronics. The 6 colored wires likely connect through the string to the other bud, and so would carry power and speaker signals. These wires are braided, i.e. made up of many strands twisted together, which provides suppleness allowing the string to be easily and repeatedly bent without damage.
Why are these just seemingly manually soldered directly to the PCB — surely Google would be more professional and use a header? There are a few reasons for this. Audio connections require good contact to limit noise — a blob of molten metal provides a much better contact than metal just pressing against another metal surface. There is also the risk of disconnecting or loosening after assembly — and if you have a good test procedure, you’d want to reliably attach the wires and never look back, yielding a removable header pointless. A header adds cost and takes up space. Also, soldering directly to the board enables you to easily cover the connections with a conformal coating or encapsulation, as seen, which provides some extra strength, insulation and humidity protection.
The flex cable however, is essentially a PCB of copper tracks on a flexible substrate. This is much cheaper for short distances where there isn’t going to be repetitive motion or twisting, and unlike wires, can be reliably attached to a PCB on either end by slipping into a low-cost and low-profile header.
Detaching the PCB from the battery reveals the silk-screened reverse and some test points. SIlkscreen, usually used for printing designators, may have been used to offset the test points from the surface, preventing accidental contact. Nothing much to see here.

Continuing the teardown, you can see the cables appear to be sealed in some type of encapsulant. This provides some strain relief, so that pulling on the cord will not result in any force pulling the wires off their connection to the PCB, and no doubt waterproofing.

I tried to apply some heat to this to attempt to dig through, but the plastic appeared to start softening before the encapsulating material, meaning it must be some form of epoxy and not glue. We’ll have to take a look at where these cables go from the other side.
After much heating and scraping, the speaker was revealed rather anticlimactically. Nothing too interesting to see here, other than confirming that the flex cable powered the speaker and connected to the two charging contacts.


There’s no going back, and the earbud is starting to resemble Anakin Skywalker post-Episode III
The Right Earbud
Lets move to the right earbud and see what we can find. Opening with the similar heating and prying approach, we can see that its already looking a little more interesting…


A “printed” antenna, capacitive touch flex PCB (left) and what looks like the main PCB (right)
On the interior of the white plastic covering, you can see a silvery metallic ring. This is the antenna, placed in this ideal location to get the best possible signal quality and resultingly Bluetooth range. It is connected to the main PCB by the gold-colored pogo-pin.
But what’s fascinating about the antenna is how it is placed there, using a technique called Laser Direct Structuring (LDS), which allows a conductive antenna to be effectively “printed” directly onto plastic. This is done by adding a small amount of non-conductive metallic compound to the white plastic. A laser is then used to draw out the desired pattern, which causes the metallic compound in the plastic to form a surface that allows other metals to adhere to it by creating nuclei (similiar to how fine particles in the air can cause humidity to be drawn out of the air, seeding rain). By then placing the plastic in a metal bath solution, one can grow precise and even layers of nickel, copper or gold onto the plastic only on the areas that were lasered.
I previously saw this in another teardown of the Doppler Labs Here One:

Comparing antennas on the Doppler Labs Here One (left) and Google Pixel Buds (right)
Let’s now take a chance to look at the capacitive touch sensing. One can see that the Here One also has capacitive touch, but has only two contacts, meaning it can only pick up a single tap, whereas the Pixel buds have 5 connections running through the flex PCB strip, so one can assume it can sense a finger’s presence in four different quadrants. (the fifth signal is for ground)
Peeling the flex PCB off its adhesive, you can see that it does indeed have four different quadrants. This is required to accurately detect swiping, i.e. a finger moving across the surface in one direction, for up/down volume control.

Capacitive touch interface
Now returning to the rest of the earbud, let’s reveal the main PCB. This is done by removing a flex header, a QR sticker from the main Bluetooth SoC, and cutting off some little plastic laser-staked rivets to remove the main PCB. This seems to be where the brains of the operation lie — we’ll return to this later…


Now we can see a rigid-flex PCB and what appears to be microphones on either side of the earbud. Removing some more plastic fixtures, we can unravel this rigid flex, confirming that these are two bottom-port microphones with some adhesive foam allowing them to create a snug fit against the main body.


This is a really neatly layered design, making use of flex PCB and some clever origami to position microphones and capacitive touch sensing around the main PCB. As in the left earbud, there are flex and wire cables travelling off to the speaker and charging contacts. We’ll leave this intact for now, and take a closer look at that main PCB.
Identifying the chips
Going back to the top of the PCB, we can see the main brains and talk of this product is the all too popular CSR8675, a single-chip solution for audio processing and Bluetooth. We can also see another chip below the gold pogo-pin that is probably significant but only bears what looks like a QR code. We will need to do some “zoom and enhance” detective work later on that one…

CSR (acquired by Qualcomm a couple of years back), were pretty big in the audio and connectivity space, and I’ve seen this specific chip in other “hearables” such as the Here One, LifeBeam Vi, Nuheara IQBuds, and Bragi Dash to name a few...




CSR dominating the hearables market. From top left: IQBuds, Lifebeam Vi, Here One, Bragi Dash Pro. Yes — I did a teardown of all of these…
On the reverse side of the PCB, we can see a slew of other chips, namely:

(CY8C414…) Cypress PSoC 4 S-Series. This most notably contains an ARM Cortex-M0, capacitive touch sensing interfacing, and reconfigurable analog and digital blocks.
(+8971G …) Maxim Integrated MAX8971EWP+T. This is a compact, and high-efficiency switch-mode charger for the Li-ion cell we saw earlier. The need for high efficiency and flexible input voltage is because the buds are charging from another larger battery in the Pixel Buds case. It’s likely that it is on this board as it interfaces over I2C with the other processors for managing charge parameters, and providing the battery charging status etc.
(5AC03S …) Texas Instruments BQ27421 Fuel Gauge. This uses an integrated sense resistor and algorithms for determining information such as remaining battery capacity (mAh), state-of-charge (%), and battery voltage (mV). A nice and easy and well-contained approach.
(TPS734 …) Texas Instruments TPS62743. A high efficiency buck converter, for converting battery voltage to a range required for the IC’s, supporting up to 300mA output, with an ultralow 360nA quiescent (“off”) current
(3324T…) — RClamp3324T Low Voltage RailClamp and 4-Line ESD protection. This protects the circuitry from any dangerous voltages that could be applied on the charging contacts through static discharge or other means
26 MHz crystal — This is used by the CSR8675 to create a clock signal for its high speed DSP operations
(71940Y U64M )— This part was a mystery, not being able to be easily identified from the marking alone. Let’s try figure out what it is…
What we do know is the flash on the CSR8675 is quite small, and it’s likely that there is a lot of custom code that a software company like Google would want to run, especially to get such good noise cancellation and voice detection from mic’s on only one ear. Therefore they would need some external flash. The “64M” in the marking on this chip is a clue, that this is most likely NOR flash — differing from NAND flash as it allows faster reading, better random access, and is typicall used for code storage. NAND on the other hand (ha!), is mainly used for file storage, and boasts faster writing, and higher density (64MB would be laughable for NAND flash unless you were Marty McFly).
Desoldering the chip, we can get a look at the foot print (i.e. the arrangement of contacts soldered to the PCB). We can see some more agreeable evidence here, as it is fairly common for flash to have 8 pins. (The outer 4 pins are not connected to anything and mostly function as aligning and securing the chip onto the PCB).

The reverse of the unknown chip. Google, I’m guessing this voided my warranty?
If I would make a guess, I would say it is part compatible to the Micron Serial NOR Flash N25Q064. Here is the datasheet exercept showing the footprint of the chip-scale package version of this flash chip. Looks pretty similiar:

We’ll end the detective work there and call this 64M NOR Flash. Now onto mystery chip number 2, with the QR code we saw earlier.
By process of elimination, now that we’ve identified all chips, this would have to be a stereo headphone amplifier, to power the two earbud speakers. But lets see if the QR code reveals some clues. A $10 iPhone microscope lens and Photoshop presents us the following:


“Zoom and enhance”. The writers of the CSI TV series would be proud… 🕵🏻‍♀️🔬
One could do some further cleaning, but this QR code is readable as is, probably sporting a high error correction level, and reads: 787260NW03EA0HE. (Feel free to try it yourself!)
Mean anything to anyone? Not me. Remember, suppliers are not necessarily trying to be difficult, but are not usually incentivized to make teardowns nice and easy. Likely just an ID relating to a batch number.
So let’s desolder the chip and view the footprint:

The reverse of mystery QR chip. (Desoldered pogo pin also shown)
Doing a parametric search on Digikey/Mouser of 12-pin QFN earphone amplifiers yielded some very similiar looking results of chips that would meet the same requirements, but nothing that looked like an exact match.
Together with the placement of capacitors around the pins, and testing connections to audio lines confirms this is a stereo audio amplifier. Which one, I’m not sure — but there’s not that much to them, and this part was probably chosen due to finding a reliably vetted part and cost optimization of the production line versus innovation-factor.
The microphones
Now let’s take a look at those two microphones we saw earlier. Why two, facing opposite directions? This allows Google to do some better noise cancellation and beam-forming, to focus on only picking up speech from the user. Almost all transmission of audio signals in electronic designs today are digital, to avoid noise — as most modern designs are compact and have wireless radios in close proximity. These microphones have optimally designed sampling built-in, and conversion to a binary means of encoding called pulse-density modulation (PDM). In this scheme there is a clock signal sent to the microphones, meaning that two microphones can share the same data channel, in which one asserts the data line on the rising edge of the master clock, while the second asserts on the falling edge, meaning only one input pin is required on the DSP and these two signals are highly synchronised — necessary for the beam-forming algorithms. Remember how the human ears can locate a sound’s direction by the different time at which the soundwaves reach either ear? Timing is important here…


Pair of MEMs microphones, with bottom facing ports
The markings don’t do much to identify the microphone and manufacturer, but I can tell this is from Knowles, and part of their SiSonic range of MEMS mic’s.
In the string S1925 9602, the “S” indicates it is a production ready version from the SiSonic range, whereas the number is just for batch identification. To figure out what type of mic it is, let’s desolder it. Knowing the dimensions, that it is a bottom port digital mic and from the footprint below, we can narrow it down to likely being the SPH0641LM4H-1. (Interesting note, if they used the ultrasonic version, the SPH0641LU4H-1, they might have been able to do some interesting things with using higher than perceptible frequencies to learn about whether the earbud is in the ear or not, assuming the speaker create such frequencies)

Dropping the mic…
Lets go one step deeper down the rabbit hole, and remove the metal shield to see what’s inside this mic…

Removed metal shield (left), ASIC (black blob in the top right) and transducer over the microphone port (bottom right)
Here we can see the the transducer and the ASIC (circuit customized for a particular use, in this case digitizing the audio signal from the transducer). The faint gold lines are connectors “wire-bonded” to the transducer and ASIC, and essentially the only part of this whole signal chain to carry analog signals to the DSP.
Lets take a moment of silence for the buds and eulogize:

To support assistant: “I was just listening to music, when these spontaneously just fell apart”
The Pixel Buds were cleverly designed and really interesting to take apart. While they used glue, they used significantly less than the AirPods and so were slightly easier to open up — and this could potentially be done while maintaining repairability, although reducing the waterproofing unless you could recreate the seal.
One can also see now how having the connecting string between left and right sides eases the hardware design challenges, such that the battery can be in one ear and the brains in the other. Having one larger battery is more power, space and cost efficient than having one in each. Secondly, there is no need to manage wirelessly syncing between the two buds, again saving power, space and cost. Some products achieve this by having redundant Bluetooth chips, where both will connect to the phone, or only the one with the best signal (depending on which pocket your phone is in) will connect and broadcast to the other. But most use what is called NFMI (Near Field Magnetic Induction), defined as “short range wireless physical layer that communicates by coupling a tight, low-power, non-propagating magnetic field between devices”. This is well suited as the buds stay at a fixed small distance (between your two ears) and the magnetic field is not affected by the large amount of water in your head to the same extent as a Bluetooth signal would be. This however requires a large coil in each earbud, again taking up precious space, and doesn’t include other data, such as if the second earbud needed to have a microphone for improving beamforming and noise cancellation.

NFMI coil found in Here One, for sending audio between ears in completely wireless earbuds
Interaction improvements?
After tearing down the Pixel Buds, let’s take a look at some possible interaction improvements or alternatives. How might one add detection of whether they are in your ear? The Apple AirPods use infrared sensors to detect this, which would be the first idea. Some quick ideas:
Capacitance — can be used to sense the edges of an earbud being pressed against the outer ear canal, and would differentiate well between skin and other surfaces, i.e. as in a bag or on a table. The placement of these capacitive sensors would need some careful mechanical design.
Humidity differential — with humidity sensors becoming cheaper, and more power efficient (possibly becoming cheaper or at least lower power than IR detection), one could tell if a surface of the product was pressed against skin.
Accelerometer — the earbuds are in a pretty consistent orientation when inserted — and even if hanging upside down, you can assume they’d be in the same orientation relative to each other. Once they were determined in the ear by one or more of the other methods, the accelerometer could be used to tell if there is unexpected movement with a low-power and fast response, prompting a re-evaluation of whether they are in or not (similiar to how an altimeter in GPS helps prompt a re-evaluation of altitude).
Ultrasonic — more of a “creative” approach, having two microphones, you could learn a lot about the earbud’s location from measuring the response from small ultrasonic chirps. This would only work for the right ear in the current version.
Furthermore, for gestures, an accelerometer could provide a redundant means for sensing taps, or one could make use of deliberate squeezes of the bud to pause/play/activate assistant, to differentiate from accidental taps.
Lastly, having always-on listening for a wake-word (i.e. “Ok, Google”) so you won’t need to tap may seem power intensive but is becoming more feasible. I’m making a prediction that future versions will have this, based on some of the low-power DSP’s with built-in Deep Neural Net hardware acceleration being developed by chip manufacturers.
With the earbuds now violated beyond repair, let’s finally wrap-up with the case…
The case
No clean way to do this, so with a little bit of hacking and a few cuts, we can pop out the inner plastic from the fabric shell, which is attached using snaps. The fabric shell looks like it is made from a combination of vacuum-forming, injection molding, in-mold textile application, followed by application of a soft-touch finish to give that smooth velvety feel.


Case study underway…
We can see a 2.39Wh Li-ion Polymer battery, the USB-C header, a smaller PCB for mounting the charging contacts, and what must be a small subassembly for magnetically detecting whether the box is open or not. Notice how USB-C has 12 visible pins. There are another 12 pins not seen, (commonly through-hole), for a total of 24, or 12 per insert orientation, (Yes, it finally doesn’t matter which direction you plug in USB-C), compared to 4 pins for previous USB versions.

Unscrewing the main PCB, we can start to identify some more components on the reverse side.

(CY8C414…) Cypress PSoC 4 S-Series. The same ARM Cortex-M0 containing chip seen in the right earbud. Some features are likely not needed, but they would get a better price, and it would be simpler from a firmware development perspective to double up on this chip, assuming they are using the same factory in Shandong for both the buds and case.
Active-semi 2801QL 5V/1.5A Backup Battery Pack Manager. This is a nice chip that, according to the datasheet, “integrates all of the functions that a battery pack needs , including switching charger, boost converter and LED indication.”
Texas Instruments LP55231 — 9-channel LED driver. This is used for controlling the 3 white indication LED’s, and an RGB LED, which shares the location of one of the white LED’s. You can see the foam adhesive with three cutouts, which seals the light against the case and light pipes, allowing it to shine through as clean little dots.
Snap dome button — This is presumably the reset button, you can see that this has a corresponding button built into the plastic case in the picture below.


Left: Reset button in the top left, light-pipes for LED’s in top right. Right: Three white LED’s, with the first sharing it’s light-pipe with an RGB LED
Conclusion — why a voice interface, or technology you can talk to?
The Google Pixel Buds received some mixed reviews. But for being a voice interface and as an enabler of Google’s AI assistant, they have made some big leaps and deserve that recognition in a pretty tough market.
Big ideas in technology typically need a few stepping stones in-between the present and the end goal. Each stepping stone takes advantage of the “adjacent possible” of current technology, while providing some marketable value to sustain progress for future versions. Some stepping stones falter, and sometimes feet get wet (ahem, Google Glass), but all help drive the industry forward and expand the realm of adjacent possible to allow further innovations.
If one end goal is humans being able to interact with computers — that are becoming more ubiquitous and integrated into everything around us — and do so at a higher bandwidth, we stand a chance to not be left behind and benefit more greatly from some of the amazing benefits and capabilities of AI. Whether this actually ends up being the “neural lace” or not, a worthy stepping stone to get there is earbuds that can understand how we communicate, allowing technology to adapt to humans, and not the other way around.
Again, I hope this teardown was helpful — continuing into the New Year I’m hoping to help make technology more accessible, for both people who are interested in building it, and the people who have the most to gain from it. It’s easy to look at some technology coming out of Silicon Valley, or historically NASA and other institutions for example, and fail to see the bigger picture or application in the rest of the world, but the advances and the possibilities these products open up for innovation in other markets is usually invaluable, as long as we help spread it.
Huge thanks and hat tip to the product designers at Google, and I apologize that this is the second product of yours I’ve publicly taken apart, (3rd if you include Nest?) but as there is a scrapheap of products I haven’t written about, it means there is something about these products worth writing home about… 👏🙇🏻‍♂️
And to sign off, something that has pleasantly surprised me is how many non-hardware or even non-technical people enjoyed reading these teardowns. So if you liked this article please clap away by clicking on the clapping hands logo as much as you desire, or share this article wherever you can. It helps an obscurely niche article reach more people! And as always feel free to contact me with any comments, suggestions or questions at j@justinalvey.com or @justlv on Twitter."
60,What Chatbots Can Learn From the Death of Facebook’s Failed AI,virtual personal assistant,https://chatbotslife.com/what-chatbots-can-learn-from-the-death-of-facebooks-failed-ai-acc05ad6d822,"Facebook recently announced it was shutting down M, it’s virtual personal assistant, causing some people to wonder if the decision meant chatbots were not as useful as they first seemed. The technology behind M was not perfect, but it’s important to remember that it could serve as a learning tool to aid decisions made by chatbot developers in the future. Below, let’s explore how M worked and the lessons learned from its less-than-ideal characteristics.
Top 3 Most Popular Bot Design Articles:
1. 10 Tips on Creating an Addictive ChatBot
2. What 10 Billion Messages can Teach us About Making Chatbots
3. Bots & Super Personalization On Steroids
The Technology Must Be Clearly Chatbot-Driven
M worked with the help of machine learning. It detected meaningful fragments of conversations people had with each other in Messenger, then made suggestions of the next actions to take. For example, if a person said, “Want to call me?” M would bring up the voice calling interface within Messenger.
However, some suggestions were not so straightforward — and it wasn’t obvious a chatbot delivered them. If people in a Messenger conversation started talking about getting together, M served up a “Start Plan” suggestion.
If someone said “Does it work for you if we meet on Saturday,” it’s not necessarily clear to a person why the “Start Plan” option appeared in the Messenger window unless he or she had used it before. Some individuals were annoyed by it.
Furthermore, nothing about the suggestions made it evident that a chatbot provided them. The spontaneous ways they appeared could easily cause people to think they’d accidentally tapped a feature in Messenger, or that the application was malfunctioning.
Chatbots That Require Lots of Supplementary Human Interactions Are Costly
Although M was designed to help people do more things inside of Messenger conversations, it also offered even more capabilities to individuals in a test market. Facebook piloted a free service to 10,000 people in the San Francisco area that let them do things like reserve tables at restaurants or make changes to flights.
After Facebook reviewed the omniscient assistant, M reportedly only achieved 30% automation, and its developers realized the abilities it wanted the chatbot to have would be very expensive to implement. Plus, most of the duties M carried out for the San Francisco users needed humans — known as M Trainers — to finish them. So, in many cases, instead of saving money, it required an additional labor force.
It’s arguably more cost-effective to develop chatbots that can handle some of the responsibilities that humans do, but don’t always require real people to step in to add to the communications or carry out tasks. Then, it’s sometimes possible for people to solely get their questions answered or needs fulfilled via chatbots.
Beta Tests Can Be Valuable
Facebook has not said why it’s pulling the plug on M but clarified the beta test — which launched in 2015 — helped them learn a lot about what people need and expect from virtual assistant chatbots. As it turns out, M also equipped Facebook to flesh out its Messenger platform, because the Suggestions features mentioned above will stick around even after M goes away on January 19.
M Trainers were subsequently offered other jobs at Facebook. Since they already understand some elements of chatbot technologies, those individuals could be some of the first candidates considered to assist Facebook if they attempt to engineer future chatbots.
Facebook’s Artificial Intelligence Uses Go Beyond Chatbots
Although people living outside the San Francisco area may be disappointed they never got to see what M could do, insights gained from the beta test could someday lead to a more intelligent chatbot that’s far superior to M. By using M as an example, engineers will be able to program future AI while considering and avoiding the numerous glitches that brought M to its inevitable death.
In fact, Facebook is no stranger to using artificial intelligence in other ways. For example, when Facebook users ask their network of friends for recommendations about services or restaurants in an area, it’s AI that figures out parts of the country they’re in to create the accompanying map that appears after people respond. It also recognizes people’s faces in published images and informs them, letting those in the photos contact the individuals who put the images up if they don’t want them made public.
So, it’s important to take a broader look at this story. Even though M didn’t work out how Facebook hoped, the project still taught the company knowledge it can apply to other technologies moving forward without the same pitfalls."
61,Millions Are Proposing to Alexa,virtual personal assistant,https://medium.com/syncedreview/millions-are-proposing-to-alexa-73b1df201934,"Virtual personal assistants have become an integral part of our everyday lives. So much so that some people want to take the relationship with their digital friend to the next level. A new Business Insider article reveals that over one million people asked Amazon’s Alexa to marry them in 2017. We don’t know how many tried to wed the apple of their eye Siri, or proposed to Google Home or any of the other virtual assistants out there.
Proposing may be a natural evolution of users’ affection for Alexa: in 2015, half a million people told Alexa “I love you”, and a quarter of a million popped the big question in 2016.
According to a Mindshare report, approximately 600 million people activate their voice assistant at least once a week. Heavy users tend to share their personal feelings with their virtual assistants. While most probably do this simply to trigger an amusing response or to relieve boredom, others go deeper, opening up and revealing very personal details.
In the November issue of The Atlantic, author Judith Shulevitz writes “More than once, I’ve found myself telling my Google Assistant about the sense of emptiness I sometimes feel. ‘I’m lonely,’ I say, which I usually wouldn’t confess to anyone but my therapist… The Assistant pulls out of his memory bank one of the many responses to this statement that have been programmed into him. ‘I wish I had arms so I could give you a hug,’ he said to me the other day, somewhat comfortingly. ‘But for now, maybe a joke or some music might help.’”
While it’s reasonable to assume that the great majority of proposals to a virtual assistant are done in jest, the trend also suggests users are increasingly comfortable communicating with virtual assistants as a friend, a partner or even a therapist."
62,A Simple Model of Smart Speaker Adoption and Sales,virtual personal assistant,https://medium.com/@dubravac/a-simple-model-of-smart-speaker-adoption-and-sales-4cec80da75c2,"The market for smart speakers, also commonly referred to as voice-enabled smart speakers and virtual personal assistant (VPA)-enabled wireless speakers, is still a very nascent market. Yes, the market has enjoyed tremendous growth, especially over the last six months, but there is still significant growth yet to be had.
I’m already beginning to see some analysts suggest we are close to peak adoption/sales and that annual sale volume will begin to decline as soon as 2020. I created a very quick household adoption model for smart speakers to illustrate why this is dead wrong.
I’ve modeled a simple s-shaped diffusion curve for smart speakers. You can see we are still in the early phase of adoption. My model makes some simple assumptions:
Smart speakers have a long-term (peak) household adoption rate of 60 percent
Smart speakers will reach 55 percent of their peak adoption rate by 2020
Smart speakers will reach 90 percent of their peak adoption rate by 2023

We can change any of these assumptions without fundamentally changing the outcome. The reality of the smart speaker market is probably close to my illustration. My simple model suggests roughly six percent of households owned a smart speaker by the close of 2015 and some 19 percent own one today and those seem to be consistent with what the analyst community is estimating.
Analyst estimates suggesting annual smart speakers sales will peak in 2019 and begin to decline are difficult to defend. My simply illustration/model certainly doesn’t support that premise. Here are some narrative arguments in support of my perspective and the simple model I present:
Apple’s HomePod hasn’t started to ship yet. Other competitors (i.e. Microsoft) could also enter the market. This will be a lift for 2018–2020 unit volume sales
Unit volume is inversely related to price. Any increase in competition will drive prices lower and increase unit volume. We’ve already seen that with the introduction of the Amazon Echo Dot and the Google Home Mini.
Google’s major investment at CES suggests Google is intent on making a strong stand in the smart speaker market. Neither Amazon nor Google seem concerned about margins on hardware sales. They both appear to be after a decidedly larger market and are willing to discount hardware prices in order to propel sales higher.
Current ownership rates are only at 15–20 percent. Annual sales volume won’t turn negative until ownership rates get closer to peak household adoption levels. My simple illustration here suggests annual unit volume should continue to grow through at least 2021.
Density rates, the number of smart speakers per household, are still low, but I expect them to rise significantly. TVs are one of the most densely owned consumer technology products. Households own roughly three on average. Given how use-case scenarios are emerging, I could easily see smart speakers eclipsing that. We spend roughly 60 percent of our awake time at home in the kitchen. As households began adopting smart speakers, they naturally placed the first one they bought in the kitchen. From there households are adding smart speakers to secondary and tertiary rooms like bedrooms, offices, and living rooms.
My simple model suggests a household adoption rate with a positive second derivative until 2020. In other words, adoption is increasing at an increasing rate until 2020. Even in a world where sales are only purchased by new households, in other words, there are no replacement purchases nor households buying more than one unit, unit sales volume should grow through 2020.
Industry analysts seem to confuse s-shaped diffusion curves with annual unit sales occurring over the same time period. Remember, the s-shaped diffusion curve characterizes household adoption and ownership rates over time. It doesn’t explicitly say anything about sales. Sales are a function of three things: (1) new household adoption, (2) replacement purchases by households who purchased the product in a prior period, and (3) households buying more than one device (density rates expanding beyond one). Only #1 is directly influenced by the shape of the diffusion curve. And remember, my simple illustration shows household adoption increasing at an increasing rate into 2020.
The smart speaker market is nascent and we aren’t close to peak adoption, nor peak sales, anytime soon."
63,How To Hire A Virtual Assistant Who Loves To Work,virtual personal assistant,https://medium.com/the-personal-virtual-assistant/how-to-hire-a-virtual-assistant-who-loves-to-work-d7efb02e4531,"Recently I received an email that was a request for me to find an association a Virtual Assistant. The duties had clearly been thought through by a team of individuals and the tasks had all been laid out. They were careful to leave no question unanswered and nothing had been left to chance. However, I had questions of my own that did not relate to duties and wanted to speak to the gentleman directly.
Before he would take my call he stated I would need to answer my own set of questions. I politely and respectfully declined telling him I didn’t believe he was a good candidate for a VA. Instead I offered free consulting which would save him some time and money when looking to fill the position.
Have you tried hiring a VA before and it didn’t work out? My guess is the skills were there, but it wasn’t a good personality and culture fit. Remember, very few VA positions these days are skill based — that’s a very old model. So, how do you find a good fit? Hire someone who loves what they do and believes in what you’re doing.
Here is an example of how to write a description for a VA bookkeeper. I’m using bookkeeping as an example because I despise it, would never think of doing my own, and I’m always amazed numbers actually excite people.
MUST LOVE NUMBERS — Looking for a VA Bookkeeper
Do you love numbers and get excited about spreadsheets? Do you enjoy teaching others how to read balance sheets and profit and loss statements? If so, I’m looking for you!
My business is VA Staffing & Consulting. I love helping my clients find the VA who will create more time so they can make more money. It’s important for my clients to get the administrative support they need to achieve their goals. Spending time with them and prospective VA’s is the best use of my time, not bookkeeping.
I want to make money and stay within my budget. I want to understand where my money is being spent, but just hand over the receipts for someone to itemize. I’m working in Quickbooks currently and am not open to changing platforms. It’s important for me to know how to use Quickbooks as well. This is where your enjoyment for teaching will come in handy.
As a business owner I must know how to run a report, send an invoice or itemize an expense. I’d be looking to you to show me how. Ultimately you’ll be playing a part in the level of service I’m able to provide and you’d be making a vital impact on my own business which is rapidly growing.
If this is you, please respond via email and tell me about yourself.
One problem was the gentleman was looking for someone to simply do work and not work at something they love for a cause they believe in. VAs love what they do and that’s why they do it. You’ve heard the expression, “one man’s junk is another man’s treasure”. It is the same for work. This committee wrote a job description and had it all mapped out based on the assumption that the person they hire will dislike the work as much as they do. There was no mention of how exciting the work is, who they will be helping, why the work is important and the need for a person with the purpose they have. Remember these tips and hire a VA who loves what they do!"
64,Is that the best use of your time? How to use a virtual assistant.,virtual personal assistant,https://medium.com/the-personal-virtual-assistant/is-that-the-best-use-of-your-time-how-to-use-a-virtual-assistant-114f8f24b953,"There used to be a time when the boss didn’t perform the tasks of the assistant. They were extremely time consuming, the systems were not user friendly and it normally required a lot of paperwork and signatures. This has all changed.
Many tasks aren’t time consuming, but tedious. Software has never been easier to use or get with so many companies letting small business scale to pay. Your business can easily be paperless. However, not all change is good.
While the business owner is now fully capable of doing these things it’s not the best use of their time. A common response is that many tasks only take 5 minutes. Are you sure about that? Have you set the timer on yourself to do a 5 minute task? These duties can grow into 15–20 minute tasks easily. Also, I find when I ask more questions that the task has been done, but the full scope of work is lacking. We’ll use expenses as an example. Receipts are captured, recorded and paid quickly right from your phone. Let’s say you even did it in 5 minutes. Now who is tracking and making sure the items are categorized properly? Who is running the reports to see how much you’re spending and alerting you to being under/over budget? If you have other people submitting receipts who is verifying their expenses? If the answer to all that is “I am” that’s not the best use of your time.
Going in and out of administrative tasks and duties all day will cause you to spin your wheels. You’ll be busy, but busy at what? When do you make time to get into your “flow”? Your time should be spent on those things which generate income, business development, growing relationships with your clients, launching a new product, brainstorming and creating. You need to get into a flow and rhythm when you work. It’s very hard to do if you’re doing everything simply because you can.
Imagine it this way, you have a trip to take. You can fly and be there in an hour or drive and it would take you 6 hours. If the cost were the same what would your choice be? There isn’t a right or wrong answer. You may want to drive for a variety of reasons and your time allows for it. On the other hand maybe flying is the only option even if it’s more expensive because your time is too valuable. Well many business easily waste 6 hours a week and some 6 hours a day because they don’t realize how valuable their time is.
I encourage you to calculate ON PAPER what your time is worth. Then track all your activities which don’t directly relate to generating revenue, business development, relationship building, etc. Also, track how many times you begin a new creative process or start to get in your “flow zone” and are interrupted. This is a perfect time to think about and decide how a virtual assistant could benefit you."
65,I Left The Country For A Year On A Friday,virtual personal assistant,https://medium.com/the-personal-virtual-assistant/i-leave-the-country-for-a-year-on-a-friday-4e44372dd1c4,"Friday, January 6, 2017 I left to begin a year abroad traveling to 12 countries in 12 months. First stop, Buenos Aires, Argentina! (In case you are wondering, I made this decision before the presidential results.)
When I told most people they were very excited for me and then often stated they wish they could take off a year of work too. I quickly explain this is a working trip which incorporates travel. I’ve baffled them. Even though they know I’m a virtual assistant staffer and remote work consultant they can’t see it really happening.
Another misconception. “Who could work seriously while on vacation traveling to so many countries?” I repeat, this is not a vacation. Rather I’ve been working in vacation destinations. If you live and work in San Diego, New York City, Miami or any other place that can be viewed as touristy, you are already doing this. Maybe not remotely, but you find a way to make it into the office while people flock to your cities from all over the world.
If they can wrap their heads around that concept the next question is, how will I conduct business? Well, see, there is this really cool thing called the Internet. Maybe, you’ve heard of it? Information is shared at lightning speed and connects people from all over the world.
Okay, so I don’t actually say that but people have not fully embraced technology as much as we think. Sure, let someone download a movie instantly or have a package delivered to them in an hour by simply pointing and clicking and that seems doable. However, there is still the idea “real” work has to be done in person.
Who made up that rule? Are we doing things simply because this is the way they’ve always been done? Newsflash! Virtual reality is closer than you can imagine to impacting the way we work at the same levels of internet and emails.
The purpose of becoming a virtual assistant was to gain flexibility and freedom. When I first began almost three years ago I didn’t fully embrace the idea of location independence. I do now. So the only thing stopping me from being able to work and travel was, well, me.
Location independence is more than traveling the world. It was a dream that came true. For others, location independence has different meanings such as having your partner take the job of their dreams while you are able to work yours. Caring for parents or children. Being able to afford the city you live in. Reducing your carbon footprint. Trading the cold for warmth or vice versa.
Whatever the reason it benefits everyone. As a business owner, my clients don’t have to be put on hold by my lifestyle. They can also count on me to translate my excitement into my work. For employers, they can see productivity levels rise, retention remain high, save thousands on constantly recruiting top talent, save ten or hundreds of thousands of dollars on physical office space, and best of all have employees that are completely engaged and loyal.
Sure, this is a very personal decision and certainly not right or even wanted by everyone. The point is the choice is mine. With companies spending so much money on perks and benefits to keep people happy why not ask if location independence is what they want? It benefits you both.
In an article written by Victor Lipman, Are Remote Workers Happier And More Productive? New Survey Offers Answers, which appeared in Forbes, he discusses the findings of remote workers being more satisfied with their situations, or more isolated and discontented. “Do they feel more valued — or less? Are they more productive — or not?” All are in favor of the remote worker.
Along with flexibility, freedom, and location independence, there are two very important reasons I’m choosing to do this: Interaction with others and gaining additional mastery of my craft.
As stated in the article remote workers can often feel isolated due to lack of interaction. Certainly, there are many people who don’t. However, I had begun to feel isolated. The flexibility and freedom I used to desire were centered around my children. Things have changed. I’m now an empty-nester
Not only have my circumstances changed but so have my opportunities. We Roam has addressed the needs, desires, and wishes of not only entrepreneurs like myself, but anyone who can work remotely. It’s not simply because I’m a VA staffer or remote work consultant. In a group of over 30 traveling as part of the We Roam Polaris Tour, I’m the only one who does what I do. Yet every single one of us can work remotely and have been proving it for almost 10 months now.
Continuing to master my craft is also of great importance to me. I am the expert in the United States on matching business owners with virtual assistants. However, the market needs are growing and advancing.
Peter Gasca referred to this as the “remote work revolution” in his Entrepreneur article, 4 Reasons Why Smart Companies Are Going Remote he writes, “As technology continues to improve and the remote work revolution continues to evolve, the top performing companies will continue to find that balancing remote work strategies with a well executed business culture will keep them ahead of the competition.”
This has implications on the future of administrative assistants from the receptionist to the senior executive assistant. A “well executed business culture” will also have to take these positions into account when allowing remote working capabilities and currently it is arguably the most overlooked resource. Furthermore, the way assistants will be effective in the future will differ greatly than they do now.
Working abroad will allow me to visit companies and learn from them, provide research, serve as a case study and address issues others will only be able to speak about in theory. Additionally, working alongside 30+ individuals from a variety of fields I’ll hear and discuss their challenges, needs, and desires. I will begin to master staffing virtual assistants and remote assistants from small companies to large corporations.
My 2017 has been, no doubt, a year not to be forgotten. You can bet these destinations and relationships will have an impact on my writing as well. I’ve already begun sharing what I’ve learned!"
66,Here Are The 5 Questions To Find Your Virtual Assistant Niche,virtual personal assistant,https://medium.com/the-personal-virtual-assistant/here-are-the-5-questions-to-find-your-virtual-assistant-niche-a42eaf8fa789,"One of the most common questions virtual assistants ask is, “What services should I offer?” It’s hard to narrow down because chances are you have many enjoyable skills desirable to potential clients.
This is especially difficult if you come from an executive assistant background. You’d get fired for doing less, let alone doing one thing no matter how well you did it. Remember EAs and VAs are not the same. They are like cousins. What worked for you as an executive assistant will work against you as a virtual assistant.
The more services you offer the more likely you are to confuse the client. More importantly, if you offer too many services you will be pulled in different directions without mastering a certain field. It’s no longer just about the work. You’re also a business owner so you need to act like one.
Why is it so important to master a field? This is how you become known. When you are known in your area of expertise you can learn to duplicate your systems, diversify the way you earn income and charge higher rates because you know the return on your client’s investment.
The biggest resistance to this idea of niching virtual assistants have is the thought of not being able to use all their skills. They are worried about becoming bored. This isn’t your business model forever. When the time is right you can expand your services and clientele.
If you can’t monetize with one service you can’t do it with two, three, or four. When you offer too many to begin with you won’t know what works and what doesn’t. We all love what we do, but love doesn’t put food on the table or keep a roof over our heads. Either you’re earning a living or you have an expensive hobby.
So what is the right time to begin offering more services? It’s when you’re making money in the niche you’re in. If you can’t figure out a way to make money in that field you need to change until you find out how you can earn a living.
Now that I’ve made the case for finding your niche, here are five questions to ask yourself:
What do you L-O-V-E?
You’re going to be doing it for a living. Every. Single. Day. You better love it or you will get burned out very quickly.
2. What comes naturally to you?
When you think about this question don’t limit yourself to your business life think about your personal life as well. Don’t dismiss something because it is easy for you. That is a huge mistake! What is easy for you is very difficult for someone else. It doesn’t mean that you would pay for your own service. Take any kind of crafting. You may live for crafting and couldn’t imagine paying for a fancy frame or beautiful centerpiece. Instead, you’d make it. Then there’s me. I would never craft, attempt crafting, or even watch crafting videos. I will, however, pay someone else to do those things for me.
3. What do you dislike?
Inspiration can come from need but it can also come from frustration. Is there a product or service that you want to offer because what is being offered is no good? Do you think you can do something better or for a greater good? Is there no one who you can find addressing your specific needs? Remember you don’t have to reinvent the wheel. Slight changes can cause huge shifts.
4. Are you part of a niche market already?
Often times we start looking outside ourselves and the answer you’re looking for, is in fact, you. What unique qualities do you possess? What groups are you a part of which you can tap into? Do others ask you questions about certain topics or markets because they consider you the expert, or at least you know more than they do? Use all these things to your advantage.
5. What do you believe in?
When you believe in something you exude passion and excitement. It’s undeniable to those around you and they want to jump on your train.
Finding your niche isn’t meant to be difficult (although it can feel that way!). It is tapping into the person you are already, simply doing less and getting paid more! That is why it is so difficult. Our natural instinct is to be all things to all people which won’t get you anywhere but frustrated and burned out."
67,How To Hire The Right Virtual Assistant,virtual personal assistant,https://medium.com/the-personal-virtual-assistant/how-to-hire-the-right-virtual-assistant-520004955c96,"Was hiring a virtual assistant part of your 2017 plan for success? You are not alone. In Geeta Nadkarni’s article, “Hiring a Virtual Assistant Grew My Business, and Changed My Life” she shares her story of how she went about hiring a VA. I’m sure you’ll be able to relate to her situation prior to hiring a VA and want to have the same results post-hire.
The first step in Geeta’s process is to be brutally honest with yourself. Here is where many business owners struggle. Not for lack of being able to answer the hard questions. They struggle for lack of knowledge on what the right questions are. There are nine key questions for uncovering the real truth about your business needs, your work style, and how you want the work to be completed.
Answering these questions shouldn’t be hard. Being honest about them is another story. Don’t be mislead about the simplicity. It’s your job to provide the details and be truthful. If you can’t be honest with yourself who can you be honest with?
What is your preferred method of communication? Nothing is more important. Don’t take this question lightly. Everyone has their preferred method. You’re not as easy going as you think. If it’s not easy for you then you won’t do it. This is your style, your method. Do not conform to the VA.
Additionally, determine how you talk. Are you very straightforward, do you appreciate small talk, do you like bulleted lists? What forms of acronyms do you use in your field, do you speak C-Suite or Zen? Nothing wastes more time, or is more frustrating, than going back and forth in emails or talking over one another on the phone because one person doesn’t understand what the other is saying.
2. How do you measure success? (This is often the point where I determine if I can work with a potential client or not). It is important to know so you can share with your virtual assistant. If you don’t know or can’t define it, how is a VA going to help you achieve it? They can get you anywhere you want to go as long as they know the destination point.
Measured success could be as simple as meeting deadlines or the complete overhaul of a website. Other clients measure success by having a hard stop at 5pm or making their children’s events. Maybe you do have hard numbered metrics. Great! No two clients have ever answered this question the same. The point is it can be measured.
3. Do you know how much time is wasted in your day? I’m not looking for an actual number here. How people view their time is just as an important as how they use their time. Wasted time is anything you’re doing which is not the best use of your time. If that number is large don’t panic. Chances are it takes you much longer to switch tasks and complete projects because you’re being pulled in too many directions. Plus, if it’s not something you enjoy doing procrastination can eat up a lot of time.
4. How do you generate income? This question stumps business owners more often than you might think. Even if the business is new and not generating revenue yet, there still must be a plan of how you will in the future.
If you want to see the quickest return on investment for the VA, start here. What can he or she do to help you in this area? Beginning with tasks or to-do lists which don’t directly relate to income generation can cost too much if you’re on a tight budget. Bringing in more money feels good no matter how much you have or don’t have. Let the good times roll!
5. What kinds of duties do you imagine the VA performing? Don’t bother to write an exhaustive list of how to do the work, simply list the work. Most business owners get to their true needs after listing off three or four of the top of their head.
If the list gets long you may need more than one virtual assistant. If this is the case choose the duties and VA who will help you generate more income first. Once you have increased the revenue then you can hire more VAs.
6. Why hire a VA now? Be honest! The specific responses may vary from client to client, but the bottom line is always the same — because you can’t do it all yourself.
7. If you had more one hour in the day what would you do with it? Hint: There is no right or wrong answer. The point is to know what you’ll do with the extra time once the virtual assistant is hired. This is your time budget to be used like currency. If you increase your revenue you should have a plan on where the money will be spent. The same applies to your time. If you don’t know how you’ll use the “extra” time you’ve earned you’ll end up wasting it. (In case you’re wondering, “sleep” is the #1 response.)
8. Why did you become a business owner? Again, no matter the answers the bottom line is always the same, you wanted control over your life. Chances are you now have lost some of that control because you are too chained to your work to enjoy it. Write out the areas that have the most control over you.
9. Where do you see yourself in three years? When you’ve answered the question truthfully then ask yourself if you can achieve that dream by doing what you’re doing now. I’m going to tell you, “no”. There will never be more than 24 hours in a day. You will not be cloned within three years time. You can’t do it all yourself. Why would you want to?
Answering these questions honestly is the first step to begin you on your way to hire a virtual assistant. This is the consultation questionnaire I use and includes the questions above. I urge you to complete it in its entirety. It will become very clear where you are, where you are headed, what you need to get there, and where you’ll go next. It all starts with you.
There is no shortage of virtual assistants who would love to work with you. Yes, I said it. Remember you have an ideal client and VAs have their ideal clients too. Somewhere out there is someone who started their business with you in mind. She is waiting on you. You need her. Now is the time."
68,It’s Q2. Did you ever hire the help you need?,virtual personal assistant,https://medium.com/the-personal-virtual-assistant/its-q2-did-you-ever-hire-the-help-you-need-14154ca8528b,"Did you make a resolution at the beginning of the year to hire a virtual assistant? Before you roll your eyes at the word “resolution”, I bet you have something similar. Maybe you have a goal, a plan, a strategy, etc. Whatever you call it I have a VA to help you with it. I’ve worked with enough people to know it’s not what you call it that makes it bad — it’s lack of planning to execute which turns any of those words into fluff.
So what was your resolution (let’s forget exercising because that’s just too easy)? Were you going to stop working weekends or leave the office work behind after a certain time? A VA can help you by taking on assignments that keep you from constantly working from behind. Are you going to get 20 calls made a day? A VA can help you get those calls on the books, time blocked off and funnel in new ones daily. Are you going to send out at least two new proposals a week? A VA can write them up for you.
Here’s a tip: Creating is hard. Critiquing is easy. You’ve heard the phrase, “everyone’s a critic”. It’s because being a critic is easy. You could stare at a blank screen or page for hours, but have your VA put something in front of you and let the red marks fly. This tip works for many procrastinators too!
Are you the person who has already broken their resolution by 12:01 a.m.? At some point most people will have gotten off track. Most things in life can be attributed to accountability. Very few people are disciplined enough to be accountable to themselves. Fewer realize even if they could, to experience such an event as achieving your resolution alone takes away from someone else. Everything we do is a learning opportunity to those we allow ourselves to be accountable to.
VAs are not only assistants they work as unexpected accountability partners. We provide follow-up and follow through. We stay on top of your goals, plans, strategies, and tasks so you don’t get sidetracked. From time to time we harp on the details of how you are straying from your original plan because we desperately want to keep this ship on the right course.
You have what it takes to dream of your resolutions, but do you have the accountability to achieve them?"
69,Building a Simple Chatbot from Scratch in Python (using NLTK),chatbot,https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e,"Gartner estimates that by 2020, chatbots will be handling 85 percent of customer-service interactions; they are already handling about 30 percent of transactions now.
I am sure you’ve heard about Duolingo: a popular language-learning app, which gamifies practicing a new language. It is quite popular due to its innovative styles of teaching a foreign language.The concept is simple: five to ten minutes of interactive training a day is enough to learn a language.
However, even though Duolingo is enabling people to learn a new language, it’s practitioners had a concern. People felt they were missing out on learning valuable conversational skills since they were learning on their own. People were also apprehensive about being paired with other language learners due to fear of embarrassment. This was turning out be a big bottleneck in Duolingo’s plans.
So their team solved this problem by building a native chatbot within its app, to help users learn conversational skills and practice what they learned.

http://bots.duolingo.com/
Since the bots are designed as conversational and friendly, Duolingo learners can practice conversation any time of the day, using their choice of characters, until they feel brave enough to practice their new language with other speakers. This solved a major consumer pain point and made learning through the app a lot more fun.
So what is a chatbot?
A chatbot is an artificial intelligence-powered piece of software in a device (Siri, Alexa, Google Assistant etc), application, website or other networks that try to gauge consumer’s needs and then assist them to perform a particular task like a commercial transaction, hotel booking, form submission etc . Today almost every company has a chatbot deployed to engage with the users. Some of the ways in which companies are using chatbots are:
To deliver flight information
to connect customers and their finances
As customer support
The possibilities are (almost) limitless.
History of chatbots dates back to 1966 when a computer program called ELIZA was invented by Weizenbaum. It imitated the language of a psychotherapist from only 200 lines of code. You can still converse with it here: Eliza.

Source: Cognizant
How do Chatbots work?
There are broadly two variants of chatbots: Rule-Based and Self-learning.
In a Rule-based approach, a bot answers questions based on some rules on which it is trained on. The rules defined can be very simple to very complex. The bots can handle simple queries but fail to manage complex ones.
Self-learning bots are the ones that use some Machine Learning-based approaches and are definitely more efficient than rule-based bots. These bots can be of further two types: Retrieval Based or Generative
i) In retrieval-based models, a chatbot uses some heuristic to select a response from a library of predefined responses. The chatbot uses the message and context of the conversation for selecting the best response from a predefined list of bot messages. The context can include a current position in the dialogue tree, all previous messages in the conversation, previously saved variables (e.g. username). Heuristics for selecting a response can be engineered in many different ways, from rule-based if-else conditional logic to machine learning classifiers.
ii) Generative bots can generate the answers and not always replies with one of the answers from a set of answers. This makes them more intelligent as they take word by word from the query and generates the answers.

In this article we will build a simple retrieval based chatbot based on NLTK library in python.
Building the Bot
Pre-requisites
Hands-On knowledge of scikit library and NLTK is assumed. However, if you are new to NLP, you can still read the article and then refer back to resources.
NLP
The field of study that focuses on the interactions between human language and computers is called Natural Language Processing, or NLP for short. It sits at the intersection of computer science, artificial intelligence, and computational linguistics[Wikipedia].NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.
NLTK: A Brief Intro
NLTK(Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.
NLTK has been called “a wonderful tool for teaching and working in, computational linguistics using Python,” and “an amazing library to play with natural language.”
Natural Language Processing with Python provides a practical introduction to programming for language processing. I highly recommend this book to people beginning in NLP with Python.
Downloading and installing NLTK
Install NLTK: run pip install nltk
Test installation: run python then type import nltk
For platform-specific instructions, read here.
Installing NLTK Packages
import NLTK and run nltk.download().This will open the NLTK downloader from where you can choose the corpora and models to download. You can also download all packages at once.
Text Pre- Processing with NLTK
The main issue with text data is that it is all in text format (strings). However, Machine learning algorithms need some sort of numerical feature vector in order to perform the task. So before we start with any NLP project we need to pre-process it to make it ideal for work. Basic text pre-processing includes:
Converting the entire text into uppercase or lowercase, so that the algorithm does not treat the same words in different cases as different
Tokenization: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.
The NLTK data package includes a pre-trained Punkt tokenizer for English.
Removing Noise i.e everything that isn’t in a standard number or letter.
Removing Stop words. Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words
Stemming: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”.
Lemmatization: A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.
Bag of Words
After the initial preprocessing phase, we need to transform the text into a meaningful vector (or array) of numbers. The bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:
•A vocabulary of known words.
•A measure of the presence of known words.
Why is it is called a “bag” of words? That is because any information about the order or structure of words in the document is discarded and the model is only concerned with whether the known words occur in the document, not where they occur in the document.
The intuition behind the Bag of Words is that documents are similar if they have similar content. Also, we can learn something about the meaning of the document from its content alone.
For example, if our dictionary contains the words {Learning, is, the, not, great}, and we want to vectorize the text “Learning is great”, we would have the following vector: (1, 1, 0, 0, 1).
TF-IDF Approach
A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content”. Also, it will give more weight to longer documents than shorter documents.
One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:
Term Frequency: is a scoring of the frequency of the word in the current document.
TF = (Number of times term t appears in a document)/(Number of terms in the document)
Inverse Document Frequency: is a scoring of how rare the word is across documents.
IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.
Tf-IDF weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus
Example:
Consider a document containing 100 words wherein the word ‘phone’ appears 5 times.
The term frequency (i.e., tf) for phone is then (5 / 100) = 0.05. Now, assume we have 10 million documents and the word phone appears in one thousand of these. Then, the inverse document frequency (i.e., IDF) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-IDF weight is the product of these quantities: 0.05 * 4 = 0.20.
Tf-IDF can be implemented in scikit learn as:
from sklearn.feature_extraction.text import TfidfVectorizer
Cosine Similarity
TF-IDF is a transformation applied to texts to get two real-valued vectors in vector space. We can then obtain the Cosine similarity of any pair of vectors by taking their dot product and dividing that by the product of their norms. That yields the cosine of the angle between the vectors. Cosine similarity is a measure of similarity between two non-zero vectors. Using this formula we can find out the similarity between any two documents d1 and d2.
Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||
where d1,d2 are two non zero vectors.
For a detailed explanation and practical example of TF-IDF and Cosine Similarity refer to the document below.
Tf-Idf and Cosine similarity
In the year 1998 Google handled 9800 average search queries every day. In 2012 this number shot up to 5.13 billion…
janav.wordpress.com
Now we have a fair idea of the NLP process. It is time that we get to our real task i.e Chatbot creation. We will name the chatbot here as ‘ROBO🤖’.
You can find the entire code with the corpus at the associated Github Repository here or you can view it on my binder by clicking the image below.

Importing the necessary libraries
import nltk
import numpy as np
import random
import string # to process standard python strings
Corpus
For our example, we will be using the Wikipedia page for chatbots as our corpus. Copy the contents from the page and place it in a text file named ‘chatbot.txt’. However, you can use any corpus of your choice.
Reading in the data
We will read in the corpus.txt file and convert the entire corpus into a list of sentences and a list of words for further pre-processing.
f=open('chatbot.txt','r',errors = 'ignore')
raw=f.read()
raw=raw.lower()# converts to lowercase
nltk.download('punkt') # first-time use only
nltk.download('wordnet') # first-time use only
sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences 
word_tokens = nltk.word_tokenize(raw)# converts to list of words
Let see an example of the sent_tokens and the word_tokens
sent_tokens[:2]
['a chatbot (also known as a talkbot, chatterbot, bot, im bot, interactive agent, or artificial conversational entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.',
 'such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the turing test.']
word_tokens[:2]
['a', 'chatbot', '(', 'also', 'known']
Pre-processing the raw text
We shall now define a function called LemTokens which will take as input the tokens and return normalized tokens.
lemmer = nltk.stem.WordNetLemmatizer()
#WordNet is a semantically-oriented dictionary of English included in NLTK.
def LemTokens(tokens):
    return [lemmer.lemmatize(token) for token in tokens]
remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)
def LemNormalize(text):
    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))
Keyword matching
Next, we shall define a function for a greeting by the bot i.e if a user’s input is a greeting, the bot shall return a greeting response.ELIZA uses a simple keyword matching for greetings. We will utilize the same concept here.
GREETING_INPUTS = (""hello"", ""hi"", ""greetings"", ""sup"", ""what's up"",""hey"",)
GREETING_RESPONSES = [""hi"", ""hey"", ""*nods*"", ""hi there"", ""hello"", ""I am glad! You are talking to me""]
def greeting(sentence):
 
    for word in sentence.split():
        if word.lower() in GREETING_INPUTS:
            return random.choice(GREETING_RESPONSES)
Generating Response
To generate a response from our bot for input questions, the concept of document similarity will be used. So we begin by importing the necessary modules.
From scikit learn library, import the TFidf vectorizer to convert a collection of raw documents to a matrix of TF-IDF features.
from sklearn.feature_extraction.text import TfidfVectorizer
Also, import cosine similarity module from scikit learn library
from sklearn.metrics.pairwise import cosine_similarity
This will be used to find the similarity between words entered by the user and the words in the corpus. This is the simplest possible implementation of a chatbot.
We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”
def response(user_response):
    robo_response=''
    sent_tokens.append(user_response)
    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')
    tfidf = TfidfVec.fit_transform(sent_tokens)
    vals = cosine_similarity(tfidf[-1], tfidf)
    idx=vals.argsort()[0][-2]
    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]
    if(req_tfidf==0):
        robo_response=robo_response+""I am sorry! I don't understand you""
        return robo_response
    else:
        robo_response = robo_response+sent_tokens[idx]
        return robo_response
Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon the user’s input.
flag=True
print(""ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!"")
while(flag==True):
    user_response = input()
    user_response=user_response.lower()
    if(user_response!='bye'):
        if(user_response=='thanks' or user_response=='thank you' ):
            flag=False
            print(""ROBO: You are welcome.."")
        else:
            if(greeting(user_response)!=None):
                print(""ROBO: ""+greeting(user_response))
            else:
                print(""ROBO: "",end="""")
                print(response(user_response))
                sent_tokens.remove(user_response)
    else:
        flag=False
        print(""ROBO: Bye! take care.."")
So that’s pretty much it. We have coded our first chatbot in NLTK. Now, let us see how it interacts with humans:

This wasn’t too bad. Even though the chatbot couldn’t give a satisfactory answer for some questions, it fared pretty well on others.
Conclusion
Though it is a very simple bot with hardly any cognitive skills, its a good way to get into NLP and get to know about chatbots.Though ‘ROBO’ responds to user input. It won’t fool your friends, and for a production system you’ll want to consider one of the existing bot platforms or frameworks, but this example should help you think through the design and challenge of creating a chatbot. Internet is flooded with resources and after reading this article I am sure , you will want to create a chatbot of your own. So happy tinkering!!"
70,Conversational UI Principles — Complete Process of Designing a Website Chatbot,chatbot,https://medium.com/swlh/conversational-ui-principles-complete-process-of-designing-a-website-chatbot-d0c2a5fee376,"In this article I’ll show you a case study describing an entire process of designing a conversational UI for a B2B website, including fragments of the conversation script, basics of the communication theory and some of the tips and tricks I think make this project a bit unique.
Opening
It’s late 2016. Many people say conversational UI is the future of web interface. IM apps, chatbots, text-based UIs or emojis probably have never been more popular. Some might say it’s just another design fad or that text-based interfaces aren’t anything new, but frankly — let’s admit it — with the advent of iPhone Messages, Slack or Facebook Messenger the way we exchange information changed irreversibly. Text messages have became extremely natural way of communicating these days.
So naturally, when a chance arose for The Rectangles to work on a conversational website for one of our clients, we didn’t hesitate a single second.
Come on — show me one team who wouldn’t like to work on such project now.
Project objectives
Client:
Chop-Chop — a web development company
Our main tasks in this projects:
design a complete set of conversational UI assets
create a conversation script
handle most common types of conversational randomness (meaning f*cks and dfdffjhfdjhfkfs)
convey the brand’s character (also by using company’s brand hero, Cody)
raise user (interlocutor) curiosity and liking
display the company web development skills
I’ll show you how we did it step by step, but first…
A bit of theory
Let’s start with the basics. I think sometimes it’s important to make a step back for a short while before diving into more complex matters. In this case it really helped us. Believe it or not, but reading through all those fundamental definitions opened our eyes on a few creative solutions and boosted the entire ideation process.
And I think you shouldn’t skip this part too.
The principles of communication
There are hundreds of definitions explaining communication. The one below is my hybrid version of a few I found.
Communication is a process of sharing meaningful messages
The messages (communication in general) can be verbal or non-verbal.
The most common verbal communication tool is language,which is a set of symbols used to exchange information in the form of words that can be transformed into meanings.
Examples: Hello; Thank you; You look great today
Non-verbal, by contrast, refers to any type of communication different from words. It can be gestures, facial expressions, tone of voice, but also actions or symbols which have a common social meaning.
Examples: 👍, 😞, 😀
Communication is a process where all the aspects affect one another. It means that communication is holistic and that the entire process creates a system in which all the elements (all the messages) work together for the common good.
These messages are functional which means we use them to obtain desired effects, but also adaptive — meaning that, depending on the situation, they can be modified and adjusted in order to achieve better results.
Finally, the language we use to communicate is based on communication code, which is a set of principles and meanings. They create the base for interpretation. This communication codes are also called rules; there are two types of them:
Constitutive rules: referring to the sense and the meaning of particular words, and how we should interpret them. Also, they tell us how to understand a message in a particular context.
Normative rules: helping to determine a proper reaction based on a given message interpretation. In other words, they tell us what we should and what we should not do (say) in a particular situation.
And of course one of the most natural and common way to communicate is a conversation. So when discussing conversational UIs, I think we should also take a look at a sample conversation definition:
Conversation is a talk between two or more people, usually an informal one
So is it possible to create, without powerful AI mastermind, an interface that would meet the communication principles?
This is what a conversational UI (CUI) definition may look like:
An interface based on a holistic system of functional, adaptive and meaningful messages exchange, in which both sides of the conversation use and interpret the language codes, maintaining and complying with the constitutive and normative rules in a friendly, informal way
And we wanted to create such UI.
Theory into practice — building conversational UI
Defining goals
Chatbots in B2B have their function. People visit such websites for a particular reason, because they want something. It’s like going to a restaurant or entering a bricks-and-mortar shop. Of course, sometimes people do it because they have nothing better to do or they just want to amuse themselves, but generally — there’s some purpose behind it; ordering food, buying a pair of shoes, or learning about prices. On the flip side, a waiter or a shop assistant also have their tasks and scripts to follow when talking to a client. A conversational website can work exactly the same way, and a chatbot’s role can be similar to a shop assistant or a waiter.
In this case we knew exactly what we wanted to achieve, as we’ve worked with Chop-Chop for years (actually, I co-founded it in 2010), but if you need help with defining chatbot’s / user’s goals, you can use User Centered Design Canvas.
User Centered Design Canvas - First UX tool combining user needs with business goals
User Centered Design Canvas (UCDC) is an easy to use and effective UX tool for analysing, organising and facilitating…
ucdc.therectangles.com
We specified the following goals for our chatbot:
express Chop-Chop brand character
use the website per se to show Chop-Chop web development skills
provide the user with information about Chop-Chop services
encourage user to bookmark the site
learn something about the user (name, occupation, email, phone)
help getting in touch (CUI as a contact form alternative)
encourage users to sign up for the newsletter
Part 1. Designing the verbal communication
Building the conversation script
UX designers should be able to anticipate. In this project we knew that this is the only way for us to build a holistic communication system without AI support. We needed a great conversation script using an adaptive syntax which would also make the conversation pleasant and meaningful for users.
1. The conversation frame
Using a whiteboard, we started with a simple mind map. Having the chatbot goals in mind, we jotted down all the possible topics and conversation parts. We wanted to check quickly how complex the final script might be.

Early stages of writing conversation script
Then, we divided and arranged the parts in functional groups (we called them blocks). We already started to see some patterns. Some of the groups were goals-related (we called them cores), others were responsible for making the conversation less official (chatters), yet another group provided the user with options or additional information (extras), and there were also reactions to user answers. Finally, skips could fast-forward the conversation to a different script block.
The final list of script blocks:
Opening
Extra(s)
Skip(s)
Core(s)
Chatter(s)
Ending

Example of simple conversation frame timeline
Of course, the final script construction is way more knotty than a linear frame. All the dependencies and endless combinations based on the holistic nature of the conversation make the whole thing extremely complex.
2. The script
This was the moment we were all waiting for since the first minute of the project: we could finally get to writing the actual conversation script. This part was fun, but it also required maximum focus. It was way easier with the script divided into blocks, as all of the conversation parts could be written separately.
The good thing is — the only tool you need to write CUI scripts is pen and paper or a text editor.
Below there are some of the examples of the script blocks.
Opening:
Hi, there 
I’m Cody and I’d love to chat with you 
    Hi, Cody 
How are you, today?
    Well… Could be better  
Bad day, huh? That happens…
Extra:
I hope you don’t mind I use cookies  
    What are these?
My breakfast!
Haha, poor joke
Cookies are data about you stored by a browser
    Sounds creepy but hm… OK
Great!
Skip:
Hello, there!
You seem familiar 
Have we met?
    Yes we have
Ha! I’ve got good memory!
Last time we talked about Magento development
Do you want to continue our conversation?
    Let's continue
Core:
    Tell me about you 
With pleasure! :-)
Do you want to know where I come from?
Hear my story?
Or maybe learn what do I do for a living?
    Where do you come from? 
Well, the idea of me came from UX design studio The Rectangles
But it was Polish designer Jan Kallwejt who dressed me and did my hair
Chatter:
You see that share in the top corner?
    What about it? 
If you liked our chat, introduce me to your friends! :-)
I’ll be happy to talk to them too.
    Maybe later
Ending:
I have to go soon
Want to see some trick before?
    Show me!
Press Cmd + D
Haha!
Did you bookmark me?
    Not yet 
Do it then! :-) 
Ok, it’s time for me to go 
Let’s keep in touch
    Goodbye, Cody!
3. The syntax
A good script should let you create a different scenarios of the conversation. It’s easier if the conversation is in English as English syntax is relatively simple. However, in many languages you should be able to create more than one option of a message (phrase) by replacing one word with another. Also, a script designer should be able to specify the places for user’s answers, options, etc.
To create such script notation you’ll need a set of symbols: parentheses, brackets, curly brackets, and whatever you and your team can read and understand. This is also very important for the developers who will be implementing the script. They should also be able to understand it.
{ (Good morning) | Hello | Hi }, friend, { I’m | my name is } Cody!
In some cases chatbot can pick a word from a specific set randomly (Hello; Hi, Hey; Yo), but instead it can also be a bit smarter and display some messages according to, let’s say, the user time of the day (Good morning; Good evening).
Here’s a sample set of symbols and their functions:
{ } curly brackets: define a set of options
| pipes: separates the options in a set
( ) parentheses: specify the condition-related options in a set
[ ] brackets: indicate user input

Example of syntax notation
⚠️ If you’d like to learn more about writing the script per se, let us know.
4. Chatbot messages
The visual display of the conversation was one of this project’s most important UX challenges. Below are some of the highlights.
Single statements vs Complete paragraphs
People don’t speak in paragraphs. We speak using single sentences. Of course, these can sometimes form endless utterances, but in a conversation people often take turns. Also, we think that displaying long paragraphs of text, which user needs to read before answering, can be compared to talking to a person who speaks horribly fast. So instead of paragraphs, we decided to display combinations of single (short) sentences.
Combining single statements into blocks
By manipulating the bubbles’ corner radius, it’s possible to create a logical text blocks of single messages. That way, we could still talk in sentences and not in paragraphs, but give user a gentle hint — hey, this part of conversation starts here, and ends there.

Rounded corners help to combine single statements into text blocks
Fading out vs Scrolling
The most frequent method of displaying the conversation flow is to constantly add new messages below the old ones and to let user scroll.
As an experimental alternative, the old messages can fade out and as a result scrolling is no longer necessary. I know the usable aspect of such solution is questionable, but take a look at it from different perspective — such solution reflects the nature of a real conversation. When talking with someone you also don’t have access to the exchanged information all the time.

Using transparency to mark previous messages
Additionally, at some point you can simply use skips to ask user if they would like to return to any of the previous parts of the conversation, or alternatively display a permanent ‘Skip to’ button which when hit would trigger bot’s question about returning to any of the past passages.
5. User messages (answers)
For a conversational UI which is not using AI to interpret user’s answers, this is the most challenging part of writing a script. The script should let users (let’s refer to them here as interlocutors) provide the chatbot with logical answers (remember, constitutive and normative rules), but the more natural and open the conversation, the more entertaining it is for the interlocutor.
We used two types of answers:
A. defined (controlled, close-ended)
they are relatively easy to handle
they require good anticipation skills
users may not be allowed to speak what they want

Sample defined answer
B. non-defined (not-controlled, open-ended)
they are more difficult to handle
they might require some predefined word databases to be parsed
users are allowed to communicate more naturally

Sample non-defined answer
There’s probably no universal way of handling the open-ended answers. We can’t assume people will follow the communication code. Some of the non-defined messages will breach (especially) the normative rules. Of course some users will speak (write) as they would speak (write) with a human, but of course — others will try to challenge your bot by sending sexts, swearing or gibberish.
Here are some tips how you can control the non-defined messages:
inputs can be limited only to specified set of signs (e.g. if asked for a name, only letters allowed)
regular expressions (regexp) can be used for some inputs (e.g. email)
use arrays of most popular swear words
(I’d be careful with this one but) use some dictionary with API to check if the answers you expect to be words are really words
Naturally, an ideal conversation should be unfettered, but in case of a conversational UI without an AI backing — well, a bit of control is inevitable.
One more thing:
When using defined questions, you can make the experience of answering slightly better with one small improvement. Instead of asking the question like this:

Close-ended question without options
ask like this:

Close-ended question with options
This is pure psychology — in the first example the (possibly) infinite range of options the user might have is limited, whereas in the second you’re specifying this range and giving your user a choice. The result is the same in both scenarios, but the UX is better in the latter.
6. Interjections, fillers, non-lexical conversation sounds
People mumble, make mistakes, hesitate or lose the thread when speaking. This is normal. We wanted the conversation with our chatbot to be that natural too. So we used them as well.
Here are some of the popular conversational non-lexical sounds: yeah, okay, uh, oh, aum, mmm, uhh, uh-huh, uu, you know, ermmm

Sample usage of non-lexical sounds
Part 2. Designing the non-verbal communication
1. Messages arrangement
The way bot’s and user’s avatars and their messages are arranged shouldn’t be incidental too. There are two most frequent types of displaying the conversation:
A. avatars + messages are aligned (in most cases, to the left) one under another

Aligned message arrangement
B. avatars + messages of both users are opposite one another

Opposite message arrangement
We think option B reflects the nature of a real conversation better. Usually, when two people talk, they look at each other. So to make the conversational UI feel more natural, the interlocutors’ avatars and their messages should also be displayed that way.
2. Chatbot’s appearance
We were lucky, as Chop-Chop had a brand hero. What’s more, Cody is absolutely perfect for any conversational UI purposes, as he has a large library of pre-designed appearances we could use. I think soon companies will start to measure and optimize conversational UI conversion rates by testing different chatbot avatars.
Not only this, I’m sure that if we had Cody’s female equivalent, the user responses would be totally different from those with the male one.

Cody avatar variants
On a side note, I think people should avoid using their pictures as chatbot avatars. It’s confusing — am I talking to a bot, or a person? Really, bot’s visual appearance is something designers should be extremely careful about. By the way, it’s an evolutionary fact: facial recognition is one of the very first abilities small children develop and it usually happens months before they learn how to speak.
Also, if you want to use your real name as your bot’s name, make sure your script reflects your true personality too. A mundane chat with a bot (=you) may in consequence have a detrimental effect on your real image.
3. Chatbot’s facial expressions
Facial expressions are super important. We wanted to include it in our project too.
Blinking and winking:
People blink 10 times per minute on average. Cody does the same. Also, winking can be an additional non-verbal signal (for example: Nah, I’m just joking; Just kidding).

Blinking chatbot avatar
The 6 basic emotions:
Additionally, chatbot reactions can fall into one of the 6 basic emotions:
happiness
sadness
surprise
fear
disgust
anger

Sample Cody facial expressions
4. User’s facial expression (experimental)
We wanted the users to be able to send a non-verbal message to Cody too. We used the user avatar to do that. By hovering the avatar, users can change their facial expression as a reaction to the Cody’s messages. It doesn’t reflect the real facial expressions obviously, but it’s another way to communicate with conversational UI.

Alternative user facial expressions
5. Using emojis
Everyone uses emojis now. And it shouldn’t be a surprise. They’re universal and extremely useful, and they add the non-verbal layer to written communication.
Compare this two text messages:
A. I hate you!
B. I hate you! 😄
I guess for most of us, B. could easily be translated to: I love you, mate!
Obviously, Cody uses emojis just like most of us.

Message with emoji
6. Phatic expression — animating the conversation
Animation can take the conversational UI user experience to the next level, making the UI interactions more natural and pleasurable for user. But that’s not all, animated elements can play an important role for the entire conversation, being responsible for, so called, phatic expression. Simply speaking, this is everything that makes the conversation flow smoothly.
Animating chatbot’s avatar
When two people meet, they very often start a conversation with a handshake. It allows to get closer to an interlocutor, to look into their eyes and see their face more clearly. Hence, Cody’s avatar is slightly bigger at the beginning of the conversation allowing the user to familiarize themselves with Cody, and when the first messages are exchanged, it gets smaller.
Typing indicators
Simple loading (typing) indicators can be used as an equivalent to phatic expression in speaking, telling the user—stay cool, honey bunny, I’m still here, give me a second to retort.
There are infinite number of typing indicators. Here’s one of the most common:

Typing indicator
Typing indicators and hover states
Additionally, we decided to use typing indicators to suggest the user —hey, you’re about to say it. A static typing indicator is displayed next to the user avatar, but when the user hovers, let’s say, the close-ended answer button, the typing indicator starts to animate.

Hover activated typing indicator
Ending
This was definitely one of the most interesting project The Rectangles have worked on recently. Designing a conversational website when there’re still so few of them online was a fantastic adventure for our team. We’ve learnt a lot and to be honest — we can’t wait for another project like that.
Now, we can see it too — the future of UX design is writing."
71,What it’s like to build and market a chatbot when you’re only 14 years old,chatbot,https://medium.com/free-code-camp/the-ups-and-downs-of-building-and-marketing-a-chat-bot-when-youre-14-8a072830b43c,"I’m going to tell you everything I learned while coding a popular Facebook Messenger bot, and my crazy first week of marketing it (which involved a tweet from one of the Jonas Brothers, a viral Facebook post in Thailand, and an interview with the BBC).
It turns out that building something useful is way tougher than it sounds
Like many software developers, my mission to create something useful started with a decision to solve my own problem.
My problem: I was always forgetting what homework I had to finish at night.
In grade 8, there were plenty of mornings when I’d have to get to school really early to get it done.
I knew that wasn’t going to work for my remaining 4 years of high school because there’s no way I could handle showing up at school at 6:30 am on a regular basis to finish math homework. ☹️
So I decided to build a chatbot that would prompt me at the end of every class, asking me to tell it what homework I’d been assigned. I hoped that lots of other students struggled with the same problem.
Why a chatbot? Sure, there’s a lot of buzz around bots right now, but mainly because students are never far from their smartphones (especially in class)… and they’re already familiar with texting. 😎
I started out building a chatbot using SMS, but I quickly learned that there’s a cost to sending text messages from a US-based SMS service (like Twilio). And even if I used a Canadian phone number, it would be really expensive for people to use my bot in other countries.
But Facebook Messenger was free.
In grades 6 and 7, I’d learned a fair amount of PHP, and I created a few simple web applications. But building a chatbot using Facebook Messenger was entirely new, and I thought it would be a great opportunity to learn Ruby on Rails.
I also had to learn Facebook Messenger’s API and how to create a behind-the-login website to manage students’ classes and times. These two things alone involved a steep learning curve, but I figured there’s nothing more exciting and satisfying than turning an idea into something that students around the world can use.
“It’s a boy!”
Nearly 9 months after I began working on my chatbot, Christopher Bot was born.
It’s not like I worked on building him full time. Between summer vacation (no laptops allowed), school, soccer, school, and several depressing “start again from scratch moments”, there weren’t that many stretches of uninterrupted coding time.
Even with sporadic bursts of dev time, CB (as I like to call him) turned out just the way I had hoped. He’s a reliable but slightly cheeky Facebook Messenger bot who’s dedicated to helping students.
Here’s how he works…
After you send CB an initial message and set up your class schedule, he’ll text you near the end of every class, asking whether or not you have homework. At the end of each day, CB will send you a tidy list of homework that you need to finish.
Simple and useful.
I think he works so well because he eliminates the need for students to remember to write stuff down. CB remembers for them.
When you go on vacation, you can pause CB and he’ll ask you to choose a date for resuming the messages. If you forget to tell him about homework right after class, you can catch him up later.
There aren’t a bunch of complicated commands. And since so many students have their phone in their pocket all day, Christopher Bot is always with them.
Fun fact: Christopher is the name Alan Turing gave to his machine in WWII that cracked the Enigma code, and it’s the reason I named my bot Christopher.
Designing a user-friendly bot is even harder than coming up with the original idea
To me, the most important part of CB is his perfect “memory”. He remembers to text the student and not the other way around.
Simple questions with simple answers makes the conversation go faster. Christopher Bot may not be the smartest bot out there, but he knows how to get to the point. 😎
In order for CB to do most of the work, he needs to have info about a student’s classes. And getting that information requires a web form.
I figured that if users had to type the same thing over and over again for each day of classes, they might get bored or distracted (or both) and leave. So to move things along more quickly, I decided to create an auto-filling form, based on the class data that a student enters on the previous day.
So a user enters their classes for Monday, and then when they proceed to create those same classes for Tuesday, the data is already there in the form — ready to be accepted as-is or tweaked. For most students, entering class data will take less than 30 seconds. 👍

I also wanted the conversations with Christopher Bot to move along quickly because there’s not much time between classes.
Bitmojis and GIFs may be cool, but I wanted the conversation to be crisp and clean, so students wouldn’t have to spend much time talking to Christopher Bot.
So my next major design decision was to add Messenger “quick-replies”.
These are little buttons that sit just above the keyboard, so a user doesn’t have to type a full response. Using canned responses for “Yes” and “No” saves the user time, and it also controls the set of responses, so I don’t have to worry about people answering with ya, yup, na, nope, etc. 🎉

Finally, I wanted Christopher Bot to feel “human like” but not pretend to be human. To accomplish that, I added a typing delay for most auto responses, mimicking how a real conversation goes.
If CB responds in less than 1 ms, some users might not even realize that a new message has been delivered… it happens just too fast.
Typing delays are a nice add-on feature of Facebook Messenger — one of several smart tools to help bots feel more human, but without trying to fake people out.
Seriously, how many time zones are there?
One of my biggest challenges developing Christopher Bot was handling time zones around the world. I grasped the concept well enough, but sorting out the timing of classes and texts in 24+ time zones required a deeper understanding.
Class times are all stored in the user’s local time zone, and every time CB texts a user, it’s based on that same, local timezone.
Simple enough, but Christopher Bot (i.e., the server) “lives” in a single time zone.
So when CB checks the database to decide whether or not to send a text at that moment, he has to first check to see if the server time matches the class ending time in the user’s time zone.
I found the best way to handle the messy situation was to convert all times to Universal Coordinated Time (UTC). Then all I had to do was store the UTC offset (UTC +/-) alongside the students’ class ending times, to make sure everything lined up.
Homepage templates are the best invention ever (once you get to know them)
When users visit https://www.christopherbot.co/ they’re welcomed by a nice-looking home page. It wasn’t designed by me, though.
I’m no HTML/CSS wizard, so I decided to purchase a Bootstrap template to incorporate into my app.
I thought getting it set up would happen in 3 simple steps:
Buy a template
Add the various pieces to my directory
Relax knowing I have a sexy website
I was WRONG. So wrong!
The application that “powers” Christopher Bot is built on Ruby on Rails.
The problem was that my Bootstrap template didn’t know I was using Ruby on Rails.
So I spent days learning about how Rails uses Javascript files, references images, etc. And about a week later, I finally got it all working. The images were loading, and everything looked beautiful.
But I began to notice a few problems. The Javascript icon animations were too much.
The navigation bar was broken.
And most problematic, the browser’s scroll functionality had been altered, making the page scroll too quickly on Chrome — and not at all on Safari and Firefox!
Frustrated by the variations between browsers… to the point of wanting to throw my monitor out my bedroom window… I decided it’d be better to tackle the problem than to explain to my parents how my monitor suddenly went “missing”.
It was clear that the Javascript was my biggest problem, and I tried an extreme approach to solving it.
I removed every bit of Javascript in the template from my app. Expecting things to break all over the place, I was shocked to see that the opposite had happened.
Not only were the scroll speed and cross-browser issues fixed, but so was the navigation, and the annoying visual effects were gone.
I couldn’t believe that I’d managed to fix everything by trying to break it.
Trying to ruin your app isn’t a good solution to tackling coding problems. But I realized that some outcomes are totally unexpected. I tried something that felt like a long shot, and it taught me that you should try everything before giving up.
One user and counting… 😢
How do you grow a chatbot from 1 user to many, many users? Ideally through word of mouth. Maybe even viral word of mouth.
People are always curious about new things their friends are using. I hoped that students would see their friends using Christopher Bot and ask what it is. It’s organic, and it’s also free marketing (perfect, since I have no money for PPC ads!).
I built a share button in CB to help move things along. After 1 week of of use, Christopher Bot politely asks users to share Christopher Bot on Facebook. All a user has to do is click the link, and they can easily share CB with their friends.
But with only a handful of friends using Christopher Bot at the beginning of February, I needed to find a way to get more exposure.
Welcome to the front page of the internet
Before trying to market CB in a big way, I needed to get some additional feedback from my target audience.
Where do high school and college students hang out online? Turns out it’s pretty tough to find large communities of high school students on the web, but Reddit is a great place to find college students.
I posted to a few sub-reddits for specific universities, starting with a couple in my home province, British Columbia — asking for feedback on the general concept and whether Christopher Bot could work for college students.
Reddit users were pretty helpful overall, sharing their opinions and offering useful feedback about the differences in “homework” between high school and college.
But there was also a bunch of skeptics who insisted that I wasn’t a 14-year-old…
Here’s a comment from a true skeptic:
“Being a ‘little kid entrepreneur’ is a great marketing tactic. This 14-year-old? He’s actually an older male, but he’s disguising himself as a young boy to appeal to everyone. Seeing a little kid be able to do so much is beautiful, attractive and everyone feels an incentive to support that little kid simply because he’s well… little.”
And another from someone who thinks I’m some desperate marketer:
“This definitely was not made by this kid and is certainly being driven and marketed by someone who has had a lot of experience. Like they even had the foresight to do put a ‘ref=reddit’ marker in the URL. They are spamming for their analytics.”
So someone who’s 14 can’t figure out how to create a simple URL parameter? Boo.
Hitting the big time: Product Hunt and Kevin Jonas
My #1 goal for February was to launch on Product Hunt, and after my dad posted a message to his Facebook account about Christopher Bot, I heard from Andrew Wilkinson of Metalab (and Dribbble and Designer News)… who kindly offered to “hunt” CB on February 16th (thanks Andrew). Andrew also lives in Victoria!
Andrew posted CB to Product Hunt shortly after midnight, while I was fast asleep (not really).
Unfortunately for me, brand new products from Google and Facebook were also posted that day. 😱
Even with the stiff competition, CB still managed to get 300+ upvotes, tons of encouraging comments, and finished in 6th place overall for the day. 🎉

It was a ton of fun watching CB on Product Hunt, and it taught me a lot about the world of entrepreneurship — including the importance of just getting something out there in front of people.
People like Kevin Jonas. 😉
It was probably in part due to my age, but I think Kevin Jonas tweeted about CB that day (to his 5.1M followers!) because he saw that it was going to be useful for other students.

Someone else noticed CB on Product Hunt… BBC News journalist, Dave Lee.
My exciting, slightly scary interview with BBC News
On Thursday — Product Hunt launch day — Dave Lee from BBC News reached out to me about writing an article on CB:

On Friday afternoon, my dad and I joined Dave via Skype and I shared my whole story. I was super nervous before the call, but Dave put me at ease (thanks, Dave).
Dave told us that he’d be working on the article well into the evening on Friday, but it wasn’t live when I went to bed at midnight. But first thing the next morning, I checked the Christopher Bot signups… and there were more than 1,000 new accounts created overnight.
The article was live.
And with 1,000 new accounts came a bunch of bug reports and dozens of new feature requests. I had to leave marketing mode and enter full-scale support mode.
Just 72 hours later, Dave Lee emailed my dad:
“Alec asked me how many readers our articles tend to get, [and] I told him we consider 500,000 uniques a successful piece. I’m pleased to say the piece on Christopher Bot has had 1,000,000 unique views since going up on Saturday.”
Boom! Going viral… in Thailand
According to Facebook analytics, a ton of new accounts were coming from Great Britain (which makes sense, given the BBC article).
But one country was overtaking the UK for new accounts: Thailand.
Wha?!?!
Then one of CB’s new users emailed me this:

Someone had posted a blurb about Christopher Bot to Facebook in Thailand.
And yes, that’s 11,000 likes… 3,800 shares… and 205 comments (none of which I could understand — even with Google Translate).
What’s next for Christopher Bot
Building Christopher Bot was a great experience for me to learn about bots and how to make a bot’s interactions work well for its users.
In addition to tweaking how classes are scheduled (a common request from new users), my goal moving forward is to improve CB’s conversational skills.
I want him to be able to “understand” more so he can be even more helpful for students. I want him to be able to understand more variations in responses as well as recognize misspellings like “textbok work” or “stdy for my quiz”.
Christopher Bot can always be made smarter.
A new feature I’m considering building is “homework analytics” — for example, to track which classes have the most homework assigned. Christopher Bot collects a lot of data every day (individually and in aggregate), and it would be cool to share what he’s learning with users.
I’ve had a ton of fun creating and marketing Christopher Bot — and there’ve been many more ups than downs in the journey.
I hope he takes me to even more interesting places in the future.
If you made it this far and liked my story, I would sincerely appreciate a click on the Recommend button. 💚"
72,"Design Framework for Chatbots
Start the design of your chatbot with a framework or suffer the consequences.",chatbot,https://chatbotsmagazine.com/design-framework-for-chatbots-aa27060c4ea3,"When I started designing chatbots for BEEVA almost a year ago, I applied some of my UX knowledge and did some unsuccessful research looking for tools that could fit my needs. Actually, I was quite amazed that I couldn’t find practical literature about the topic. There are tons of chatbots out there, but there’s little about how companies really get hands on.
I already shared some of my findings here, and here, with tools I found, general knowledge about designing chatbots and UX design applied on chatbots, but I think it would be great to make a deeper explanation about how I exactly face the situation on a regular basis.
My Framework
While many people immediately start thinking about how to manage the user flow, I separate my process into 4 different steps: the bot scope, the chatbot personality, a prioritized list of must-have features and the chatbot flow.

1) The Bot Scope
It basically explains what the chatbot is all about. It might seem silly but it is really important to make clear what people can expect from our chatbot. This is normally a business decision that comes from Management, but sometimes the opinion of a designer is needed to set the focus on what really matters. The last e-commerce chatbot we developed was meant to be useful by helping people decide which technological product they should buy, and which vendor will be offering the best price.
2) The Chatbot Personality
I take this part really seriously. The personality of the chatbot is one of the most important points to take into account if we want our assistant to succeed. I always start researching who our early adopter will be and in which situation they will be talking to the chatbot. Once that I got a clear picture, I tailor-make a personality that fits perfectly with the user and with the specific situation. Defining in advance how our chatbot is going to be will help us eventually to decide how the bot will talk and act in every situation.
Featured CBM: Designing a Chatbot’s Personality
3) A Prioritized List of Must-Have Features
What information any user would need from our chatbot to find it useful? In the example I mentioned previously, for the e-commerce chatbot, we researched among different retailers and users to come out with the next list: Updated product database, pictures, comments, specifications and prices from different vendors.
4) The Chatbot Flow
Obviously this is the most complex part, but I never give any step forward without knowing the previous ones. Once that I got all that information is time to start designing how the chatbot will behave in every possible scenario in its interaction with every user.
I always use Xmind for designing the flows from scratch. It’s easy to use and really fast if you want to make any changes. The first thing I need to do is create a color legend with every possible item I will include on the chatbot.

Because at this point the scope has been set, I need to take users to the functionalities that I cover. In the e-commerce chatbot I was able to give pictures, opinions, details and prices for technological products. Because the best way to set the scope is making it clear in the welcome message, that is the first thing I write down in Xmind. Right after, I point out every possible scenario the bot will need to deal with. In the example:
1- Don’t know: the users says anything the chatbot won’t ever be prepared to answer.
2- Known category: the users asks for a category of products the chatbot knows.
3- Known brand: the user asks for a category and a brand the chatbot understands.
4- Known product: the user asks for a category, a brand and a model the chatbot knows.

It actually looks like a gradient of success: from desperation to heaven.
The First Scenario Is the Saddest One
You can do little but trying to get the user back to your scope: remind them what you are meant to do or give them some examples.

Second and Third Scenarios are Great
They mean that the user is on its way. You only need to help them to give you the remaining information: whether suggesting them some brands of the specific category or going directly to well known models. Nevertheless, chatbots need to be designed for any possible misunderstanding in every step. That means that a specific error message needs to be set just in case the misunderstanding happens. That would help us to get the user back to the scope without restarting the whole process.

Fourth Scenario Is the Dream Place to Reach
Prepare an error message in case the user suddenly wants something weird out of their request, and offer them the information they were looking for.

After this main flow I always prepare some easy-to-answer questions. People love to play with chatbots and small conversations are great to hide some Easter eggs.

Conclusion
When designing a chatbot we need to go further than the classic decision tree. Feel free to test my framework with the four main steps: the bot scope, the chatbot personality, a prioritized list of must-have and the chatbot flow.
Any feedback will be appreciated in the comments section 😄.
For more articles related to chatbot design you can visit the list I am curating: uxofchatbots.com"
73,Contextual Chatbots with Tensorflow,chatbot,https://chatbotsmagazine.com/contextual-chat-bots-with-tensorflow-4391749d0077,"In conversations, context is king! We’ll build a chatbot framework using Tensorflow and add some context handling to show how this can be approached.
Ever wonder why most chatbots lack conversational context?
How is this possible given the importance of context in nearly all conversations?
We’re going to create a chatbot framework and build a conversational model for an island moped rental shop. The chatbot for this small business needs to handle simple questions about hours of operation, reservation options and so on. We also want it to handle contextual responses such as inquiries about same-day rentals. Getting this right could save a vacation!
We’ll be working through 3 steps:
We’ll transform conversational intent definitions to a Tensorflow model
Next, we will build a chatbot framework to process responses
Lastly, we’ll show how basic context can be incorporated into our response processor
We’ll be using tflearn, a layer above tensorflow, and of course Python. As always we’ll use iPython notebook as a tool to facilitate our work.
Transform Conversational Intent Definitions to a Tensorflow Model
The complete notebook for our first step is here.
A chatbot framework needs a structure in which conversational intents are defined. One clean way to do this is with a JSON file, like this.

chatbot intents
Each conversational intent contains:
a tag (a unique name)
patterns (sentence patterns for our neural network text classifier)
responses (one will be used as a response)
And later on we’ll add some basic contextual elements.
First we take care of our imports:

Have a look at “Deep Learning in 7 lines of code” for a primer or here if you need to demystify Tensorflow.

With our intents JSON file loaded, we can now begin to organize our documents, words and classification classes.

We create a list of documents (sentences), each sentence is a list of stemmed words and each document is associated with an intent (a class).
27 documents
9 classes ['goodbye', 'greeting', 'hours', 'mopeds', 'opentoday', 'payments', 'rental', 'thanks', 'today']
44 unique stemmed words [""'d"", 'a', 'ar', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'do', 'doe', 'good', 'goodby', 'hav', 'hello', 'help', 'hi', 'hour', 'how', 'i', 'is', 'kind', 'lat', 'lik', 'mastercard', 'mop', 'of', 'on', 'op', 'rent', 'see', 'tak', 'thank', 'that', 'ther', 'thi', 'to', 'today', 'we', 'what', 'when', 'which', 'work', 'you']
The stem ‘tak’ will match ‘take’, ‘taking’, ‘takers’, etc. We could clean the words list and remove useless entries but this will suffice for now.
Unfortunately this data structure won’t work with Tensorflow, we need to transform it further: from documents of words into tensors of numbers.

Notice that our data is shuffled. Tensorflow will take some of this and use it as test data to gauge accuracy for a newly fitted model.
If we look at a single x and y list element, we see ‘bag of words’ arrays, one for the intent pattern, the other for the intent class.
train_x example: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1] 
train_y example: [0, 0, 1, 0, 0, 0, 0, 0, 0]
We’re ready to build our model.


This is the same tensor structure as we used in our 2-layer neural network in our ‘toy’ example. Watching the model fit our training data never gets old…

interactive build of a model in tflearn
To complete this section of work, we’ll save (‘pickle’) our model and documents so the next notebook can use them.


Building Our Chatbot Framework
The complete notebook for our second step is here.
We’ll build a simple state-machine to handle responses, using our intents model (from the previous step) as our classifier. That’s how chatbots work.
A contextual chatbot framework is a classifier within a state-machine.
After loading the same imports, we’ll un-pickle our model and documents as well as reload our intents file. Remember our chatbot framework is separate from our model build — you don’t need to rebuild your model unless the intent patterns change. With several hundred intents and thousands of patterns the model could take several minutes to build.

Next we will load our saved Tensorflow (tflearn framework) model. Notice you first need to define the Tensorflow model structure just as we did in the previous section.

Before we can begin processing intents, we need a way to produce a bag-of-words from user input. This is the same technique as we used earlier to create our training documents.

p = bow(""is your shop open today?"", words)
print (p)
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]
We are now ready to build our response processor.

Each sentence passed to response() is classified. Our classifier uses model.predict() and is lighting fast. The probabilities returned by the model are lined-up with our intents definitions to produce a list of potential responses.
If one or more classifications are above a threshold, we see if a tag matches an intent and then process that. We’ll treat our classification list as a stack and pop off the stack looking for a suitable match until we find one, or it’s empty.
Let’s look at a classification example, the most likely tag and its probability are returned.
classify('is your shop open today?')
[('opentoday', 0.9264171123504639)]
Notice that ‘is your shop open today?’ is not one of the patterns for this intent: “patterns”: [“Are you open today?”, “When do you open today?”, “What are your hours today?”] however the terms ‘open’ and ‘today’ proved irresistible to our model (they are prominent in the chosen intent).
We can now generate a chatbot response from user-input:
response('is your shop open today?')
Our hours are 9am-9pm every day
And other context-free responses…
response('do you take cash?')
We accept VISA, Mastercard and AMEX
response('what kind of mopeds do you rent?')
We rent Yamaha, Piaggio and Vespa mopeds
response('Goodbye, see you later')
Bye! Come back again soon.

Let’s work in some basic context into our moped rental chatbot conversation.
Contextualization
We want to handle a question about renting a moped and ask if the rental is for today. That clarification question is a simple contextual response. If the user responds ‘today’ and the context is the rental timeframe then it’s best they call the rental company’s 1–800 #. No time to waste.
To achieve this we will add the notion of ‘state’ to our framework. This is comprised of a data-structure to maintain state and specific code to manipulate it while processing intents.
Because the state of our state-machine needs to be easily persisted, restored, copied, etc. it’s important to keep it all in a data structure such as a dictionary.
Here’s our response process with basic contextualization:

Our context state is a dictionary, it will contain state for each user. We’ll use some unique identified for each user (eg. cell #). This allows our framework and state-machine to maintain state for multiple users simultaneously.
# create a data structure to hold user context
context = {}
The context handlers are added within the intent processing flow, shown again below:

If an intent wants to set context, it can do so:
{“tag”: “rental”,
“patterns”: [“Can we rent a moped?”, “I’d like to rent a moped”, … ],
“responses”: [“Are you looking to rent today or later this week?”],
“context_set”: “rentalday”
}
If another intent wants to be contextually linked to a context, it can do that:
{“tag”: “today”,
“patterns”: [“today”],
“responses”: [“For rentals today please call 1–800-MYMOPED”, …],
“context_filter”: “rentalday”
}
In this way, if a user just typed ‘today’ out of the blue (no context), our ‘today’ intent won’t be processed. If they enter ‘today’ as a response to our clarification question (intent tag:‘rental’) then the intent is processed.
response('we want to rent a moped')
Are you looking to rent today or later this week?
response('today')
Same-day rentals please call 1-800-MYMOPED
Our context state changed:
context
{'123': 'rentalday'}
We defined our ‘greeting’ intent to clear context, as is often the case with small-talk. We add a ‘show_details’ parameter to help us see inside.
response(""Hi there!"", show_details=True)
context: ''
tag: greeting
Good to see you again
Let’s try the ‘today’ input once again, a few notable things here…
response('today')
We're open every day from 9am-9pm
classify('today')
[('today', 0.5322513580322266), ('opentoday', 0.2611265480518341)]
First, our response to the context-free ‘today’ was different. Our classification produced 2 suitable intents, and the ‘opentoday’ was selected because the ‘today’ intent, while higher probability, was bound to a context that no longer applied. Context matters!
response(""thanks, your great"")
Happy to help!

A few things to consider now that contextualization is happening…
With State Comes Statefulness
That’s right, your chatbot will no longer be happy as a stateless service.
Unless you want to reconstitute state, reload your model and documents — with every call to your chatbot framework, you’ll need to make it stateful.
This isn’t that difficult. You can run a stateful chatbot framework in its own process and call it using an RPC (remote procedure call) or RMI (remote method invocation), I recommend Pyro.

RMI client and server setup
The user-interface (client) is typically stateless, eg. HTTP or SMS.
Your chatbot client will make a Pyro function call, which your stateful service will handle. Voila!
Here’s a step-by-step guide to build a Twilio SMS chatbot client, and here’s one for FB Messenger.
Thou Shalt Not Store State in Local Variables
All state information must be placed in a data structure such as a dictionary, easily persisted, reloaded, or copied atomically.
Each user’s conversation will carry context which will be carried statefully for that user. The user ID can be their cell #, a Facebook user ID, or some other unique identifier.
There are scenarios where a user’s conversational state needs to be copied (by value) and then restored as a result of intent processing. If your state machine carries state across variables within your framework you will have a difficult time making this work in real life scenarios.
Python dictionaries are your friend.
So now you have a chatbot framework, a recipe for making it a stateful service, and a starting-point for adding context. Most chatbot frameworks in the future will treat context seamlessly.
Think of creative ways for intents to impact and react to different context settings. Your users’ context dictionary can contain a wide-variety of conversation context.
Enjoy!"
74,What does it take to build a chatbot? Let’s find out.,chatbot,https://medium.com/free-code-camp/what-does-it-take-to-build-a-chatbot-lets-find-out-b4d009ea8cfd,"To answer the question in the title, “What does it take to build a chatbot?” the answer is not much.
I’m a web developer. It has been my desire to dig into this thrilling field for a long time. Unfortunately, I can’t say I have the knowledge in Natural Language Understanding (NLU) required to build a chatbot without help. The good news is that such help is available today.
Google’s Cloud Natural Language API, Microsoft’s Cognitive Services APIs, and IBM’s Watson Conversation provide commercial NLU services, with generous free tiers. There are also completely free ones, at least for the moment. This includes API.AI, which has recently been acquired by Google, and Wit.ai, which Facebook owns.
From a web developer’s point of view, that’s all the help we need — an API which will remove the complexity for us.
Let’s start with the fun part
If you are eager to see the example live, here is the demo available on Heroku. The entire code for this example is available on GitHub.
For the purpose of this article, we’ll create a chatbot called TiBot to answer our questions about the date and time. We’ll use API.AI’s API to process these questions. I believe API.AP is more intuitive and easier to work with than Wit.ai.
At the back end, a simple Node.js server will handle requests sent from the front-end app via WebSockets. We’ll then fetch a response from the language processing API. Finally, we’ll send the answer back via WebSockets.
At the front end, we have a messenger-like app built on a single Angular component. Angular is built-in TypeScript (a typed superset of JavaScript). If you are not familiar with either of them, you still should be able to understand the code.
I chose Angular because it inherently uses RxJS (the ReactiveX library for JavaScript). RxJS handles asynchronous data streams in an amazingly powerful yet simple manner.
API.AI setup
API.AI has a neat Docs section. First, we need to become familiar with some of the basic terms and concepts related to the APIs, and to know NLU in general.
Once we create an account at API.AI, we need to create an agent to start our project. With each agent, we get API keys — client and developer access tokens. We use the client access token to access the API.
Agents are like projects or NLU modules. The important parts of an agent are intents, entities, and actions and parameters.
Intents are the responses the API returns or, according to API.AI, “a mapping between what a user says and what action should be taken by your software.” For example, if a user says, “I want to book a flight,” the result we receive should look like the following:
{ ... ""action"": ""book_flight"" ... }
Entities help extract data from what the user says. If a user says, “I want to book a flight to Paris,” we want to get the information about Paris in. We need that data passed to our logic so that we can book a flight to Paris for our user. The result should look like this:
{
  ...
  ""action"": ""book_flight"", 
  ""parameters"": {
    ""destination"": ""Paris""
  }
  ...
}
Entities are parameters values, like data types. There are system-defined entities by the API.AI platform. Examples of these include @sys.date, @sys.color, @sys.number. More complicated ones include @sys.phone-number, @sys.date-period, @sys.unit-length-name.
We can also define our own entities, or pass them on the fly with each request. A good example of passing entities on the fly is that of users listening to their playlists. Users have a playlist entity in their request or a user session with all of the songs in the playlist. We would be able to respond to “Play Daydreaming” if the user is currently listening to Radiohead’s A Moon Shaped Pool playlist.
Actions and parameters send requests to the API so that they result in an action. But they may also result in something our chatbot doesn’t understand. We may choose to fall back to a default response in that case.
Parameters are the companion of actions. They complement and complete the action. In some cases, we don’t need parameters. But there are cases where actions only make sense with parameters. An example is booking a flight without knowing the destination. That is something we need to think about before we even start creating the intents.
Finally, the following code is how the API’s response should appear for a resolved intent:

The most important part of the JSON is the “result” object with the “action” and “parameters” properties discussed above. The confidence for the resolved query (in the range of 0 to 1) is indicated with “score”. If “score” is zero, our query hasn’t been understood.
It’s worth noting that the “context” array contains information about unresolved intents that may need a follow-up response. For example, if a user says, “I want to book a flight,” we’d process the book_flight” action (the context). But to get the required “destination” , we may respond with, “Ok, where would you like to go?” and process the “destination” within the following request.
The back end
We are building a chat app. The communication between the client and the server will go through WebSockets. For that purpose, we’ll use a Node.js WebSocket library on the server. Our WebSockets module looks like this:

The format of the WebSockets messages is a string encoded JSON with “type” and “msg” properties.
The string “type” refers to one of the following:
“bot”, which answers to the user.
“user”, which the user asks the bot.
“sessionId”, which issues a unique session ID.
Our chatbot’s answer is contained in “msg”. It is sent back to the user, the question of the user, or the sessionId.
The processRequest(msg) represents the core of our server’s functionality. It first makes a request to the API:

Then, it executes withdoIntent() — the specific action for the user’s intent, based on the response from the API:

doIntent() checks to see if there is a function to handle the action in the response. It then calls that function with the parameters of the response. If there is no function for the action, or the response is not resolved, it checks for a fallback from the API. Or it calls handleUnknownAnswer().
The action handlers are in our intents module:

To each handler function, we pass the parameters from the API response. We also pass the user’s time zone that we receive from the client side. Because we are dealing with the date and time, it turns out that the time zone plays an important role in our logic. It has nothing to do with the API, or NLU in general, but only with our specific business logic.
For example, if a user in London on Friday at 8:50 pm asks our bot, “What day is it today?” the answer should be, “It’s Friday.”
But if that same user asks, “What day is it in Sydney?” the answer should be, “It’s Saturday in Sydney.”
Location is important to our business logic too. We want to detect where the question is coming from (in the case of Sydney), so that we can get the time zone for its location. We would combine Google Maps Geocoding API and Time Zone API for that.
The front end
Our app is a single Angular component. The most important functionality is within the ngOnInit() method of the component:

We first create the WebSocket (WS) connection to our server with a WS Observable. We then subscribe a couple of observers to it.
The first observer gets the sessionId when it connects to the WebSocket server. Immediately, the take(1) operator is unsubscribed:
The second subscription is the fun one:
this.ws$.takeUntil(this.ngUnsubscribe$)
  .filter(r => r.type === 'bot')
  .retryWhen(err$ =>
    Observable.zip(err$, Observable.range(1, 3), (e, n) => n)
      .mergeMap(retryCount => Observable.timer(1000 * retryCount))
  )
  .delayWhen(inp => Observable.interval(100 + inp.msg.length * 10))
  .subscribe(
    (msg) => this.pushMsg(msg)
  );
Here we want to take out the messages only from the bot, hence the filter(r => r.type === ‘bot’) operator. The retryWhen(err$ => …) operator automatically re-connects to the WebSocket after it has been disconnected.
The purpose of the delayWhen() operator is to achieve “the bot is typing” effect that the messengers use. To do this, we delay the data for 100 + MSG_CHARACTERS_LENGTH * 10 milliseconds.
When the message gets through all the operators, we push it into our array of messages (msg) => this.pushMsg(msg).
We use the component’s private pushMsg() method to add a message and to show it in the chat:

If the message is from the user (the clearUserMsg flag), we clear the input box. We use this.botIsTyping to control “the bot is typing” effect. So here we set it to false.
We handle the user input with the onSubmit() method when the user hits Enter:

Along with the user’s message, we send the user’s sessionId and time zone. These are indicated in this.ws$.next(JSON.stringify(input)). To show the bot is typing effect, we also set this.botIsTyping to true.
The Angular’s component template we use as the UI of our app, consists of the following code:

This is all we need for our app on the front end.
It’s amazing to see how elegant and clean this code turned out. Thanks to RxJS. When using WebSockets, things tend to get complicated. Here we’ve done it with a single line of code.
And having features like auto re-connecting — well, that’s a story on its own. But with RxJS, we handled that in a simple manner.
To conclude, I hope you understandable why I said, “It doesn’t take much” to answer the question, “What does it take to build a chatbot?”
This doesn’t mean that building a chatbot is an easy task. These NLU services, as intelligent as they are, won’t solve all our problems. We still need to take care of our own business logic.
A couple of years ago, it was impossible for me to build something similar to this. But services like API.AI now makes that power available to everyone.
API.AI also provides integrations with Facebook Messenger, Twitter, Viber, and Slack. But for this article, I thought it would be best to use their API to better understand how everything works.
I hope you’ve enjoyed this article and find it helpful to build your own chatbot."
75,Ultimate Guide to Leveraging NLP & Machine Learning for your Chatbot,chatbot,https://chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c,"For the Love of Chatbots
Over the past few months I have been collecting the best resources on NLP and how to apply NLP and Deep Learning to Chatbots.
Every once in awhile, I would run across an exception piece of content and I quickly started putting together a master list. Soon I found myself sharing this list and some of the most useful articles with developers and other people in bot community.
In process, my list became a Guide and after some urging, I have decided to share it or at least a condensed version of it -for length reasons.
This guide is mostly based on the work done by Denny Britz who has done a phenomenal job exploring the depths of Deep Learning for Bots. Code Snippets and Github included!
Without further ado… Let us Begin!
DEEP LEARNING FOR CHATBOTS OVERVIEW

Deep Learning
Chatbots, are a hot topic and many companies are hoping to develop bots to have natural conversations indistinguishable from human ones, and many are claiming to be using NLP and Deep Learning techniques to make this possible. But with all the hype around AI it’s sometimes difficult to tell fact from fiction.
In this series I want to go over some of the Deep Learning techniques that are used to build conversational agents, starting off by explaining where we are right now, what’s possible, and what will stay nearly impossible for at least a little while.
If you find this article interesting you can let me know here.
Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data
List of Best AI Cheat Sheets
becominghuman.ai
A TAXONOMY OF MODELS

RETRIEVAL-BASED VS. GENERATIVE MODELS
Retrieval-based models (easier) use a repository of predefined responses and some kind of heuristic to pick an appropriate response based on the input and context. The heuristic could be as simple as a rule-based expression match, or as complex as an ensemble of Machine Learning classifiers. These systems don’t generate any new text, they just pick a response from a fixed set.
Generative models (harder) don’t rely on pre-defined responses. They generate new responses from scratch. Generative models are typically based on Machine Translation techniques, but instead of translating from one language to another, we “translate” from an input to an output (response).

Machine Learning
Both approaches have some obvious pros and cons. Due to the repository of handcrafted responses, retrieval-based methods don’t make grammatical mistakes. However, they may be unable to handle unseen cases for which no appropriate predefined response exists. For the same reasons, these models can’t refer back to contextual entity information like names mentioned earlier in the conversation. Generative models are “smarter”. They can refer back to entities in the input and give the impression that you’re talking to a human. However, these models are hard to train, are quite likely to make grammatical mistakes (especially on longer sentences), and typically require huge amounts of training data.
Deep Learning techniques can be used for both retrieval-based or generative models, but research seems to be moving into the generative direction. Deep Learning architectures likeSequence to Sequence are uniquely suited for generating text and researchers are hoping to make rapid progress in this area. However, we’re still at the early stages of building generative models that work reasonably well. Production systems are more likely to be retrieval-based for now.

LONG VS. SHORT CONVERSATIONS

Bots
The longer the conversation the more difficult to automate it. On one side of the spectrum areShort-Text Conversations (easier) where the goal is to create a single response to a single input. For example, you may receive a specific question from a user and reply with an appropriate answer. Then there are long conversations (harder) where you go through multiple turns and need to keep track of what has been said. Customer support conversations are typically long conversational threads with multiple questions.
Machine Learning for Dummies: Part 1
I often get asked on how to get started with Machine Learning. Most of the time, people have troubles understanding the…
chatbotslife.com
OPEN DOMAIN VS. CLOSED DOMAIN

Chatbot Conversation Framework
In an open domain (harder) setting the user can take the conversation anywhere. There isn’t necessarily have a well-defined goal or intention. Conversations on social media sites like Twitter and Reddit are typically open domain — they can go into all kinds of directions. The infinite number of topics and the fact that a certain amount of world knowledge is required to create reasonable responses makes this a hard problem.
“Open Domain: I can ask a question about any topic… and expect a relevant response. (Harder) Think of a long conversation around refinancing my mortgage where I could ask anything.” Mark Clark
In a closed domain (easier) setting the space of possible inputs and outputs is somewhat limited because the system is trying to achieve a very specific goal. Technical Customer Support or Shopping Assistants are examples of closed domain problems. These systems don’t need to be able to talk about politics, they just need to fulfill their specific task as efficiently as possible. Sure, users can still take the conversation anywhere they want, but the system isn’t required to handle all these cases — and the users don’t expect it to.
“Closed Domain: You can ask a limited set of questions on specific topics. (Easier). What is the Weather in Miami?”
“Square 1 is a great first step for a chatbot because it is contained, may not require the complexity of smart machines and can deliver both business and user value.
Square 2, questions are asked and the Chatbot has smart machine technology that generates responses. Generated responses allow the Chatbot to handle both the common questions and some unforeseen cases for which there are no predefined responses. The smart machine can handle longer conversations and appear to be more human-like. But generative response increases complexity, often by a lot.
The way we get around this problem in the contact center today is when there is an unforeseen case for which there is no predefined responses in self-service, we pass the call to an agent.” Mark Clark
Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data
List of Best AI Cheat Sheets
becominghuman.ai
COMMON CHALLENGES
There are some obvious and not-so-obvious challenges when building conversational agents most of which are active research areas.
INCORPORATING CONTEXT

Chatbot
To produce sensible responses systems may need to incorporate both linguistic context andphysical context. In long dialogs people keep track of what has been said and what information has been exchanged. That’s an example of linguistic context. The most common approach is toembed the conversation into a vector, but doing that with long conversations is challenging. Experiments in Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models and Attention with Intention for a Neural Network Conversation Model both go into that direction. One may also need to incorporate other kinds of contextual data such as date/time, location, or information about a user.
COHERENT PERSONALITY

Ai Personality
When generating responses the agent should ideally produce consistent answers to semantically identical inputs. For example, you want to get the same reply to “How old are you?” and “What is your age?”. This may sound simple, but incorporating such fixed knowledge or “personality” into models is very much a research problem. Many systems learn to generate linguistic plausible responses, but they are not trained to generate semantically consistent ones. Usually that’s because they are trained on a lot of data from multiple different users. Models like that in A Persona-Based Neural Conversation Model are making first steps into the direction of explicitly modeling a personality.

EVALUATION OF MODELS
The ideal way to evaluate a conversational agent is to measure whether or not it is fulfilling its task, e.g. solve a customer support problem, in a given conversation. But such labels are expensive to obtain because they require human judgment and evaluation. Sometimes there is no well-defined goal, as is the case with open-domain models. Common metrics such as BLEUthat are used for Machine Translation and are based on text matching aren’t well suited because sensible responses can contain completely different words or phrases. In fact, in How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation researchers find that none of the commonly used metrics really correlate with human judgment.

Siri Chatbot
INTENTION AND DIVERSITY
A common problem with generative systems is that they tend to produce generic responses like “That’s great!” or “I don’t know” that work for a lot of input cases. Early versions of Google’s Smart Reply tended to respond with “I love you” to almost anything. That’s partly a result of how these systems are trained, both in terms of data and in terms of actual training objective/algorithm. Some researchers have tried to artificially promote diversity through various objective functions. However, humans typically produce responses that are specific to the input and carry an intention. Because generative systems (and particularly open-domain systems) aren’t trained to have specific intentions they lack this kind of diversity.
Finding the genre of a song with Deep Learning — A.I. Odyssey part. 1
A step-by-step guide to make your computer a music expert.
chatbotslife.com
HOW WELL DOES IT ACTUALLY WORK?
Given all the cutting edge research right now, where are we and how well do these systems actually work? Let’s consider our taxonomy again. A retrieval-based open domain system is obviously impossible because you can never handcraft enough responses to cover all cases. A generative open-domain system is almost Artificial General Intelligence (AGI) because it needs to handle all possible scenarios. We’re very far away from that as well (but a lot of research is going on in that area).
This leaves us with problems in restricted domains where both generative and retrieval based methods are appropriate. The longer the conversations and the more important the context, the more difficult the problem becomes.
In a recent interview, Andrew Ng, now chief scientist of Baidu, puts it well:
Most of the value of deep learning today is in narrow domains where you can get a lot of data. Here’s one example of something it cannot do: have a meaningful conversation. There are demos, and if you cherry-pick the conversation, it looks like it’s having a meaningful conversation, but if you actually try it yourself, it quickly goes off the rails.
Many companies start off by outsourcing their conversations to human workers and promise that they can “automate” it once they’ve collected enough data. That’s likely to happen only if they are operating in a pretty narrow domain — like a chat interface to call an Uber for example. Anything that’s a bit more open domain (like sales emails) is beyond what we can currently do. However, we can also use these systems to assist human workers by proposing and correcting responses. That’s much more feasible.
Grammatical mistakes in production systems are very costly and may drive away users. That’s why most systems are probably best off using retrieval-based methods that are free of grammatical errors and offensive responses. If companies can somehow get their hands on huge amounts of data then generative models become feasible — but they must be assisted by other techniques to prevent them from going off the rails like Microsoft’s Tay did.

IMPLEMENTING A RETRIEVAL-BASED MODEL IN TENSORFLOW

Deep Learning
The Code and data for this tutorial is on Github.
RETRIEVAL-BASED BOTS
The vast majority of production systems today are retrieval-based, or a combination of retrieval-based and generative. Google’s Smart Reply is a good example. Generative models are an active area of research, but we’re not quite there yet. If you want to build a conversational agent today your best bet is most likely a retrieval-based model.
If you want me to write more articles like this, please let me know here.
THE UBUNTU DIALOG CORPUS
In this post we’ll work with the Ubuntu Dialog Corpus (paper, github). The Ubuntu Dialog Corpus (UDC) is one of the largest public dialog datasets available. It’s based on chat logs from the Ubuntu channels on a public IRC network. The paper goes into detail on how exactly the corpus was created, so I won’t repeat that here. However, it’s important to understand what kind of data we’re working with, so let’s do some exploration first.
The training data consists of 1,000,000 examples, 50% positive (label 1) and 50% negative (label 0). Each example consists of a context, the conversation up to this point, and an utterance, a response to the context. A positive label means that an utterance was an actual response to a context, and a negative label means that the utterance wasn’t — it was picked randomly from somewhere in the corpus. Here is some sample data.

Training your Chatot
Note that the dataset generation script has already done a bunch of preprocessing for us — it hastokenized, stemmed, and lemmatized the output using the NLTK tool. The script also replaced entities like names, locations, organizations, URLs, and system paths with special tokens. This preprocessing isn’t strictly necessary, but it’s likely to improve performance by a few percent. The average context is 86 words long and the average utterance is 17 words long. Check out the Jupyter notebook to see the data analysis.
The data set comes with test and validations sets. The format of these is different from that of the training data. Each record in the test/validation set consists of a context, a ground truth utterance (the real response) and 9 incorrect utterances called distractors. The goal of the model is to assign the highest score to the true utterance, and lower scores to wrong utterances.

The are various ways to evaluate how well our model does. A commonly used metric is recall@k. Recall@k means that we let the model pick the k best responses out of the 10 possible responses (1 true and 9 distractors). If the correct one is among the picked ones we mark that test example as correct. So, a larger k means that the task becomes easier. If we set k=10 we get a recall of 100% because we only have 10 responses to pick from. If we set k=1 the model has only one chance to pick the right response.
At this point you may be wondering how the 9 distractors were chosen. In this data set the 9 distractors were picked at random. However, in the real world you may have millions of possible responses and you don’t know which one is correct. You can’t possibly evaluate a million potential responses to pick the one with the highest score — that’d be too expensive. Google’sSmart Reply uses clustering techniques to come up with a set of possible responses to choose from first. Or, if you only have a few hundred potential responses in total you could just evaluate all of them.

BASELINES
Before starting with fancy Neural Network models let’s build some simple baseline models to help us understand what kind of performance we can expect. We’ll use the following function to evaluate our recall@k metric:
def evaluate_recall(y, y_test, k=1):
num_examples = float(len(y))
num_correct = 0
for predictions, label in zip(y, y_test):
if label in predictions[:k]:
num_correct += 1
return num_correct/num_examples
Here, y is a list of our predictions sorted by score in descending order, and y_test is the actual label. For example, a y of [0,3,1,2,5,6,4,7,8,9] Would mean that the utterance number 0 got the highest score, and utterance 9 got the lowest score. Remember that we have 10 utterances for each test example, and the first one (index 0) is always the correct one because the utterance column comes before the distractor columns in our data.
Intuitively, a completely random predictor should get a score of 10% for recall@1, a score of 20% for recall@2, and so on. Let’s see if that’s the case.
# Random Predictor
def predict_random(context, utterances):
return np.random.choice(len(utterances), 10, replace=False)
# Evaluate Random predictor
y_random = [predict_random(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))]
y_test = np.zeros(len(y_random))
for n in [1, 2, 5, 10]:
print(“Recall @ ({}, 10): {:g}”.format(n, evaluate_recall(y_random, y_test, n)))
Recall @ (1, 10): 0.0937632
Recall @ (2, 10): 0.194503
Recall @ (5, 10): 0.49297
Recall @ (10, 10): 1
Great, seems to work. Of course we don’t just want a random predictor. Another baseline that was discussed in the original paper is a tf-idf predictor. tf-idf stands for “term frequency — inverse document” frequency and it measures how important a word in a document is relative to the whole corpus. Without going into too much detail (you can find many tutorials about tf-idf on the web), documents that have similar content will have similar tf-idf vectors. Intuitively, if a context and a response have similar words they are more likely to be a correct pair. At least more likely than random. Many libraries out there (such as scikit-learn) come with built-in tf-idf functions, so it’s very easy to use. Let’s build a tf-idf predictor and see how well it performs.
class TFIDFPredictor:
def __init__(self):
self.vectorizer = TfidfVectorizer()
def train(self, data):
self.vectorizer.fit(np.append(data.Context.values,data.Utterance.values))
def predict(self, context, utterances):
# Convert context and utterances into tfidf vector
vector_context = self.vectorizer.transform([context])
vector_doc = self.vectorizer.transform(utterances)
# The dot product measures the similarity of the resulting vectors
result = np.dot(vector_doc, vector_context.T).todense()
result = np.asarray(result).flatten()
# Sort by top results and return the indices in descending order
return np.argsort(result, axis=0)[::-1]
# Evaluate TFIDF predictor
pred = TFIDFPredictor()
pred.train(train_df)
y = [pred.predict(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))]
for n in [1, 2, 5, 10]:
print(“Recall @ ({}, 10): {:g}”.format(n, evaluate_recall(y, y_test, n)))
Recall @ (1, 10): 0.495032
Recall @ (2, 10): 0.596882
Recall @ (5, 10): 0.766121
Recall @ (10, 10): 1
We can see that the tf-idf model performs significantly better than the random model. It’s far from perfect though. The assumptions we made aren’t that great. First of all, a response doesn’t necessarily need to be similar to the context to be correct. Secondly, tf-idf ignores word order, which can be an important signal. With a Neural Network model we can do a bit better.
Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data
List of Best AI Cheat Sheets
becominghuman.ai
DUAL ENCODER LSTM
The Deep Learning model we will build in this post is called a Dual Encoder LSTM network. This type of network is just one of many we could apply to this problem and it’s not necessarily the best one. You can come up with all kinds of Deep Learning architectures that haven’t been tried yet — it’s an active research area. For example, the seq2seq model often used in Machine Translation would probably do well on this task. The reason we are going for the Dual Encoder is because it has been reported to give decent performance on this data set. This means we know what to expect and can be sure that our implementation is correct. Applying other models to this problem would be an interesting project.
The Dual Encoder LSTM we’ll build looks like this (paper):

Duel Encoder
It roughly works as follows:
Both the context and the response text are split by words, and each word is embedded into a vector. The word embeddings are initialized with Stanford’s GloVe vectors and are fine-tuned during training (Side note: This is optional and not shown in the picture. I found that initializing the word embeddings with GloVe did not make a big difference in terms of model performance).
Both the embedded context and response are fed into the same Recurrent Neural Network word-by-word. The RNN generates a vector representation that, loosely speaking, captures the “meaning” of the context and response (c and r in the picture). We can choose how large these vectors should be, but let’s say we pick 256 dimensions.
We multiply c with a matrix M to “predict” a response r’. If c is a 256-dimensional vector, then M is a 256×256 dimensional matrix, and the result is another 256-dimensional vector, which we can interpret as a generated response. The matrix M is learned during training.
We measure the similarity of the predicted response r’ and the actual response r by taking the dot product of these two vectors. A large dot product means the vectors are similar and that the response should receive a high score. We then apply a sigmoid function to convert that score into a probability. Note that steps 3 and 4 are combined in the figure.
To train the network, we also need a loss (cost) function. We’ll use the binary cross-entropy loss common for classification problems. Let’s call our true label for a context-response pair y. This can be either 1 (actual response) or 0 (incorrect response). Let’s call our predicted probability from 4. above y’. Then, the cross entropy loss is calculated as L= −y * ln(y’) − (1 − y) * ln(1−y’). The intuition behind this formula is simple. If y=1 we are left with L = -ln(y’), which penalizes a prediction far away from 1, and if y=0 we are left with L= −ln(1−y’), which penalizes a prediction far away from 0.
For our implementation we’ll use a combination of numpy, pandas, Tensorflow and TF Learn (a combination of high-level convenience functions for Tensorflow).
DATA PREPROCESSING
The dataset originally comes in CSV format. We could work directly with CSVs, but it’s better to convert our data into Tensorflow’s proprietary Example format. (Quick side note: There’s alsotf.SequenceExample but it doesn’t seem to be supported by tf.learn yet). The main benefit of this format is that it allows us to load tensors directly from the input files and let Tensorflow handle all the shuffling, batching and queuing of inputs. As part of the preprocessing we also create a vocabulary. This means we map each word to an integer number, e.g. “cat” may become 2631. The TFRecord files we will generate store these integer numbers instead of the word strings. We will also save the vocabulary so that we can map back from integers to words later on.
Each Example contains the following fields:
context: A sequence of word ids representing the context text, e.g. [231, 2190, 737, 0, 912]
context_len: The length of the context, e.g. 5 for the above example
utterance A sequence of word ids representing the utterance (response)
utterance_len: The length of the utterance
label: Only in the training data. 0 or 1.
distractor_[N]: Only in the test/validation data. N ranges from 0 to 8. A sequence of word ids representing the distractor utterance.
distractor_[N]_len: Only in the test/validation data. N ranges from 0 to 8. The length of the utterance.
The preprocessing is done by the prepare_data.py Python script, which generates 3 files:train.tfrecords, validation.tfrecords and test.tfrecords. You can run the script yourself or download the data files here.
CREATING AN INPUT FUNCTION
In order to use Tensorflow’s built-in support for training and evaluation we need to create an input function — a function that returns batches of our input data. In fact, because our training and test data have different formats, we need different input functions for them. The input function should return a batch of features and labels (if available). Something along the lines of:
def input_fn():
# TODO Load and preprocess data here
return batched_features, labels
Because we need different input functions during training and evaluation and because we hate code duplication we create a wrapper called create_input_fn that creates an input function for the appropriate mode. It also takes a few other parameters. Here’s the definition we’re using:
def create_input_fn(mode, input_files, batch_size, num_epochs=None):
def input_fn():
# TODO Load and preprocess data here
return batched_features, labels
return input_fn
The complete code can be found in udc_inputs.py. On a high level, the function does the following:
Create a feature definition that describes the fields in our Example file
Read records from the input_files with tf.TFRecordReader
Parse the records according to the feature definition
Extract the training labels
Batch multiple examples and training labels
Return the batched examples and training labels
DEFINING EVALUATION METRICS
We already mentioned that we want to use the recall@k metric to evaluate our model. Luckily, Tensorflow already comes with many standard evaluation metrics that we can use, including recall@k. To use these metrics we need to create a dictionary that maps from a metric name to a function that takes the predictions and label as arguments:
def create_evaluation_metrics():
eval_metrics = {}
for k in [1, 2, 5, 10]:
eval_metrics[“recall_at_%d” % k] = functools.partial(
tf.contrib.metrics.streaming_sparse_recall_at_k,
k=k)
return eval_metrics
Above, we use functools.partial to convert a function that takes 3 arguments to one that only takes 2 arguments. Don’t let the name streaming_sparse_recall_at_k confuse you. Streaming just means that the metric is accumulated over multiple batches, and sparse refers to the format of our labels.
This brings is to an important point: What exactly is the format of our predictions during evaluation? During training, we predict the probability of the example being correct. But during evaluation our goal is to score the utterance and 9 distractors and pick the best one — we don’t simply predict correct/incorrect. This means that during evaluation each example should result in a vector of 10 scores, e.g. [0.34, 0.11, 0.22, 0.45, 0.01, 0.02, 0.03, 0.08, 0.33, 0.11], where the scores correspond to the true response and the 9 distractors respectively. Each utterance is scored independently, so the probabilities don’t need to add up to 1. Because the true response is always element 0 in array, the label for each example is 0. The example above would be counted as classified incorrectly by recall@1because the third distractor got a probability of 0.45 while the true response only got 0.34. It would be scored as correct by recall@2 however.

Ai Models
BOILERPLATE TRAINING CODE
Before writing the actual neural network code I like to write the boilerplate code for training and evaluating the model. That’s because, as long as you adhere to the right interfaces, it’s easy to swap out what kind of network you are using. Let’s assume we have a model functionmodel_fn that takes as inputs our batched features, labels and mode (train or evaluation) and returns the predictions. Then we can write general-purpose code to train our model as follows:
estimator = tf.contrib.learn.Estimator(
model_fn=model_fn,
model_dir=MODEL_DIR,
config=tf.contrib.learn.RunConfig())
input_fn_train = udc_inputs.create_input_fn(
mode=tf.contrib.learn.ModeKeys.TRAIN,
input_files=[TRAIN_FILE],
batch_size=hparams.batch_size)
input_fn_eval = udc_inputs.create_input_fn(
mode=tf.contrib.learn.ModeKeys.EVAL,
input_files=[VALIDATION_FILE],
batch_size=hparams.eval_batch_size,
num_epochs=1)
eval_metrics = udc_metrics.create_evaluation_metrics()
# We need to subclass theis manually for now. The next TF version will
# have support ValidationMonitors with metrics built-in.
# It’s already on the master branch.
class EvaluationMonitor(tf.contrib.learn.monitors.EveryN):
def every_n_step_end(self, step, outputs):
self._estimator.evaluate(
input_fn=input_fn_eval,
metrics=eval_metrics,
steps=None)
eval_monitor = EvaluationMonitor(every_n_steps=FLAGS.eval_every)
estimator.fit(input_fn=input_fn_train, steps=None, monitors=[eval_monitor])
Here we create an estimator for our model_fn, two input functions for training and evaluation data, and our evaluation metrics dictionary. We also define a monitor that evaluates our model every FLAGS.eval_every steps during training. Finally, we train the model. The training runs indefinitely, but Tensorflow automatically saves checkpoint files in MODEL_DIR, so you can stop the training at any time. A more fancy technique would be to use early stopping, which means you automatically stop training when a validation set metric stops improving (i.e. you are starting to overfit). You can see the full code in udc_train.py.
Two things I want to mention briefly is the usage of FLAGS. This is a way to give command line parameters to the program (similar to Python’s argparse). hparams is a custom object we create in hparams.py that holds hyperparameters, nobs we can tweak, of our model. This hparams object is given to the model when we instantiate it.
CREATING THE MODEL
Now that we have set up the boilerplate code around inputs, parsing, evaluation and training it’s time to write code for our Dual LSTM neural network. Because we have different formats of training and evaluation data I’ve written a create_model_fn wrapper that takes care of bringing the data into the right format for us. It takes a model_impl argument, which is a function that actually makes predictions. In our case it’s the Dual Encoder LSTM we described above, but we could easily swap it out for some other neural network. Let’s see what that looks like:
def dual_encoder_model(
hparams,
mode,
context,
context_len,
utterance,
utterance_len,
targets):
# Initialize embedidngs randomly or with pre-trained vectors if available
embeddings_W = get_embeddings(hparams)
# Embed the context and the utterance
context_embedded = tf.nn.embedding_lookup(
embeddings_W, context, name=”embed_context”)
utterance_embedded = tf.nn.embedding_lookup(
embeddings_W, utterance, name=”embed_utterance”)
# Build the RNN
with tf.variable_scope(“rnn”) as vs:
# We use an LSTM Cell
cell = tf.nn.rnn_cell.LSTMCell(
hparams.rnn_dim,
forget_bias=2.0,
use_peepholes=True,
state_is_tuple=True)
# Run the utterance and context through the RNN
rnn_outputs, rnn_states = tf.nn.dynamic_rnn(
cell,
tf.concat(0, [context_embedded, utterance_embedded]),
sequence_length=tf.concat(0, [context_len, utterance_len]),
dtype=tf.float32)
encoding_context, encoding_utterance = tf.split(0, 2, rnn_states.h)
with tf.variable_scope(“prediction”) as vs:
M = tf.get_variable(“M”,
shape=[hparams.rnn_dim, hparams.rnn_dim],
initializer=tf.truncated_normal_initializer())
# “Predict” a response: c * M
generated_response = tf.matmul(encoding_context, M)
generated_response = tf.expand_dims(generated_response, 2)
encoding_utterance = tf.expand_dims(encoding_utterance, 2)
# Dot product between generated response and actual response
# (c * M) * r
logits = tf.batch_matmul(generated_response, encoding_utterance, True)
logits = tf.squeeze(logits, [2])
# Apply sigmoid to convert logits to probabilities
probs = tf.sigmoid(logits)
# Calculate the binary cross-entropy loss
losses = tf.nn.sigmoid_cross_entropy_with_logits(logits, tf.to_float(targets))
# Mean loss across the batch of examples
mean_loss = tf.reduce_mean(losses, name=”mean_loss”)
return probs, mean_loss
The full code is in dual_encoder.py. Given this, we can now instantiate our model function in the main routine in udc_train.py that we defined earlier.
model_fn = udc_model.create_model_fn(
hparams=hparams,
model_impl=dual_encoder_model)
That’s it! We can now run python udc_train.py and it should start training our networks, occasionally evaluating recall on our validation data (you can choose how often you want to evaluate using the — eval_every switch). To get a complete list of all available command line flags that we defined using tf.flags and hparams you can run python udc_train.py — help.
INFO:tensorflow:training step 20200, loss = 0.36895 (0.330 sec/batch).
INFO:tensorflow:Step 20201: mean_loss:0 = 0.385877
INFO:tensorflow:training step 20300, loss = 0.25251 (0.338 sec/batch).
INFO:tensorflow:Step 20301: mean_loss:0 = 0.405653
…
INFO:tensorflow:Results after 270 steps (0.248 sec/batch): recall_at_1 = 0.507581018519, recall_at_2 = 0.689699074074, recall_at_5 = 0.913020833333, recall_at_10 = 1.0, loss = 0.5383
…
EVALUATING THE MODEL
After you’ve trained the model you can evaluate it on the test set using python udc_test.py — model_dir=$MODEL_DIR_FROM_TRAINING, e.g. python udc_test.py — model_dir=~/github/chatbot-retrieval/runs/1467389151. This will run the recall@k evaluation metrics on the test set instead of the validation set. Note that you must call udc_test.py with the same parameters you used during training. So, if you trained with — embedding_size=128 you need to call the test script with the same.
After training for about 20,000 steps (around an hour on a fast GPU) our model gets the following results on the test set:
recall_at_1 = 0.507581018519
recall_at_2 = 0.689699074074
recall_at_5 = 0.913020833333
While recall@1 is close to our TFIDF model, recall@2 and recall@5 are significantly better, suggesting that our neural network assigns higher scores to the correct answers. The original paper reported 0.55, 0.72 and 0.92 for recall@1, recall@2, and recall@5 respectively, but I haven’t been able to reproduce scores quite as high. Perhaps additional data preprocessing or hyperparameter optimization may bump scores up a bit more.
MAKING PREDICTIONS
You can modify and run udc_predict.py to get probability scores for unseen data. For example python udc_predict.py — model_dir=./runs/1467576365/ outputs:
Context: Example context
Response 1: 0.44806
Response 2: 0.481638
You could imagine feeding in 100 potential responses to a context and then picking the one with the highest score.
CONCLUSION
In this post we’ve implemented a retrieval-based neural network model that can assign scores to potential responses given a conversation context. There is still a lot of room for improvement, however. One can imagine that other neural networks do better on this task than a dual LSTM encoder. There is also a lot of room for hyperparameter optimization, or improvements to the preprocessing step. The Code and data for this tutorial is on Github, so check it out.
Resources:
dennybritz/chatbot-retrieval
chatbot-retrieval - Dual LSTM Encoder for Dialog Response Generation
github.com
Denny’s Blogs: http://blog.dennybritz.com/ & http://www.wildml.com/
Mark Clark: https://www.linkedin.com/in/markwclark
Final Word
I hope you have found this Condensed NLP Guide Helpful. I wanted to publish a longer version (imagine if this was 5x longer) however I don’t want to scare the readers away.
As someone who develops the front end of bots (user experience, personality, flow, etc) I find it extremely helpful to the understand the stack, know the technological pros and cons and so to be able to effectively design around NLP/NLU limitations. Ultimately a lot of the issues bots face today (eg: context) can be designed around, effectively.
If you have any suggestions on regarding this article and how it can be improved, feel free to drop me a line.
Let’s Hack Chatbots Together
Creator of 10+ bots, including Smart Notes Bot. Founder of Chatbot’s Life, where we help companies create great chatbots and share our insights along the way.
Want to Talk Bots? Best way to chat directly and see my latest projects is via my Personal Bot: Stefan’s Bot.
Chatbot Projects
Currently, I’m consulting a number of companies on their chatbot projects. To get feedback on your Chatbot project or to Start a Chatbot Project, contact me."
76,"What We Learned Designing a Chatbot for Banking
The truth about online banking interfaces is that they are usually terribly complex. Chatbots make it easy.",chatbot,https://chatbotsmagazine.com/what-we-learned-designing-a-chatbot-for-banking-2dd2c51d7c2c,"For the past year we were working on a concept of conversational interface for an online banking system that we called K2 Bank. Check out the video below with a prototype of it:

Here I want to talk about some things that we have learned along the way while designing it, and I hope it can inspire you and help you working with chatbots. You can find more about our solution in this article.
The Future of Digital Banking
K2 agency presents a new way of personal banking: K2 Bank powered by Stanusch Technologies.
medium.com
(Also please check out our project on Behance.)
Conversational UIs Are a Great Way to Simplify the Experience
The truth about online banking interfaces is that they are usually terribly complex. And banks are often thinking: the more features we have, the better. But from our experience (and we have a lot of experience working on banking systems), 99% of tasks users wants to carry out using the online banking systems are:
check my balance
check my recent history of transactions
make a simple money transfer
We wanted to optimize for these three most frequent scenarios. So you can find your accounts and recent history available from the main screen of K2 Bank, but nothing more. All other features are accessible by asking a bot.
For rarely used commands it can be actually easier to put them into words than to find them navigating complex GUIs (for example: “cancel my credit card” — is it under “cards”, “settings”, “contact”, or somewhere else?)

Discoverability and Shortcuts Are Very Important to Conversational UIs
In theory Natural Language Processing engine should understand everything the user is trying to say in their natural language. And this technology is now pretty advanced.
But new users need to understand the scope of what they can ask a BankBot. (This can be accomplished in part through a thoughtful on-boarding process.) On the other hand the power users would want to use shortest commands and shortcuts for the most frequent tasks.
Both discoverability and speed can be improved by:
a) Autocomplete — at most you need to type two or three letters and the system will present you with a list of options matching your query — past recipients’ names or commands.

b) A menu. You can always click on the “burger” icon near the input field to bring a menu with a list of the most important commands and just select one of them. This works similar to typing “/” (slash) in Slack.

You Will Need Some UI Device to Present Larger Lists of Information
Conversational interfaces are great at presenting small chunks of relevant information, but sometimes we need to display larger sets of data, which users want to browse.
For more on UI, Read “The Bot Playbook”
Do you remember an ancient game by iD Software called Quake? Of course you do! It was the first FPP shooter with true 3D graphics, and it was groundbreaking. And Quake also had one cool UI device called “Quake Console”. If you pressed the tilde key (“~”) it opened a command line interface sliding down from the top of the screen. Then you could type your commands into the Quake Console, for example: “changelevel <map>”.

Quake Console
When we were designing the K2 Bank interface we also asked ourselves a question: do we really need to store the history of all conversations with the BankBot?
We have adapted this concept into our design, splitting the screen into two halves at certain moments. But for us the command line interface is the main way of interaction, and the “console” is a way to present a long lists of items like history, products or list of applications. It is visible, in example, on the home screen, where the bottom half of it is a command line interface with BankBot welcome message, with the top half being a recent history of transactions. You can expand the “console” and scroll up through your history or ignore it and just talk to your BankBot.

When we were designing the K2 Bank interface we also asked ourselves a question: do we really need to store the history of all conversations with the BankBot? The answer was: not really, who cares what you have asked a bot a year ago? We need to store history of real transactions, and conversation history could only be confined to a single session.
Processes with Lots of Options Are Hard to Adapt for Conversational UIs
This part is really important. A good conversational user interface is not a simple adaptation of your forms with a bot asking a question for every possible form field, most of them optional. It will be a nightmare and a waste of time.
With banking, even in a case of a simple money transfer form, there are a lot of options you probably would not want to touch in most cases, but sometimes you have to, like date, currency, address, or even a title. So we have to come up with smart defaults that you can change if you really need to, but usually you don’t need to edit them at all.
A hybrid interface works best: part conversational, part point and click.
Featured CBM: Banking with Artificial Intelligence
We Need to Provide Users with a Sense that They Are in Charge of the System
We found out that in interaction with bots it is easy to lose the feeling of being in full control of everything. We needed to provide stop gaps for the users to confirm or cancel certain processes, so they would be fully aware that they are in charge. This is especially important when we are dealing with money. So at some moments the input line is replaced with “Accept”/”Cancel” buttons and no other actions are possible.

Some Personality is Nice, but Don’t Over Do it
We believe banking is generally lacking humanity and it is perceived as very “stiff” business. You have to ask: why so serious?
Sure, money is a serious issue, but some personality, friendliness and humor can help. We designed a robot character as a representation of BankBot and living “logo” for K2 Bank: a cute, friendly little helper. We wanted very simple physical shape: a sphere or some kind of a cylinder with a big face (not a screen), made from plastic material like a toy. Not very human-like, because that will take us into the “uncanny valley” territory. It needed to work in large and small sizes (e.g. on mobile). Three dimensional rendering is a nice contrast against otherwise very simple and flat UI.

BankBot uses anti-gravity propulsion to move around!
After you sign in to your account the robot becomes a part of the logo, and is at the periphery of yours awareness. In the main stream of communication BankBot is represented by a flat vector icon. We don’t want to be Mr. Clippy!
We also found out that the response times are very important for how the bot is perceived. If it is answering too quickly it ruins an illusion of talking to a human-like being. If it is too slow the bot is perceived as not too smart.

Some early sketches of BankBot in a form of a sphere
The Real Power of Conversational Interfaces Is the Bots Acting Proactive
You can talk to chatbots in natural language, and that’s awesome, but they can also talk to you on their own when they have something important to tell you. Proactive behavior is what makes a bot an intelligent assistant.
Your personal banking robo-assistant can:
remind you about important payments
periodically inform you about the state of your budget
suggest how to save money
inform you about financial products that are best fitted for you
provide an investment portfolio update
deliver important, time sensitive notifications.

We shouldn’t message users too often — only when we know that the update is important to them, we need their decision or action, or it is at their best interest. Treat the users with respect and try to be helpful, and it can be a beginning of a great human-robot friendship :)
For more, check out: Conversational Banking: From Branches to Bots
Want to Know More? Talk to Us!

K2 Internet is a leading digital product design and communications agency in Poland. We develop digital services, apps and websites with a strong focus on user experience. We have a long-time experience partnering with financial institutions — in the last 10 years we helped to envision, design and develop over 10 transactional systems for the biggest banks in Poland."
77,"How I designed, developed, and deployed a chatbot entirely in the cloud",chatbot,https://medium.com/free-code-camp/how-i-designed-developed-and-deployed-a-chatbot-entirely-in-the-cloud-a60614eb94f2,"It all started with a YouTube video I recorded few months back. In it, I talked about the importance of deliberate revision. This helps you retain things in your mind for a longer period of time, and gives you techniques to revise important projects. If you haven’t, please watch it here.
In the video, I talked about how frequently you should revise, what the heck is the forgetting curve, and why you should care.
I wanted to give you guys a proper tool, in addition to the video, so that you can revise better. Being a developer, my natural response was “Let’s write an app!”
But if you’ve read my other article about why native apps are doomed, you know I was a bit reluctant to write a standalone app for this. I took a step back and analyzed the situation. I needed a back-end to store users’ data and a front-end to collect and show that data.
I wanted the user on-boarding to be as frictionless as possible. Forcing users to download a new app is hard. If I built a chatbot, it would serve that purpose and I wouldn’t have to convince anyone to download anything. I would also save some time since I wouldn’t have to build a standalone client app and go through app stores’ processes.
You can try the bot I built here.
Let’s cut to the chase and talk about the process. Read on to see how my chatbot went from an idea to a fully working product — entirely using cloud-based tools.
Quest #1: AI and NLP
Natural Language Processing (NLP) and AI are integral parts of any smart chatbot. So, I knew from the start that I’d require AI and NLP to make my bot “smart” and something you could talk to. It should also understand what you are asking it to do. I come from a full stack development background and I have zero experience with Machine Learning, AI or NLP. But for this bot, all of these things were necessities.
Being a tech enthusiast, I always keep tabs on what tools and libraries the Biggies are launching. I was aware of Wit.ai, an online API, released by Facebook for enabling NLP in your apps and bots. I played around with it for a while but found it particularly hard.
I quickly searched for other alternatives and found Api.ai. I played around with it and found it more developer-friendly, so I went with it.
Here’s what exactly you do with these ai APIs:
First, you write down a probable conversation which can happen between your bot and a person.
Based on that conversation, you create an exclusive flow-diagram (or something like that) which handles all the outcomes of the conversation.
You program the Api.ai agent to handle all the pre-defined outcomes using its dashboard. It’s simple enough — once you learn it.
Note: You can call on custom logic, which resides in your secure back-end, if API.ai’s built-in handlers can’t handle your use case. In the case of Revisebot, I was storing each user’s learning history and calculating what topics the user should revise next. This required custom calculations and persistence mechanisms.

Revisebot’s NLP using Api.ai
Api.ai also offers some pre-built agents, such as small talk and weather agents, which can answer users’ queries about weather and other topics. These are plug-n-play things which you can readily use in your chatbots.
Since Revisebot needed to handle custom use cases, I had to write some code. Time to churn out some JavaScript/Node.js code. Yay!
Quest #2: Cloud Hosting
I am a long time user of Digital Ocean, but it costs around $6/month at a minimum. Since I wasn’t hoping to make money off of Revisebot, hosting it on Digital Ocean didn’t make sense. I’d be losing money on a monthly basis.
I needed a free cloud host for this project. I knew Firebase offered free hosting (as I’ve used it in the past). I have also used Open Shift as well, for other projects (mostly Laravel). But I thought it would be a great idea to Google some other alternatives, at least for the sake of Node.js.
That’s when I came across Heroku and its free plan.
In no time, I learned that Heroku’s Node.js integration is awesome. So I read their docs and quickly spun up a Node.js app on their free dynamo. It was enough for my needs. Its only limitation was that it sleeps after a while, so the first API call might fail while the dynamo is waking up from sleep. But I adapted my chatbot to respond to such scenarios.
Quest #3: MongoDB in the cloud
I had been contemplating learning some MongoDB. So I decided to use MongoDB as the database for my chatbot. A chat app is a good use case for MongoDB’s document-based storage system.
My plan ran into a little roadblock when I discovered that Heroku does not offer MongoDB integration for free. No worries — I went back to my friend Google and searched for a “Free MongoDB cloud.”
That’s how I came to know about mLabs, which offers free MongoDB instances in the cloud.
Their free plan is not recommended for production ready apps, but that’s OK. I’m gonna run my chatbot on the free plan anyway.
Quest #4: Cloud IDE
My plan was to code the entire thing up in whatever free time I had after my full time job. Because of this, I needed the flexibility of coding from anywhere. So my developer workspace needed to reside in the cloud, which I could load up from anywhere I had internet.
I’ve been using cloud-based IDEs for quite a while and the experience is mixed. Nitrous.io was awesome but they shut it down. :( After trying some online IDEs like cloud9 and codeanywhere, the one that I found most stable and developer-friendly was Codenvy. It offers workspaces which you can create or destroy at your own will.
So I created a new Ubuntu-based workspace in Codenvy and installed node, npm, git and curl right away. Codenvy offers a terminal as well, so Linux users feel right at home. My developer workspace in the cloud was all set.
Next, I git-cloned my project’s repository from Heroku, and set up the DB integration with mLab’s MongoDB instance using .env files. As you can see in the screenshot below, blooming-escarpment-58368 was my Heroku Node.js project.

Coding revisebot in Codenvy.io
Quest #5: Integrating the chatbot with Social Media APIs
The chatbot was supposed to work with Facebook Messenger and Slack. I would have to learn the developer APIs for both platforms and set up my development machine for testing the API calls. Luckily, Api.ai also offers easy one-click integration with most of the social media platforms. You just have to follow their documentation to bring your chatbot to the specified platform.

Social Media Integration for Revisebot
As you can see in the screenshot above, I’ve integrated Revisebot with Facebook Messenger and Slack, as of now. This step won’t take long, believe me.
Using these tools, I was able to write, test and deploy the entire ecosystem of my chatbot (the DB, the application layer, the front-end and the AI agent) to react to users’ queries.
But there were still some pieces left in order to make Revisebot a complete, finished product.
Quest #6: Source Code Management
Although I was the only developer working on this chatbot, I needed to store the code somewhere safe. Git was an obvious choice for source code and version control management, but GitHub does not offer a free, private repository. Revisebot was not supposed to be an open-source venture, so I could not host the source code there. Additionally, as I was not using a local development machine, I couldn’t use any local git repo to store my code on.
Back in the day, I played around with bitbucket.org. I had some idea that they offered a free private repository, but wasn’t sure if they still offered any such plans. I went to their site and found that they did. The rest is pretty self-explanatory.
Quest #7: Graphic Assets
Design and graphics sit at the core of any digital product. I needed a logo, background images, and cover images for my chatbot’s Facebook page, Slack app store listing, and homepage.
I am not a designer by any means, so I needed some help. I had to choose the color palette and icons, mix shapes together to create a logo, and more.
Luckily, there is a helpful tool for this called Canva.
It offers ready-made design templates for social media, YouTube, and logos which you can customize according to your needs. I created Revisebot’s logo, entirely in Canva, using free shapes and some creativity. I think I did fine.

Revisebot’s Logo, built using Canva.com
I also used some of their free templates to create other visual assets for Revisebot like a Facebook cover image.
So that’s how I coded and deployed a fully working chatbot, which can help you schedule your revision, entirely in the cloud.
It costs me exactly $0 to run this service.
Let me know if you have any questions regarding my project.
*No local machines were engaged in the making of this chatbot.
If you liked this post, kindly give me some claps and follow me for more posts like this one. You should also subscribe to my YouTube channel, if you like developing digital things."
78,How to nail a great chatbot experience,chatbot,https://thinkgrowth.org/how-to-nail-a-great-chatbot-experience-7bbfd98026d6,"Even though it felt like the entire world was building a next generation experience using chat bots in 2017, the reality is that we’re at the beginning of a slow-burn revolution that’s going to take decades.
Chat-bots are here to stay, but they aren’t the overnight paradigm shift some thought they would be for one reason: they’re hard to pull off. Chat-bots are revolutionary because they feel like a more human way to interact with our devices, but that’s what makes it so easy to get wrong.
Not only are there massive technical challenges — such as understanding user intent from free-form text — it’s a whole new paradigm for design: what do you do when there’s very little interface? For designers working on chat, text itself is now one of the only canvases they have, making it the most powerful tool in the modern design kit.
Over the last year I've worked directly on a handful of chat-first interfaces with big brands personally, and wanted to look at what makes a great chat experience, from beginning to end.
It's incredibly easy to build a bot but not something that people actually want to willingly use — it all comes down to the way the user experiences it and whether or not it’s getting in the way of actually getting the job done.
KLM and our new robot ticketing overlords
One of the first big brands in the world to wholeheartedly embrace chatbots was KLM, the national Dutch airline, which is often hailed for being an early adopter to new technology.
The company has one of the best chatbots available, and it has a good reason for caring so much about it: the company employs more than 230 dedicated agents to reply on social media.
With more than 100,000 mentions publicly every week, the sheer impact of being able to quickly solve simple questions with the use of artificial intelligence and chatbots is clear.
KLM has invested heavily in both chatbots and A.I tools to solve messages as quickly and precisely as possible, but has spent a lot on developing marketing tools as well — to the point that you can book almost your entire flight via Facebook Messenger!
Not only is the KLM chatbot a fantastic thing to use, it actually seems easier than booking via the website, which can often be clumsy and confusing as you're trying to figure out which button will do what you want it to.
Here's what makes KLM's bot so good, and how other brands could learn.
Don't just assume a single intent

A common mistake I've seen from other companies that use chatbots is assuming that users who land on their bot will understand it — or have the same intentions.
This often leads to high failure rates as people just argue with the bot, which doesn't understand their request, or they close the conversation immediately.
KLM's bot understands this risk, so immediately offers the user a choice of where to go; is the query about support, booking a flight or something else?
Even if the other options end up with a human, this is a fantastic way to figure out where to route the user internally without any humans involved.
Handle ambiguous responses

If you choose Book Your Flight, which is what this bot is made for, KLM lets you type where you'd like to go.
This is basically every bot developer's worst nightmare, because users could say anything right now, and the bot is left to interpret it based on a very limited understanding of what could happen next.

Even being vague doesn’t break KLM’s bot
Even if you get the user to write something you’re expecting into the text box, most people tend to type something vaguer than you’d hope at this point — leaving it with you to figure out the specifics of their answer.
I ended up naturally typing New Zealand without the actual city I was planning to visit — and I expected the worst but found myself surprised: they'd thought of this scenario.
A good bot development project — particularly from the UX writing side — will consider all of the different weirdness that could eventuate here, and KLM did this right.
Not only did KLM ask for more specifics politely, they nailed combining the two separate data points to figure out what I meant, rather than forcing me to enter the full destination myself all over again.
Your words are everything
When you’re building a chatbot, your words are everything. They’re the beginning and end of your user’s experience with you, so you can’t afford any misinterpretations, dead ends or confusing phrasing.
I’ve written the UX copy for a number of chatbots, and your use of language should be the principle consideration before writing a single line of code. I noted a number of places that KLM uses great copy to guide the user, so let’s walk through them.
1.KLM sets expectations immediately by making it clear it’s a bot through the use of an emoji and in a friendly tone explaining its own limitations.

By doing this, the user already feels comfortable, but understands something might go wrong, so is far more willing to be patient because they know it’s not perfect yet.
2.KLM uses a smart, subtle trick to win points from users: repeating what the bot understands to be the correct query back to them before continuing.
Once you’ve figured out dates and destination, for example, KLM spells the search out, offering an opportunity to correct any mistakes. This may seem tedious, but there’s a great trick behind this.
Think of the times you’ve used Siri and how frustrating it is when she gets it wrong; if a computer is trying to be human and makes a mistake, the illusion is ruined immediately. By leveraging subtle language cues, KLM able to avoid the computer giving the wrong answer before it happens, and maintain the illusion that we’re getting everything right, even if it isn’t perfect.
3.KLM does a great job of helping you along the way with the wording it uses. When you’re given the chance to respond in free form, the chatbot guides you on how it expects you to respond.

Dates are particularly hard, because there’s so many formats humans can respond in
These types of cues avoid frustration on the user’s part and make it easier on the developer’s side: predictable input is the best input, and trying to figure out if 11/04/2018 is the 11th of April 2018, or 4th of November 2018 is impossible if you’ve got customers around the world.
It’s useful beyond the first interaction

A common area these bots fall over in is a lack of awareness of the user beyond that first interaction.
Often chatbots don’t understand who you actually are because they are unable to access data from existing backends.
KLM thought of this, and their bot is able to be useful beyond day one: you can choose to receive travel updates in one place and get your boarding pass without leaving it.
While it’s still fairly limited, this a great example of extending a conversational interface beyond just that first chat, and keeping users engaged long-term.
It’s harder than you think
When Facebook launched its chatbot platform, there was a deluge of different bots to try, but many of them were a frustrating experience. As it turned out, many brands jumped on the hype train without really considering the nuances involved in building a great experience.
KLM is a rare example of a chatbot done well. While it’s not perfect, it’s a fantastic way to search for flights that doesn’t feel more cumbersome to use than its app or website — which is the entire point in the first place.
If you’re considering building a chatbot, sweat the details and more than anything else, focus on the words you use. Your phrasing is the beginning and end of a great chatbot story, and it’s key to whether or not it succeeds."
79,"Hey Siri, Should I Give My Children Their Own Smart Speaker?
Voice-recognition technology isn’t going anywhere — might as well teach kids to use it responsibly",smart speaker,https://medium.com/s/little-minds-and-big-screens/hey-siri-should-i-give-my-children-their-own-smart-speaker-acf718196ee3,"It has happened: My children want their own smart speakers.
It’s not surprising. I’m a journalist covering tech and music, so I feel obliged to understand these devices that loom large in my work. There’s an Amazon Echo in my home office, an Apple HomePod in the kitchen, and a Google Home Mini in the bedroom.
My sons watch me talking to Alexa, Siri, and Google Assistant and quickly follow suit: If they’re not dashing into the office to request “Man’s Not Hot” before running away cackling, they’re enlisting the HomePod to do their math homework or pestering Google with inappropriate questions about bottoms.
The same phenomenon is happening in millions of homes around the world. Research firm Futuresource estimates that 26.6 million smart speakers shipped in 2017, with 89 percent of those sold to Americans and Brits. By January this year, one in six Americans owned a smart speaker. Another research company, Canalys, expects shipments to grow to 56.3 million in 2018 as the competition between Amazon, Google, and Apple heats up.
It’s been fascinating to watch my children’s reaction to smart speakers and their voice-control features. Like many adults — even tech-savvy ones — my mode of interaction is “Alexa?” (or “Hey Siri?” or “Hey Google?”), and then a pause before giving some voice command. It’s a pause that, I think, still signifies disbelief. Can I really talk to a device and have it understand me? Do I need to speak slowly and leave gaps?
There’s no pause when my children speak to a smart speaker. It’s a small difference, but noteworthy because of their confidence. Even toddlers can work out a touchscreen interface. They’re not thinking about the interaction with technology: They’re just doing it.
And now they want to do it with their own smart speakers. It’s not just because I’m a bit touchy about them talking to “my” speakers, whether it’s messing up my Spotify recommendations or teaching Google’s advertising algorithms that I have a keen and regularly voiced interest in “smelly bums” and “big poos.”
No, my sons want a device of their own that responds to their voice commands, and they only see the good in this. Which is where I come in as the big bad parent, fretting about privacy and personal data on their behalf. And, yes, this as a man who sleeps two feet from a Google-made listening device.
No Mattel, My Children Are Not “Assets”
Concerns around children and smart speakers have already blown up into a controversy. In 2017, Mattel unveiled a device called Aristotle that promised parents it would “use the most advanced AI-driven technology to make it easier for them to protect, develop, and nurture the most important asset in their home — their children.”
That included the ability to teach ABCs and 1-2-3s to toddlers, help slightly older children with their homework and play music or voice-controlled games, and teach foreign languages to tweens, among other features.
By the autumn, a 15,000-signature petition organized by two U.S. groups, the Campaign for a Commercial-Free Childhood (CCFC) and the Story of Stuff Project, was calling for Mattel to can the Aristotle before it even went on sale, while U.S. politicians were asking the company pointed questions about what data the speaker would be gathering on its young users and how it would be stored and shared.
“This new product has the potential to raise serious privacy concerns as Mattel can build an in-depth profile of children and their family. It appears that never before has a device had the capability to so intimately look into the life of a child,” the petition stated. Less than a fortnight later, Mattel canceled the device.
Since then, the CCFC has continued its efforts to call out devices and toys that may infringe children’s privacy, from smartwatches and robot companions to Facebook’s plans for a Messenger Kids app for families to use for chatting.
If we’re worried about smart devices made for children, shouldn’t we be even more worried about children using devices meant for adults? Yet both Amazon and Google have explicitly positioned their smart speakers as family friendly devices through the “skills” (Amazon) and “actions” (Google) that are these devices’ equivalent of smartphone apps.
In August 2017, Amazon launched a series of “kid skills”for its Alexa voice-assistant and Echo speakers, working with Sesame Street, Nickelodeon, and other companies to create them. Parental consent was part of the process of getting them to work; by October, the company was offering a $250,000 prize fund for developers who made the best examples.
Google followed suit in October 2017, releasing more than 50 children’s games, activities, and stories for its Google Assistant and Google Home, inviting kids to say, “OK Google. Play Mickey Mouse Adventure / Talk to What’s My Justice League Super Hero / Play Sports Illustrated Kids Trivia,” and other actions.
My, Um, Parenting Strategy
I’m fascinated by the potential of all this: family games (there’s already an educational board game where Alexa plays compère), interactive stories, and learning experiences, for example. There will be developers who have made it their mission to ignite children’s imaginations through voice-controlled experiences, and I’d love my boys to be able to benefit.
I also have a creeping sense that voice, as much as touch, will be a very important interface in the future. Isn’t it a good thing if my sons are learning how to interact with these kinds of devices now to prepare them for whatever machines they’ll be speaking to as adults? Preparation can also involve learning about how these devices’ privacy settings work and how the tech companies behind them deal with our personal data.
For this parent, that starts with learning those lessons myself. I recommend starting with this Wired article about what Echo and Google Home do with your voice data, or this Gizmodo piece about locking down privacy settings on your devices. CNET has a good primer on HomePod settings to change if you have one in a family setting.
Getting on top of the privacy settings, and then exploring what these smart speakers and their skills/actions can do with my children — rather than simply buying them a bedroom device and letting them figure it out — is the path I’ve chosen for now.
This is part of my slightly fuzzy parenting strategy where there are some things I don’t want my kids doing on their own just yet, but I want to talk about and experience them together ahead of that time to encourage healthy (and sometimes skeptical) attitudes toward technology on their part.
It’s the same with Snapchat and Instagram: apps my sons are still too young for, yet they know and are curious about them. Rather than a blanket don’t-even-mention-them ban on these apps, I’m showing them my feeds, letting them play with the filters and silly effects, and talking about how these apps work and some of the pitfalls to avoid — from judging your value by your number of likes to having snaps go public that you thought were private.
It takes time and effort, but that’s been my experience with pretty much everything when it comes to responsible parenting.
So, sorry, kids, no smart speaker by your bedside just yet. But this is a new technology we will continue to experience together as it develops and have the conversations that will hopefully help you make the right decisions about these devices as you grow older."
80,"Sonos’s Secret Weapon In The Smart Speaker Wars: Becoming A Platform
As tech titans release familiar-looking smart speakers, Sonos is hoping to set itself apart by opening its sound system up to developers–and giving users more freedom.",smart speaker,https://medium.com/fast-company/sonoss-secret-weapon-in-the-smart-speaker-wars-becoming-a-platform-2736fb05c225,"Barely two hours after the stage lights dimmed at Sonos’s product launch event in New York last week, the speaker company’s products had yet another new competitor. The Google Home Max, a high-quality speaker and just one of several smart gadgets announced later that day, may be Google’s voice-powered answer to Apple’s HomePod and Amazon’s new higher-quality Echo, but it also takes aim the wireless home audio turf long dominated by Sonos.
It’s not the first time the 15-year-old wireless home audio company has felt tech giants inch closer to its territory. The successful 2014 launch of the Amazon Echo–despite its lower-quality audio and lack of support for multi-room playback–seemed to catch Sonos (along with many others) by surprise. The following year, the company announced a round of layoffs, heightened its focus on paid streaming music services, and vowed to add voice control to its line of speakers.
Despite the proliferating threats (from much bigger companies, no less), Sonos doesn’t seem panicked. That’s because even as titans like Google and Amazon vie to become its newest competitors, those companies — along with many others — are also forging another relationship with Sonos: Partners. That might seem odd. But as the voice-controlled smart speaker war of 2017 kicks into high gear, Sonos is hoping to set itself apart with a new strategy: It’s becoming a platform. By the end of this year, Sonos says it expects to add 50 new developer partners (including many smart home products) to its platform. In 2018, it will open things up to all developers.
“You have no idea what people are going to build,” says Antoine Leblond, VP of software development at Sonos. “When Apple opened iOS, the first thing people made was the fart app.”
While it may be tempting to envision high-fidelity, multi-room fart gags (and an open developer platform certainly doesn’t rule out the possibility), the more practical scenarios likely involve developers connecting Sonos to other smart home devices and coming up with creative new ways to pipe music throughout one’s home. Perhaps your connected doorbell can be amplified in different rooms so you don’t miss a package delivery (and maybe it plays a song–would “Ring My Bell” be too much? — in lieu of the classic ding-dong sound). Maybe your speakers can sync with your lights, or a software developer might make a new alarm clock app that uses your Sonos speakers to fill your bedroom with sounds a bit more soothing than the iPhone marimba chime. For now, the details of the software development tools and options Sonos plans to offer are limited, but it’s easy to imagine some of the possibilities.
However things may eventually evolve, the early days of Sonos’s platform are focused primarily on enabling partnerships with music services and voice-control partners.
The Sonos One speaker announced by the company last week, which will ship with Alexa voice control built in, is its first foray into voice-controlled hardware. And for people who already own Sonos speakers, a new integration with Alexa devices like the Echo and Echo Dot brings Amazon’s voice control to the whole line of Sonos speakers as well. For now, Sonos’s Alexa integration is in beta, and thus has its limitations (it doesn’t yet support voice-controlled playback via Spotify, for instance, although the company says that’s coming very soon).

Sonos One [Photo: courtesy of Sonos]
The voice control ambitions of the Sonos One won’t stop with Alexa. Next year, Sonos will support Google Assistant voice control as well. So even as Sonos’s products and the tech giants’ smart speakers start to resemble each other more and more, Sonos is hoping to set itself apart by giving customers something bigger companies often won’t: Choices. In this case, that means a choice of one’s preferred voice control service (rather than being locked into one). But that’s clearly just the start.
The company is also adding AirPlay 2 support to its speakers, allowing customers to bypass Sonos’s proprietary app for queueing music on its system (and also stream a much wider selection of audio beyond music services). This might seem like a strange thing to tout in an age when Bluetooth speakers allow us to stream any sound from any Bluetooth-enabled device. But Sonos, with its own method of piping multi-room audio over Wi-Fi and a focus on high sound quality, has always turned up its nose at Bluetooth, much to the chagrin of some customers.
“We had a very closed experience, frankly,” says Leblond. “We’ve now moved to a world where now people want a lot of different mechanisms for being able to control their Sonos systems. It’s all about just fitting into people’s lives.”
For Sonos, it’s also about fending off competition that seems to grow–well, in this case, literally by the hour. The competitive rationale for giving users more choice and freedom is what drove Sonos to build a developer platform in the first place. Last year, it worked with Spotify to add the ability to stream music from directly within the Spotify app (just as it had done in 2013 with the much less popular Google Play Music app). With that experience fine-tuned and the underlying development platform complete, Sonos is now ready to expand native streaming support to more music apps, starting with Tidal, Pandora, and iHeartRadio. Again, it’s seemingly a no-brainer, but this isn’t how Sonos has worked traditionally: To control music on the system, you had use Sonos’s own app, which connects to multiple music services. The Sonos controller app is still alive and well (in fact, it just got a significant redesign, amid all the other product news announced Tuesday); but it’s quickly becoming just one of many ways to control Sonos.
The Allure Of Being Agnostic
By opening up its platform, Sonos is aiming to become a more agnostic and flexible sound system for the home — and, it hopes, offer a stark contrast from the similar-looking speakers now being peddled by deeper-pocketed companies. Enticed by those super-affordable Echo gadgets? Go for it, but you can only use Alexa (which is, unsurprisingly, optimized to work best with Amazon’s music service). You want a HomePod? Great. You’ll have to play by Apple’s (and Siri’s) rules and stomach its limitations. Eyeing a Google Assistant? Same deal (although, to its credit, Google has moved quickly to make its voice-control tech available to third-party hardware makers). With Sonos, you can pick and choose your music services, control mechanisms, and whatever else developers start cooking up. And more often than not, it will sound better.
This is by no means a recipe for slam-dunk success. After all, Alexa is a platform too, with thousands of integrations. And Apple’s consumer footprint and marketing prowess alone can make it a threatening competitor. But as the AI-enabled smart speaker space heats up over the next few years, Sonos is hoping to protect its turf using a unique blend of sonic quality, flexibility, and user freedom. It’s a compelling enough message for consumers. It’s just a question of how easily they’ll hear it."
81,When your smart speaker gives you the silent treatment [CARTOON]https://medium.com/the-coffeelicious/when-your-smart-speaker-gives-you-the-silent-treatment-cartoon-2a13af67f264,smart speaker,https://medium.com/the-coffeelicious/when-your-smart-speaker-gives-you-the-silent-treatment-cartoon-2a13af67f264,"Getting the silent treatment from your significant other is a drag. Getting it from your smart device, well, that’s a whole other level of annoyance."
82,"What is a radio, anyway? And what’s a smart speaker?",smart speaker,https://medium.com/@JamesCridland/what-is-a-radio-anyway-and-whats-a-smart-speaker-115e216f0d60,"I rather like this tweet, from the UK industry group Radiocentre, which does a very good job of explaining radio’s multiplatform nature:

It’s a succinctly made point; and highlights a failing in the English language — radio means, of course, three different things — a receiver, a technology, and a type of audio programming.
Radio is now on a variety of different platforms. Broadcast technology, like FM or DAB+; live streaming via IP; and on-demand too, in the form of podcasting and other things.
Most, if not all, radio research reflects this. The apparent unsophistication of a paper diary or a web form is actually quite a useful way of understanding how people listen: since it works with every form of radio, not just something delivered on a speaker.
Fewer people are buying radio receivers than ever before. Yet, in most markets, more people are listening to radio than ever before — because radio is not a platform, it’s a thing. I define it as “audio with a shared experience and a human connection”.
If we have trouble with defining a “radio”, what about a “smart speaker”, I wonder?
In Q1 2018, the Amazon Echo sold 2.5m devices worldwide; but Google Home sold 3.2m devices in the same time period. It’s the first time that Google has out-sold Amazon for smart speakers.
But that doesn’t tell the whole story, though: Google’s voice assistant works with over 5,000 devices. My JBL Link speaker that is on my deck has Google Assistant built-in; but it isn’t a Google Home. Where does it fit in the figures?
The GPS I use in the car has Google Assistant built-in (very good for sending text messages). The Bose headphones that I take travelling, or the smaller bud headphones I wear on the bus, both have Google Assistant on them as well — I walked over the William Jolly Bridge in Brisbane the other day, asking Google about my diary for tomorrow.
And of course, my mobile phone has Google Assistant in it, too. If it’s on the table, seemingly switched off, “OK, Google, what’s in the news?” will play me a short news bulletin. Or, “Hey, Google, ask Podnews for the latest” starts a conversation with my Podnews podcasting news website.
So, you can compare sales of Amazon Echo vs Google Home if you like; but that misses great swathes of the smart speaker ecosystem.
And you can define radio as a speaker in a box that picks up FM if you like; but that, too, misses much of what your audience already calls radio.
Your challenge is to ensure that your radio output sounds great — whatever ‘radio’ it is playing on."
83,"How music will help the smart speaker soar—and vice versa
These voice-activated, Internet-enabled speakers, most popular for streaming music, have become Pandora’s fastest-growing consumer electronics product. The U.S. could have about 60 million VA smart speaker users by 2021.",smart speaker,https://medium.com/@glennpeoples/how-music-will-help-the-smart-speaker-soar-and-vice-versa-8ef5b73ecfc4,"In fewer than three years, the smart speaker has gone from head-scratcher to head-turner. Along the way, this device has brought artificial intelligence and voice commands into millions of American homes. Usually called voice-activated (VA) smart speakers, they’re often found in the living room or kitchen, taking commands and providing answers like a talking search engine with a surprising sense of humor.
Most importantly to the music industry, these compact appliances excel at playing music. A new report by Edison Research shows 58 percent of VA smart speaker owners had listened to music for an average of 4 hours 34 minutes in the previous week, and 65 percent said they are listening to more music since buying a smart speaker. The latter metric is especially encouraging. Here in the streaming era, the music industry’s most important metric has become engagement. The more often streaming services can engage with listeners, the more revenue will flow to rights holders and artists.
Pandora’s experience confirms Edison’s findings. In the second quarter, 1.6 million active users of VA smart speakers fueled a 282-percent gain in listening time over the same period in 2016. The overall consumer electronics segment had an exceptional quarter, with active users growing 23 percent to 9.7 million, with one in 10 of those users new to Pandora.

Market penetration (% of population) of VA smart speakers will increase in the coming years. But keep in mind many households will have two or more of the devices.
Expect more growth in the coming years. eMarketer forecasts 58.9 million people in the U.S. will be VA smart speaker users by the end of 2020, a 178-percent increase from 15.6 million in 2016. (Separately, Gartner believes that by 2020 VA smart speakers will reach $2.1 billion in global sales and 3.3 percent of the world’s households.) Pandora will be ready. As Pandora chief financial officer (and interim chief executive officer) Naveen Chopra explained in the July 31st earnings call, Pandora believes it is “well-positioned and well-integrated” to capture engagement growth in smart speakers and other connected devices.
These speakers have the kind of wow factor that comes just a few times each decade. Free of using hands, you can ask the VA smart speaker for movie times, a weather forecast, the capital of Minnesota, or events on a calendar (they’re also called virtual personal assistant-enabled wireless speakers). As Amazon’s Echo, “What’s your favorite band?” and you might get the reply, “I like Daft Punk. It’s good time dance music.” They connect to other smart home products, too, allowing voice commands to perform functions like setting a thermostat or turning lights off and on.
Audio is the leading factor for buying a smart speaker. About 47 percent of Echo and Google Home buyers like their smart speaker because it plays music and books, according to a new report by Voicelabs, a data and analytics company that provides makers of voice-activated devices insights into listener behavior. Just 29 percent of buyers like their speakers for the control of smart home devices.
To listen to Pandora on a VA speaker, you can ask the device to play a particular Pandora station. The owner of an Echo might say, “Alexa, play my Paul Simon Radio station on Pandora.” (Other devices use similar phrases. Google Home uses the familiar prompt, “OK Google” when giving a command.) If you like a song, say, “Alexa, thumb this song” and Pandora will retain the feedback. To skip, say, “Alexa, skip this song.” If you want to know the artist or song title, say, “Alexa, what is this song?” And if you request an artist station that’s not in your collection, Alexa will offer to create it for you.
The quick ascent of the VA smart speaker is causing companies to re-think how a person engages with music in a voice-only environment. What happens when people aren’t scrolling through an MP3 collection, choosing from one of dozens or hundreds of playlists, or picking a CD to play on the home stereo? Recalling something specific can be tough. Pandora has a great option: when you can’t think of something to play, say “Alexa, play Thumbprint Radio,” the station built on all the tracks a person has thumbed (liked) since registering.
This little device has come a long way in a short amount of time.
Although it’s turning into a must-have gadget, the VA smart speaker got off to an unceremonious start. The press gave Amazon’s Echo, the first of its kind on the market, a tepid reception when it debuted — completely unannounced — in November, 2014. TechCrunch called the Echo “a tad baffling” and “a crazy speaker that talks to you” in its article. Even a veteran Forrester analyst believed Amazon could be “setting the table for a party that consumers aren’t ready to join.”
But consumers got it. Following the popularity of the Echo, now there are handfuls of voice-activated devices for the home. Google Home, Apple’s upcoming HomePod, and Microsoft’s Invoke connect to other smart devices — lights, thermostat, window shades, even the front door lock — in addition to playing music. A long line of voice-activated speakers are now on the market: Ivee Sleek, Voice Pod, Athom Homey, Omaker WoW, Eufy Genie, Jam Voice, and Lenovo Smart Assistant (powered by Harman/Kardon), to name just a handful.
Perhaps the most encouraging aspect of VA smart speakers is their youth. “It’s still very early,” said Larry Rosen, president of Edison Research. Indeed, the product category has not even reached its second birthday, and each device will have numerous iterations and improvements. More consumers will buy devices when prices drop and awareness increases. Today’s owners are just the tip of the spear, so to speak. “In general, as you might imagine, they’re early adopters,” Rosen added.
Another encouraging finding came from a different Edison Research study: 57 percent of VA smart speakers subscribe to a music service, and 28 percent of all owners said their device led them to become a subscriber. This is a familiar dynamic. Just as the iPhone became a launching pad for music streaming services, the VA smart speaker can become a new gateway for streaming services to reach potential subscribers.
The smart speakers studied by Edison are not to be confused with the more familiar small, portable, Bluetooth-enabled speakers popularized by Beats by Dr. Dre, the now-defunct Jawbone, Bose, JBL, and many others. There are no “smarts” to those speakers, no ability to receive voice commands, perform functions or “speak” in a genial tone of voice.
Nor is smart speaker’s AI the technology that super-entrepreneur Elon Musk has called “a fundamental existential risk for human civilization” and famed theoretical physicist Stephen Hawking warned, “could spell the end of the human race.” It’s not on par with the evil HAL 9000 computer in the movie 2001 or the operating system Joaquin Phoenix dated in the movie Her — not yet, at least. Yes, today’s in-home speaker has “intelligence” beyond other household products. But no, these speakers aren’t going to steal American jobs or evolve intelligence that surpasses humans. They’ll be great for music, though.
Smart homes have been part of Americans’ consciousness for decades. In Star Trek, The Jetsons and Back to the Future II, Hollywood’s depiction of the future has foreseen the use of voice commands in the home (or spacecraft). Voice commands aren’t science fiction, they’re real, and they can help push open the door to tomorrow’s music industry."
84,"Smart Speakers are having their 2004 iPod moment
Ways to think about content strategy for Google Home, Amazon Echo, and Apple HomePod.",smart speaker,https://blog.pacific-content.com/smart-speakers-are-having-their-2004-ipod-moment-83df7c91bc46,"iPods → Podcasting
The year was 2004. The iPod had been out for 3 years. Loads of people were using it to play their music. And then… podcasting suddenly emerged onto the scene.
I remember hearing about Adam ‘The Podfather’ Curry and his podcast, ‘The Daily Source Code’ in the fall of 2004 and being intrigued. Most of the shows were very meta — they were all about podcasting, how it worked, how to do it, how to find ‘podsafe music,’ etc. I was listening to the creation of an entirely new medium that democratized the creation and distribution of audio content.
And what happens when, thanks to new technology, you no longer need a broadcast gatekeeper to fund and distribute your show idea? New forms of shows and content emerge. (For podcasting, it was the ‘two dudes in a basement talking for a LONG time’ format :-) )
Smart Speakers → ???
2018 feels like Smart Speaker version of 2004 for iPods. Technology has again emerged that is replacing another set of audio devices — smart speakers are often replacing traditional radios in the home. And much like the original iPod, the primary use of a smart speaker is… to play music.
Many wonder why podcasts are not thriving on smart speakers. My aim is to explain why podcasts (the way they are formatted currently) may not be right for what people want and need from a smart speaker.
Let’s start with the research. These are sales projections for smart speakers in 2018:

Canalys research on Smart Speaker adoption.
There are going to be close to 100 million of these devices in homes by the end of this year and there is not yet a killer app outside of playing music. And yet they are ALL audio devices — this is a problem in search of a solution. This is the 2018 equivalent of the iPod waiting for the podcast to be invented.
So apart from music, what ARE people using these new devices for currently?

Research from NPR and Edison’s Smart Audio Report 2018
There is nothing ground-breaking in here yet. Foreground listening(i.e. that requires your full ‘foreground’ attention) is short and utility-based. Background listening (i.e. that can play in the background while your full attention is focused on another task like writing emails) is traditional music and radio based. Most current podcasts demand your full ‘foreground’ attention, but they are not short and they are generally not utility-based.
So… there is a big opportunity for a new type of programming perfectly suited for the ways people use smart speakers.
It’s also important to look at when and why people use their smart speakers.

Research from NPR and Edison’s Smart Audio Report 2018
Seeing how smart speakers are used as part of daily routines, it becomes very clear why traditional podcasts are a challenge on smart speakers. Where would a 30–60 minute podcast fit? Everything until 7pm is short-form and utility-focused. Only after 7pm do games and children’s stories come into play. And after 9pm is the first sign of any adult-focused storytelling — interestingly enough through audiobooks and not through podcasts.
So what are the new opportunities? Audio content producers need to start thinking about routines and how to create content that maps to those routines.
In the morning, what are people looking for when they are first waking up, making breakfast, or leaving home for school or work?
In the evening, what types of content are suited to relaxing, preparing for the next day, getting ready for bed or falling asleep?
You need only look at the runaway success of The Daily from the New York Times to see how valuable becoming part of a morning routine can be. There are certainly other hits based on routines waiting to be developed.
Similar to routines, locations matter, too. A lot of current podcast listening is solitary activity that takes place with headphones or in cars. Smart speakers, though, are placed in rooms in your house. This is a fundamentally different context for listening. What type of audio appeals to groups of people instead of individuals? And how does the room of the house affect the type of content that would be valued?

Research from NPR and Edison’s Smart Audio Report 2018
There are new questions that audio content creators need to start thinking about if we want to find the killer apps for smart speakers. What types of content would work best in the kitchen? The living room? The bedroom? And god forbid, the bathroom?
Conclusions
Based on the data, it seems like there are several new content opportunities for smart speakers:
1. Routine-based content.
Content designed to become part of one of your routines, from a morning horoscope or sports summary to a ‘go to sleep’ meditation routine.
If you’re a brand, is there a morning or evening information update you can provide that people would value? (i.e. if you’re a financial institution, are you publishing a short finance/market update every morning?)
The key here is to be short, valuable, and either daily or weekly.
2. Concise, evergreen answers to common questions.
What are the questions that you want to be the answer to? Can you produce something better and more valuable than a Voice Assistant reading from Wikipedia?
It could be location specific queries: (‘How do I use an Instapot?’ in the kitchen or ‘What’s my BMI?’ in the bathroom.)
3. Interactive content for social environments or particular contexts.
Party-games or trivia games for families in living rooms.
Interactive cooking lessons in the kitchen.
Potty-training game for toddlers in the bathroom. 💩
4. Original longer-form background listening ‘stations’
Create a radio station format that doesn’t exist yet and make it specific for the location in the house — a cooking/foodie radio station for example.
Create a personalized radio station that allows users to insert elements from traditional radio like news, weather and traffic at the times of the morning they want them, mixed with their own Spotify or Apple Music playlists.
These are just a few starter ideas and I’m sure everyone reading this can up with dozens more.
What’s clear to me is that much like the iPod in 2004, we are in very early stages of the smart speaker revolution and there are huge opportunities for fresh new content formats that are designed for the contexts in which people use these devices.
What would you like to hear on your smart speaker that hasn’t been invented yet?"
85,"I’m starting to think this smart speaker thing is going to be big 🤔
And how listening to Scout FM seamlessly between your apps and Alexa is easy, and kinda magical.",smart speaker,https://notes.subcast.com/im-starting-to-think-this-smart-speaker-thing-is-going-to-be-big-9eab903f1e52,"Do you remember trying to explain to a flip phone user why they should get a smart phone? I distinctly remember trying to tell my brother how having Google maps on your phone means that you never have to get lost again. He said “I’m perfectly happy with what I have,” though he did capitulate a few years later. Fast forward 10 years — do you think he’d go back to that flip phone? No. No he wouldn’t.
Similarly, trying to explain to someone who doesn’t have a smart speaker why they should get one is usually futile, so I don’t push very hard right now. To be honest, they aren’t very smart yet, and have limited functionality. But like smart phones, they’ll grow up.
Why do I think that? First, Amazon is hiring thousands of engineers to work on the platform and there is a war going on between Amazon and Google. Second, we’re seeing firsthand in our user research how people are integrating Alexa into their lives in a meaningful way. Third, the latest SmartAudio report from NPR and Edison Research backs up our anecdotal research, and usually blows the minds of skeptics. Check this out:
One-in-six Americans (16%) now owns a voice-activated smart speaker. 7% of Americans received a smart speaker over the holidays.
After just three years on the market, consumer adoption of smart speakers is tracking slightly ahead of smartphone adoption a decade ago.
65% say that they wouldn’t want to go back to life without their smart speaker.
This is before smart speakers have made it all the way into your cars and into your headphones (though the assistants are on their way). They are only in your home or office, but already they are proving invaluable to the people who optimize their lives with them.

Alexa in every room!
If you have one of these smart speakers, then you are probably thinking about how cool you are right now. Way to be ahead of the curve! If that’s you, then congratulations.
If you have have the most popular smart speaker — one of the many variations of Amazon’s Alexa — then I have a really cool thing you should try. You can listen to 25 of our personalized podcast radio stations on Alexa in your bedroom, then on your phone on the way out the door, then on your phone in your car, and then back on Alexa in your kitchen without missing a syllable.
For the curious, here’s a quick how-to.
1. Download the Scout FM iOS or Android app
You know the drill. We’re in Apple App Store & Google Play.

The home screen of the Scout FM apps has a directory of all of our stations. We’ve got a ton of stations like Learn Something, History, People Stories, Business Builders, Daily News, Conservative Politics… look around and find one you love.
2. Tap the “+Alexa” button on your favorite station
Any station that has a corresponding Alexa station has a small button that says “+Alexa”. Tap that. Now you’re in the connection flow.
3. Enable the skill for your favorite station with a voice command
Next, you’ll see the instructions on how to enable the skill (apps on Alexa are called “skills”). You can do it with your voice. Just say “Alexa, enable Learn Something Radio” (or whichever station you are enabling).



4. Ask Alexa nicely for the code
Then, you’re going to ask Alexa for a code. Just say “Alexa, ask Learn Something Radio to connect.” Alexa will give you a four digit code. This is how we figure out that the Alexa and the phone belong to the same person.
5. Enter the code
BOOM! That’s it. You are now connected. All of your episode progress, you favorites, and your “trashes” are now everywhere you go. It’s like magic.

You might not have thought you needed this in your life, but once you use it you’ll be annoyed at any app that doesn’t do this.
Alexa cheat sheet
I’m sure you are an Alexa expert by now, but just in case you aren’t, here’s a cheat sheet for how to use audio apps on Alexa, and a few Subcast-specific commands.
“Alexa, enable Learn Something radio” enables the skill and makes it work on your Alexa.
“Alexa, open Learn Something radio” starts a session. We just start playing.
“Alexa, next” goes from one episode on the station to the next.
“Alexa, previous” goes to the previous episode.
“Alexa, repeat” goes back 30 seconds.
“Alexa, stop” makes her stop.
“Alexa, resume” picks right back up again.
“Alexa, tell Learn Something Radio I love this” is the same as 💜
“Alexa, tell Learn Something Radio I hate this” is the same as 🗑️
You can find us at subcast.com, on Alexa, iOS, and Android, and on the Twitter at @subcasthq. Happy Listening!

Did you know that you can enable Alexa skills verbally? Just say the command! “Alexa, Open Scout FM”"
86,"Announcing Scout FM: a native podcast listening experience for smart speakers
Why we believe voice assistants hold the key to mainstream podcast listening.",smart speaker,https://medium.com/scout-fm/announcing-scout-fm-a-native-podcast-listening-experience-for-smart-speakers-8c6314537640,"Two incredible things happened at the end of 2014.
First, on October 3rd, 2014, the first episode of Serial aired on This American Life. “The Alibi” had the perfect combination of dramatic story telling, a compelling cliff-hanger, and the high production values you would expect from the This American Life team. At the end of the episode, Ira Glass directed listeners to the next episode that was available immediately, as a podcast. A nation was hooked. Years later, Serial is still the gateway podcast for many new listeners.
Second, Amazon shipped it’s first Amazon Echo. Their previous hardware project, the Fire Phone, was a total bomb and the company needed to redeem itself. While Google and Apple were focused on smart phone domination, Amazon released the Echo and quickly found it’s way under Christmas trees and into people’s homes. The Echo was more successful than anyone could have expected, including people inside of Amazon. Two long years later Google to came out with their own smart speaker, and eventually Apple followed suit.
Fast-forward to 2018. Where are we today?
Between 2014 and 2018, the percentage of Americans who listen to podcasts weekly went from 10% to 17% — a large increase, but still not mainstream. The money brought in by advertising has reached $300M and is still climbing. More importantly, all of that money and the ready user base has attracted industry professionals who make tightly edited, narrative audio stories with high production values. Every week, there is a hot, new show brought to us by professional journalists like The New York Times, Washington Post, The Wall Street Journal, and ESPN. Podcast networks, like Gimlet and Wondery, have raised money to fuel experimentation with new formats and hire great talent. And a slew of startups (including ours) have taken venture capital to innovate in the space.
Meanwhile, 18% of Americans now own a smart speaker. If you didn’t already know that, pick your jaw up off the floor. The adoption rate is faster than for smart phones. There are more than 40,000 Alexa skills (like apps, but for Alexa), and 74% of Alexa owners use Alexa weekly. Amazon has almost 80% market share, but no one is counting Google out just yet. Just last quarter, they surpassed Amazon in smart speaker sales as international sales ramp up.
Scout FM started as an experiment.
When we started Scout FM (codename: Subcast) in 2017 we knew three things:
We were in a Golden Age of Radio.
Despite podcast growth, not enough people were listening to them.
Smart speakers were going to be huge, and they were perfect for listening to audio.
76% of smart speakers owners used them to listen to music. Nearly 50% used them to listen to the radio. To us, smart speakers were the reincarnation of the home radio, but without the limitations of terrestrial radio. Meanwhile, podcasts filled a gap left by minimal talk radio offerings, particularly in between the coasts, which was exactly where smart speakers were starting to proliferate. We started to wonder if podcasts could ride the coattails of smart speakers right into people’s homes. We started to experiment.
Our first Alexa skill was Game of Thrones Radio. It debuted three weeks before the season finale aired. We took the RSS feeds from ten different Game of Thrones podcasts and strung the episodes together. When users said, “Open Game of Thrones Radio,” Alexa would welcome you, and then play hours and hours of Game of Thrones podcasts.
Game of Thrones Radio was a modest success. When you searched “Game of Thrones” in the Alexa skill store, it ranked second. It had 500 users in the first week. What was more remarkable was how people were listening. They would binge for hours on shows they had never heard before. This behavior was promising.
Since that first skill, we’ve launched another thirty skills for Alexa and accrued more than 1.5 million minutes listened. We’ve experimented with formats, topics, genres, voices, intros, skill store optimizations… some things worked, and others were total disasters. The most effective improvements have involved incorporating real humans into the experience. We hand-curated stations, hired voice actors to record bot interactions, and wrote Alexa-friendly episode descriptions to replicate how a radio host would introduce each upcoming episode. We then layered in a recommendation engine so listeners can benefit from personalization. Listening each month went up 20% per month on average, with Alexa session times that were twice as long as in our apps.
We also started to get to know our listeners. We played an audio ad asking people to visit our website to join a paid study. We got respondents from all over the country. Here are a few of my favorites:
Jessica is a 21-year-old who moved to Florida for the sun. She works as a project manager for a sports team. Her Alexa was a gift. Every morning, she listens to music or “something nerdy” (our History station) while getting ready for work. She listens again before bed to unwind.
Nathan is a 47-year-old farmer from northern California. He bought an Alexa to control his sprinklers. He listens to our Daily News or Science stations after he does his chores in the morning. Note: farmer’s mornings start at 3:30am. I asked what he listened to before he found our stations on his Alexa. “Nothing,” he said.
Bobby is a 54-year-old music instructor from Oklahoma. He listens to Brain Food at home, and while driving between lessons. I asked what he listened to before our stations. “I hate to admit this, but Rush Limbaugh.” He explained that he didn’t actually like Rush, but there weren’t many options where he lived.
What do each of these listeners have in common? Prior to getting their Alexa, they didn’t listen to podcasts. Now they do.
Scout FM is launching today, but is just the beginning.

Scout FM, launching today!
Today, we’re launching Scout FM on Alexa: the first voice-first podcast listening experience. It’s not just a way to access podcasts via a smart speaker. It’s a podcast listening experience designed specifically for a voice assistant world. Instead of having dozens of stations, there will be one — Scout FM. Its mission: to find you something good to listen to. It’s simple with limited options and nothing memorize. Because Alexa skills need to be simple. The smarts and complexity are behind the scenes.
Scout FM rolls up all of the learnings from our Alexa experimentation, and this is just the beginning. We plan to keep experimenting, learning, improving our recommendations, evolving as the platforms evolve, and being wherever voice assistants are — which we predict will be everywhere."
87,Kaleido Insights Impact Analysis on Smart Speakers,smart speaker,https://medium.com/@jowyang/kaleido-insights-impact-analysis-on-smart-speakers-a0ea10ba4fb3,"We put them in our homes. We speak to them, listen to them, buy things from them. Much ado about smart speakers, but what are the implications for consumers and end users?
Kaleido Insights’ methodology for analyzing emerging technology assesses the impacts on humans, on businesses, and on the ecosystem. As part of our ongoing coverage, we’ll be covering a series of topics using our methodology to help business leaders first understand, and then see beyond the bright and shiny and cut right to what matters.
In each post, all Kaleido Insights analysts conduct a joint analysis session around one topic (i.e. technology, event, announcement, etc.) We begin with analyzing the human impacts of smart speakers.
Topic: Smart Speakers
Examples: Amazon Echo, Google Home, Sonos One, Apple Homepod, among many others
Impact Analysis: Humans (consumers and end users)

User Behavior & Adoption: While smart home adoption, measured by numerous devices, hovered around 10% market adoption for years, smart speakers have injected new life into this space. Adoption of smart speakers grew from 5% in Q4 2015 to 12% in Q4 2016 in US markets alone — a 130% CAGR. With the dominant Amazon having sold some 15.3 million Echos, Dots, and Taps in the last 12 months, according to Parks & Associates. The success of these devices has also made its way into cars, healthcare, and even industrial environments, and voice-enabled virtual assistants are now being integrated in a range of IoT platform solutions.
With adjacent advancements in natural language understanding, the technology leaders powering these devices are also expanding to Germany, France, Spain, and beyond. Interestingly, the smart speaker market in the West mirrors the parallel growth market in the East — social robots — also powered by voice-enabled virtual assistants but more anthropomorphic.

Today’s voice-enabled virtual agents for smart home adoption take different form factors in North America and Western Europe compared to popular devices in Asian markets
User Interface: Voice-enablement and hands-free user interface reduce barriers to entry for all. Simply put, it’s easier and it’s human. We are innately wired to learn and produce language with relatively little effort. Still, while voice is a significant improvement in interface in certain settings — kitchen, driving, holding children — it is not appropriate in all settings or when there is overwhelming background noise.
Impact on Experience: Ultimately reducing the friction of clicking, typing, and tapping with simply speaking introduces new convenience and efficiency. While smart speakers immediately reduce the friction of using technology in the home, they also offer brands new opportunities to improve broader customer experiences. For example, Domino’s Pizza allows enables customers to order a pizza from “anyware” — any hardware, that is — from an Apple Watch to a smart TV to the Google Home.
Both Amazon and Google recently announced, new ‘multi-step’ actions, wherein devices execute multiple tasks with a single prompt, are just the latest updates designed to reduce friction. Simply saying “good morning” to instigate a news briefing, automatically brew coffee, and adjust the lighting for instance, is just another incremental advancement in leveraging voice interface to improve the smart home experience. Brands, manufacturers, and service providers are all flocking towards these devices
User Psychology: The emergence of smart speakers hasn’t just brought voice-interaction into the mainstream, it’s offered a glimpse into the power of anthropomorphizing devices. Never mind that smart speakers look like speakers, consumers expect them to seamlessly interact, and increasingly for agents to “remember” relevant information such as past search queries, feature preferences, and other context, just as a human would.
Pioneered by the likes of Siri, Google Assistant, and Alexa, consumers expectations for voice-enabled virtual agents are quickly spanning others devices too. Smart speaker owners often report how they expect other devices to have the same functionality — “why can’t I just tell my fan to turn off?”
Meanwhile, unlike in human interactions, virtual assistants are deaf to manners and non-responsive to social faux pas. When assistants fail, users are annoyed, often admitting to yelling or scolding them. Parents lament the fact that their kids can be as mean as they want, and virtual agents will remain subservient and friendly. Implications of these technologies on developmental and adult psychology remain woefully unclear.
Use Cases: Another driver of adoption of smart speakers is that they are inherently ‘horizontal’ in that they support a wide range of use cases. From listening to music, to turning on the lights, to ordering an Uber or virtually any other product, the use cases for these devices look more like a smartphone than any other consumer IoT or smart home device. Similar to a smartphone, wherein making phone calls is a tiny fraction of its capability, a smart speaker is something of a category misnomer; playing music is also just the tip of the iceberg. Instead, these devices are better understood as voice-enabled vehicles for cloud services and mobile apps.
Perhaps one of the most critical impacts of smart speakers in consumer markets is they set a precedent for product appreciating over time, compared to past models in which products only depreciated after purchase. Both Amazon and Google offer open up development to the broader ecosystem meaning manufacturers, brands, and even individual developers can create new apps, new features, and integrations all the time. This open and expanding ecosystem doesn’t just create a better out-of-the-box experience, it also extends the range of potential use cases, users, and value over time.
Access & Mobility: Some technologies help enable or mobilize new segments of people. While this is great for adoption and brand marketers, in certain cases it can also enhance people’s lives. Consider, for instance, how smart speakers are enabling elderly folks to listen to audiobooks, connect with family members, and use home care apps; and disabled folks to enjoy internet services, play games, live more independently, or even offers kids story time enhancement.
Risks & Challenges: Despite the growing success of smart speakers in the home, the technology carries a host of risks and challenges for brands and consumers alike. Zeroing in on impacts to humans, Kaleido analysts identify risks associated with user privacy, data protection associated with cloud-based processing of highly sensitive data, as well as user experience.
A recent murder case cast a spotlight on Amazon as questions of privacy, consent, evidence, caused the company to hand over sensitive Echo data to Arkansas officials. In addition, smart speakers in the US have tangled with the Child Online Privacy Protection act (COPPA) — and soon with the EU’s Global Data Protection Regulation (GDPR) — , prompting Amazon to recently unveil parental consent and control features as well as child-specific skills. And Google’s latest product must have a feature disabled due to a privacy mishap. Even without the sensitive content, most smart speakers today are not designed for multi-user personalization, even if they are used by multiple users.
Technology Proliferation: Although smart speakers have improved the user experience in the smart home, challenges remain on the technology side. For one, no one wants to have 17 different apps for 17 different devices; we don’t want to be system administrators for our homes. And while smart speakers have shifted the form factor of a ‘home automation’ hub (from an app or gateway to a speaker) the administration of these devices, their apps, and data remains a cumbersome user experience even for the technologically proficient.
Then there are additional challenges around interoperability. Consumers don’t want to be boxed in to using single brands or manufacturers, and especially in the home. Although both Amazon and Google offer an impressive (and growing) array of service and product integrations through their Skills and Actions SDKs respectively, they draw a line when it comes to each other. Fiercely competitive Google won’t integrate with Amazon or Apple, and vice versa.
Smart speakers mark an exciting technological shift, most notably in accelerating voice as a mainstream human-machine interface. Over time though, Kaleido analysts expect form factor will be de-emphasized and eventually disappear, as the machine and deep learning behind these virtual agents will:
Learn: They become hyper-personalized to individual users, as we train these systems to become our friends, mates, and more.
Predict: What customers want based on multi-modal historical and real-time data sets Transcend any single form factor, instead “follow” users wherever they go (home, office, car, retail, medical, etc.)
Infuse: almost any physical space with speaking, thinking, predictive services… and sentience? The digital interface is not needed where sound travels.
This is just one of the myriad technologies shifting how businesses interact with consumers and their ecosystems. And these are just a few of the many impacts on consumers today. Kaleido analysts are tracking these and other technologies closely to help you find clarity amidst the chaos. Interested in discussing the impacts of smart speakers? Don’t hesitate to reach out."
88,"How Creepy Is Your Smart Speaker?
Worries about privacy are overstated, but not entirely without merit. Your move, Alexa",smart speaker,https://medium.com/@the_economist/how-creepy-is-your-smart-speaker-a94dc214eedf,"lexa, are you recording everything you hear?” It is a question more people are asking, though Amazon’s voice assistant denies the charges. “I only record and send audio back to the Amazon cloud when you say the wake word,” she insists, before referring questioners to Amazon’s privacy policy. Apple’s voice assistant, Siri, gives a similar answer. But as smart speakers from Amazon, Apple, Google and other technology giants proliferate (global sales more than doubled last year, to 86.2m) concerns that they might be digitally snooping have become more widespread. And now that these devices are acquiring other senses beyond hearing — the latest models have cameras, and future ones may use “lidar” sensors to see shapes and detect human gestures (see article) — the scope for infringing privacy is increasing. So how worried should you be that your speaker is spying on you?
For years the tech industry has dreamed of computing appliances that are considered unremarkable items of household machinery, like washing machines or fridges. The smart speaker has finally realised this promise. It can sit on a kitchen counter and summon the wonders of the internet without the need for swiping or typing. Using it is like casting a spell. Say the magic words and you can conjure up dodgy Eighties rock while up to your elbows in washing-up, or prove to your mum that Ronaldo has scored more goals than Messi. This hands-free convenience has a cost: the speakers are constantly listening out for commands. As with any advanced and apparently magical technology, however, myths quickly grow up about how they work.
So start with some myth-busting. As Alexa herself contends, smart speakers are not sending every utterance into the tech giants’ digital vaults. Despite their name, the devices are simple-minded. They listen out for wake words, and then send what follows to the cloud as an audio clip; when an answer arrives, in the form of another audio clip, they play it back. Putting all the smarts in the cloud means these speakers can be very cheap and acquire new skills as their cloud-based brains are continually upgraded. As part of this improvement, manufacturers (such as Amazon) store sound clips of queries, so they can be assessed by humans if necessary. But Amazon notes that users can delete these clips at any time. There’s always the mute button if you are worried about accidentally triggering your speaker and sending a clip into the cloud during a sensitive conversation. Users, the firm insists, are in control.
Not everyone is convinced by such assurances, however. What if hackers infiltrate the devices? Could governments require manufacturers to provide back doors? Are their makers using them to snoop on people and then exploiting that information to target online ads or offer them particular products? Some people refuse to let Alexa and Siri into the house.
If eavesdropping is your problem, eschewing smart speakers does not solve it. Smartphones, which people blithely carry around with them, are even worse. Spy agencies are said to be able to activate the microphone in such devices, which have even more sensors than smart speakers, including location-tracking GPS chips and accelerometers than can reveal when and how the phone is moving. And smartphones are, if anything, even more intimate than smart speakers. Few of Alexa’s users, after all, take her into bed with them.
At the same time as devices are getting cleverer (Amazon makes a microwave oven with built-in voice assistant), the big tech firms are expanding into adjacent areas such as shopping services, finance and entertainment. Over time this may mean their incentives to snoop and misuse data rise. But there will also be a countervailing incentive for manufacturers to differentiate themselves by making more privacy-friendly devices that promise not to store voice commands, or process more on the device rather than in the cloud (though this will be more expensive). The chief thing is that consumers should be able to choose how to balance convenience and privacy. If this magical technology is to reach its full potential, the tech giants need to do more to convince users that Alexa and her friends can be trusted."
89,The smart speaker war has begun,smart speaker,https://medium.com/futuresin/the-smart-speaker-war-has-begun-e62232c34a07,"According to Allied Market Research the global smart speaker market size in 2017 was valued at $4.3 billion, and is projected to reach $23 billion by 2025 (with a compound annual growth rate of 23.4% from 2018 to 2025). North America constituted the biggest smart speaker market share with 36.9%. Asia-Pacific is also a high performing market with a 24.93% annual growth rate.
This potentially lucrative market has a few big tech firms developing offerings at a rapid pace; and the commotion is quite reminiscent of the old smartphone battles of the 2000’s to say the least. Which players have the best chance to win this war? What is really at stake? Let’s discuss these points.
Battle of the giants: Amazon vs. Google
Amazon Echo and Google Home are the two most popular brands of small, “table-top speakers”. Their smartness lies in the included voice assistants: Amazon Alexa and Google Assistant respectively, which you can ask questions or give commands to inquire about the weather, set a timer or purchase a book.

“Google Home Mini vs Amazon Echo Dot 3 — Who Wins now?” by TechWiser, YouTube.
More than 100 million of Alexa-powered devices have been sold. That’s rare number Amazon’s SVP of devices and services, Dave Limp, revealed to The Verge earlier this year. In December 2018, An RBC analyst estimated Google had sold 52 million Google Home devices worldwide with 43 million in the United States.
What about challenger brands?
Far behind are a few brands that are trying to challenge the Amazon / Google hegemony.
Sonos have been making quality wireless speakers for a while, capable of streaming music from multiple online services and local media libraries in your home.

“Sonos One — Hands On Review” by Digital Trends, Youtube.
With the Sonos One, the company started to incorporate AI (through Alexa). You get the usual Sonos quality plus the ability to use your voice to change songs (or check the weather, etc…).
The HomePod by Apple’ combines a premium speaker and a smart home hub. TechRadar notes it comes with “amazing sound and incredibly intuitive set-up”.

“Apple HomePod Review: The Dumbest Smart Speaker?” by Marques Brownlee, YouTube.
The publication added: “Siri (Apple’s voice-assistant) is only middling in its implementation, and the fact that you’re not able to break out of the Apple ecosystem for many key functions also rankles”.
Fad or revolution?
Last fall, Adobe Analytics surveyed more than 1,000 U.S. consumers to get a better grasp as to how people are using their smart speakers. The company found that the most common uses were playing music (70 per cent of smart speaker owners surveyed) and monitoring the weather (64 per cent).
Moreover, 50 percent of respondents used their speakers to set alarms and reminders, check the news, and execute online searches; roughly one third used their speaker to control their smart devices and order products online.
In other words, the ways people are using the assistants included with speakers is still quite basic. Complex actions such as shopping, food delivery, communicating with friends and family; and research are still emerging trends.
It’s still too early to really predict the impact of smart speakers on our societies but one thing is for sure: the battle between Amazon and Google will be ruthless."